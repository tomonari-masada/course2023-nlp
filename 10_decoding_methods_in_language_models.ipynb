{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/10_decoding_methods_in_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgSATSBHqe_W"
      },
      "source": [
        "# „ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„Å´„Åä„Åë„Çãdecoding\n",
        "* „Åù„ÅÆ‰ªñ„ÅÆÂèÇËÄÉ„Å´„Å™„ÇãË≥áÊñô\n",
        " * https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/\n",
        "* ÂèØË¶ñÂåñ„Å´Èñ¢„Åó„Å¶ÂèÇËÄÉ„Å´„Åó„ÅüË≥áÊñô\n",
        " * https://mlabonne.github.io/blog/decoding/\n",
        " * „Åì„ÅÆË≥áÊñô„ÅØ„ÄÅ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅÆË™¨Êòé„ÅØÊ≠£Á¢∫„Åß„Å™„ÅÑ„Åü„ÇÅ„ÄÅÊ≥®ÊÑè„ÄÇ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8VKedOjlt_W"
      },
      "source": [
        "## Ë®ÄË™û„É¢„Éá„É´„Çí‰Ωø„Å£„Åü„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê\n",
        "* ‰ªäÂõû„ÅØ„ÄÅË®ÄË™û„É¢„Éá„É´„Çí„ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„Å´‰Ωø„ÅÜ„ÄÇ\n",
        "* „ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åô„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†Ôºàdecoding„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†Ôºâ„ÅØË§áÊï∞„ÅÇ„Çã„Åì„Å®„ÇíÂ≠¶„Å∂„ÄÇ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zApYZfF3c3"
      },
      "source": [
        "### „ÉÜ„Ç≠„Çπ„Éà„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å´ÁîüÊàê„Åï„Çå„Çã„ÅãÔºü\n",
        "* ÁîüÊàê„Å´‰Ωø„ÅÜË®ÄË™û„É¢„Éá„É´„ÅÆÊúÄÁµÇÁöÑ„Å™Âá∫Âäõ„ÅØ„ÄÅË™ûÂΩôÈõÜÂêà‰∏ä„Å´ÂÆöÁæ©„Åï„Çå„ÅüÁ¢∫ÁéáÂàÜÂ∏É„ÄÇ\n",
        " * „Å§„Åæ„Çä„ÄÅÂêÑÂçòË™û„Å´Êï∞ÂÄ§„ÅåÂâ≤„ÇäÊåØ„Çâ„Çå„Å¶„ÅÑ„Å¶„ÄÅ„Åù„Çå„Çâ„ÇíÂêàË®à„Åô„Çã„Å®1„Å´„Å™„Çã„ÄÇ\n",
        " * „Åì„Çå„Çâ„ÅÆÊï∞ÂÄ§„ÅØ„ÄÅÊ¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Åå„Åù„ÅÆÂçòË™û„ÅÆ„Éà„Éº„ÇØ„É≥„Å´„Å™„ÇãÁ¢∫Áéá„ÇíË°®„Åó„Å¶„ÅÑ„Çã„ÄÇ\n",
        "* „Åó„Åã„Åó„ÄÅÊ¨°„ÅÆ‰∏Ä„Å§„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÁîüÊàê„Åô„Çã„Åü„ÇÅ„Å´„ÄÅ„Åì„ÅÆÁ¢∫ÁéáÂàÜÂ∏É„Çí„Å©„ÅÆ„Çà„ÅÜ„Å´‰Ωø„ÅÜ„ÅÆ„Å†„Çç„ÅÜ„ÅãÔºü\n",
        " * Á¢∫ÁéáÊúÄÂ§ß„ÅÆÂçòË™û„ÇíÊ¨°„ÄÖ„Å´ÈÅ∏„Å∂„Å®„ÅÑ„ÅÜ„Ç¢„É´„Ç¥„É™„Ç∫„É†„ÅØ„ÄÅËâØ„ÅÑ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å†„Çç„ÅÜ„ÅãÔºü\n",
        "* Ê¨°„ÅÆ‰∏Ä„Å§„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÈÅ∏„Å∂„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´„ÄÅË§áÊï∞„ÅÇ„Çã„Åì„Å®„Çí‰ªäÊó•„ÅØÂ≠¶„Å∂„ÄÇ\n",
        " * „ÅÑ„Åö„Çå„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Çí‰Ωø„ÅÜ„Åã„Åß„ÄÅÁîüÊàê„Åï„Çå„Çã„ÉÜ„Ç≠„Çπ„Éà„ÅÆÊßòÂ≠ê„ÅåÈÅï„Å£„Å¶„Åè„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbhuFLTG_RW"
      },
      "source": [
        "**„É©„É≥„Çø„Ç§„É†„ÅÆ„Çø„Ç§„Éó„ÅØGPU„Å´„Åó„Å¶„Åä„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW1IYEw0sBtc"
      },
      "source": [
        "## Ê∫ñÂÇô"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-TkVrykSCQz"
      },
      "source": [
        "### „Ç§„É≥„Çπ„Éà„Éº„É´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKY1FX7beLim"
      },
      "source": [
        "* graphviz„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Çà„ÅÜ„Å®„Åó„Å¶„ÄÅutf8Èñ¢‰øÇ„ÅÆ„Ç®„É©„Éº„ÅåÂá∫„Åü„Çâ„ÄÅ‰ª•‰∏ã„ÅÆ„Çª„É´„ÇíÂÆüË°å„Åó„Å¶„Åã„Çâ„Ç§„É≥„Çπ„Éà„Éº„É´„Åô„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7aCeNp2eIQp"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iKAfvyNrYd9"
      },
      "source": [
        "* graphviz„Å®transformers„ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As0ruNRPrV-S"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install graphviz graphviz-dev\n",
        "!pip install transformers pygraphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVNRkLVHSFkk"
      },
      "source": [
        "### „Ç§„É≥„Éù„Éº„Éà"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhFveia6JmQH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R4IKg4lbNcN"
      },
      "source": [
        "## Ë®ÄË™û„É¢„Éá„É´\n",
        "* ‰ªäÂõû„ÅØGPT2„Çí‰Ωø„ÅÜ„ÄÇ\n",
        " * ÊúÄÂàù„Å†„Åë„É¢„Éá„É´„ÅÆ„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Å´ÊôÇÈñì„Åå„Åã„Åã„Çã„ÄÇ\n",
        "* GPT2LMHeadModel„Å®„ÅØ\n",
        "\"The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\"\n",
        " * https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel\n",
        "\n",
        "* „ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åï„Åõ„Çã„Å†„Åë„Å™„ÅÆ„Åß`eval()`„É°„ÇΩ„ÉÉ„Éâ„ÇíÂëº„Å∂„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt6NhGjANTEm"
      },
      "source": [
        "### „É¢„Éá„É´„ÅÆÂèñÂæó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsvrVwdJsZuT"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHRHB4xJt7SY"
      },
      "source": [
        "* Ë™ûÂΩô„Çµ„Ç§„Ç∫„ÇíË™ø„Åπ„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4osQpOPVt6PL"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H75oB9oouAqf"
      },
      "source": [
        "* ÈÅ©ÂΩì„Å™ÂçòË™û„ÅÆid„ÇíË™ø„Åπ„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4hdLwJIuCr7"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab()[\"hello\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqMyKWKJuMol"
      },
      "source": [
        "* ÈÅ©ÂΩì„Å™„ÉÜ„Ç≠„Çπ„Éà„Çíid„ÅÆÂàó„Å´Â§âÊèõ„Åó„Å¶„Åø„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Gd3xOjuPs-"
      },
      "outputs": [],
      "source": [
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeOVit1JstZg"
      },
      "source": [
        "* „Éá„Éï„Ç©„É´„Éà„ÅÆË®≠ÂÆö„ÅÆ„ÇÇ„Å®„Åß„ÉÜ„Ç≠„Çπ„Éà„ÅÆÁ∂ö„Åç„ÇíÁîüÊàê„Åï„Åõ„Å¶„Åø„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJn0V5mmvK4c"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=100,\n",
        "    )\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB5Fns4SvOlO"
      },
      "source": [
        "* ÁîüÊàê„Åï„Çå„ÅüidÂàó„Çí„Éà„Éº„ÇØ„É≥Âàó„Å´Â§âÊèõ„Åô„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LS5sCUPwzaD"
      },
      "outputs": [],
      "source": [
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Generated text:\\n\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB-nF44SxLnU"
      },
      "source": [
        "### „É¢„Éá„É´„ÅåÂá∫Âäõ„Åô„Çã‰∫àÊ∏¨logits„ÅÆÁ¢∫Ë™ç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJAvgFzewwNN"
      },
      "outputs": [],
      "source": [
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4boscktHxCz"
      },
      "source": [
        "* „ÉÜ„Ç≠„Çπ„Éà„ÇíÁîüÊàê„Åï„Åõ„Çã„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅ„É¢„Éá„É´„ÅåÁõ¥Êé•Âá∫Âäõ„Åô„Çã„Éá„Éº„Çø„ÇíË™ø„Åπ„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYTu8I3Pwy6b"
      },
      "outputs": [],
      "source": [
        "outputs = model(input_ids)\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HteyJvl-IBx4"
      },
      "source": [
        "* `logit`„ÅØÁ¢∫Áéá„Å´Â§âÊèõ„Åï„Çå„ÇãÂâç„ÅÆÂÄ§„ÄÇ\n",
        " * „Åù„ÅÆ`shape`„ÅØ[„Ç∑„Éº„Ç±„É≥„ÇπÊï∞, „Éà„Éº„ÇØ„É≥Êï∞, Ë™ûÂΩô„Çµ„Ç§„Ç∫]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j2Hg6M8w14E"
      },
      "outputs": [],
      "source": [
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0mAesGuyUIc"
      },
      "source": [
        "* Ê¨°„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„ÅÆ‰∫àÊ∏¨logit„ÇíÂèñÂæó„Åô„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LnsYQ4dyBAr"
      },
      "outputs": [],
      "source": [
        "logits = outputs.logits[0,-1]\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPpNC7QSyjhm"
      },
      "source": [
        "* Á¢∫ÁéáÊúÄÂ§ß„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„ÇíË™ø„Åπ„Çã„ÄÇ\n",
        " * Á¢∫ÁéáÊúÄÂ§ß„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„ÇíÊ¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Å´ÈÅ∏„Å∂„ÅÆ„Åå„ÄÅgreedy search„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egCYdRHfKbXs"
      },
      "outputs": [],
      "source": [
        "torch.argmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVKOns7hysag"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(torch.argmax(logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKtQqhJNPtnM"
      },
      "source": [
        "## decoding„ÅÆÂèØË¶ñÂåñ„ÅÆÊ∫ñÂÇô"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D7Orsd4L33c"
      },
      "source": [
        "### ÂØæÊï∞Á¢∫Áéá„ÇíÊ±Ç„ÇÅ„Çã„Éò„É´„ÉëÈñ¢Êï∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEgpaxpSL22C"
      },
      "outputs": [],
      "source": [
        "def get_log_prob(logits, token_id):\n",
        "  log_probabilities = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "  token_log_probability = log_probabilities[token_id].item()\n",
        "  return token_log_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkrbJCG5OeQh"
      },
      "source": [
        "### „Ç∞„É©„ÉïÊßãÈÄ†„ÇíÂèØË¶ñÂåñ„Åô„Çã„Éò„É´„ÉëÈñ¢Êï∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMNyl5maqTc"
      },
      "outputs": [],
      "source": [
        "def plot_graph(graph, length, beams, score_type):\n",
        "  fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')\n",
        "\n",
        "  # „Éé„Éº„Éâ„ÅÆ„É¨„Ç§„Ç¢„Ç¶„Éà„ÇíÁ¢∫ÂÆö„Åï„Åõ„Çã\n",
        "  pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
        "\n",
        "  # Á¢∫Áéá„Å´Âøú„Åò„Å¶„Éé„Éº„Éâ„ÅÆËâ≤„ÇíÊ±∫„ÇÅ„Çã\n",
        "  assert score_type in ['token', 'sequence']\n",
        "  scores = [\n",
        "      data[score_type + 'score']\n",
        "      for _, data in graph.nodes(data=True)\n",
        "      if data['token'] is not None\n",
        "      ]\n",
        "  vmin = min(scores)\n",
        "  vmax = max(scores)\n",
        "  norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
        "  cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256)\n",
        "\n",
        "  # „Éé„Éº„Éâ„ÇíÊèè„Åè\n",
        "  nx.draw_networkx_nodes(\n",
        "      graph, pos, node_size=2000, node_shape='o',\n",
        "      alpha=1, linewidths=4,\n",
        "      node_color=scores, cmap=cmap,\n",
        "      )\n",
        "\n",
        "  # „Ç®„ÉÉ„Ç∏„ÇíÊèè„Åè\n",
        "  nx.draw_networkx_edges(graph, pos)\n",
        "\n",
        "  # „É©„Éô„É´„ÇíÊèè„Åè\n",
        "  suffix = \"%\" if score_type == \"token\" else \"\"\n",
        "  labels = {\n",
        "      node: data['token'].split('_')[0] + f\"\\n{data[score_type + 'score']:.2f}\" + suffix\n",
        "      for node, data in graph.nodes(data=True)\n",
        "      if data['token'] is not None\n",
        "      }\n",
        "  nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n",
        "  plt.box(False)\n",
        "\n",
        "  # Á¢∫Áéá„ÅÆÈ´ò‰Ωé„ÇíË°®„Åô„Ç´„É©„Éº„Éê„Éº„ÇíËøΩÂä†„Åô„Çã\n",
        "  sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "  sm.set_array([])\n",
        "  if score_type == 'token':\n",
        "    label = 'Token probability (%)'\n",
        "  elif score_type == 'sequence':\n",
        "    label = 'Sequence score'\n",
        "  fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label=label)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7mElwjbPYW"
      },
      "source": [
        "## üèÉ‚Äç‚ôÇÔ∏è Greedy Search\n",
        "* „Åù„ÅÆÈÉΩÂ∫¶Á¢∫ÁéáÊúÄÂ§ß„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÈÅ∏„Å∂„Å®„ÅÑ„ÅÜ„ÄÅÊúÄ„ÇÇ„Ç∑„É≥„Éó„É´„Å™decoding„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy3681h6AJlS"
      },
      "source": [
        "### „Éá„Éº„Çø„Å®„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu9hB8smMD-Z"
      },
      "outputs": [],
      "source": [
        "# „Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥Áî®„ÅÆ„Éá„Éº„Çø\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# ÂèØË¶ñÂåñ„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "length = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxS7QzKMQn1r"
      },
      "source": [
        "### decode„ÅÆÂÆüË°å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm_AiUhK8yrP"
      },
      "outputs": [],
      "source": [
        "# „Éé„Éº„Éâ„Åå‰∏Ä„Å§„ÅÆ„Ç∞„É©„Éï„Çí‰ΩúÊàê\n",
        "graph = nx.DiGraph()\n",
        "graph.add_node(graph.number_of_nodes())\n",
        "node = graph.number_of_nodes() - 1\n",
        "\n",
        "# „Ç∞„É©„Éï„ÅÆÊ†π„Éé„Éº„Éâ„ÅåÊåÅ„Å§ÊÉÖÂ†±„ÇíÂàùÊúüÂåñ„Åô„Çã\n",
        "graph.nodes[node]['tokenscore'] = 100\n",
        "graph.nodes[node]['token'] = text\n",
        "\n",
        "# Ê†π„Éé„Éº„Éâ„ÇíÂá∫Áô∫ÁÇπ„Å´Ë®≠ÂÆö„Åô„Çã\n",
        "node = 0\n",
        "\n",
        "for n in range(length):\n",
        "\n",
        "  # „É¢„Éá„É´„ÅåÂá∫Âäõ„Åô„Çãlogits„ÇíÂæó„Çã\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "  predictions = outputs.logits\n",
        "\n",
        "  # Ê¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Çí‰∫àÊ∏¨„Åô„Çãlogit„Å†„Åë„ÇíÂèñ„ÇäÂá∫„Åô\n",
        "  logits = predictions[0][-1]\n",
        "\n",
        "  # Á¢∫ÁéáÊúÄÂ§ß„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„ÇíÂæó„Å¶„ÄÅ„Åù„ÅÆÁ¢∫Áéá„ÅÆÂØæÊï∞„Çí„Å®„Å£„Åü„ÇÇ„ÅÆ„ÇíÊ±Ç„ÇÅ„Çã„ÄÇ\n",
        "  token_id = torch.argmax(logits).unsqueeze(0)\n",
        "  token_score = get_log_prob(logits, token_id)\n",
        "\n",
        "  # ÂèØË¶ñÂåñÁî®„ÅÆ„Ç∞„É©„Éï„Å´ÈÅ∏„Å∞„Çå„Åü„Éà„Éº„ÇØ„É≥„ÅÆÊÉÖÂ†±„ÇíËøΩÂä†„Åô„Çã„ÄÇ\n",
        "  next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "  graph.add_node(graph.number_of_nodes())\n",
        "  new_node = graph.number_of_nodes() - 1\n",
        "  graph.add_edge(node, new_node)\n",
        "  node = new_node\n",
        "  graph.nodes[node]['tokenscore'] = np.exp(token_score) * 100\n",
        "  graph.nodes[node]['token'] = next_token + f\"_{length}\"\n",
        "\n",
        "  # ÈÅ∏„Å∞„Çå„Åü„Çµ„Éñ„ÉØ„Éº„Éâ„ÇíÂÖ•Âäõ„ÅÆ„Éà„Éº„ÇØ„É≥Âàó„Å´Á∂ô„ÅéË∂≥„Åô„ÄÇ\n",
        "  input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "output = tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "print(f\"Generated text: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BS_w2dZWG5y"
      },
      "source": [
        "### „Ç∞„É©„Éï„ÅÆÊèèÁîª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSt2J6qBOlPf"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, 1.5, 'token')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J16z-Mw295Jx"
      },
      "source": [
        "## ‚öñÔ∏è Beam Search\n",
        "* Ë§áÊï∞„ÅÆÂÄôË£ú„ÇíÊÆã„Åó„Å§„Å§„Éà„Éº„ÇØ„É≥„ÇíÁîüÊàê„Åó„Å¶„ÅÑ„Åè„ÄÇ\n",
        "* „ÅÇ„ÇãÁ®ãÂ∫¶ÁîüÊàê„ÇíÈÄ≤„ÇÅ„Åü„Å®„Åì„Çç„Åß„Éô„Çπ„Éà„Å™„Éà„Éº„ÇØ„É≥Âàó„ÇíÈÅ∏„Å∂„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfhkM6-ULkq"
      },
      "source": [
        "### „ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„ÅÆ„Éò„É´„ÉëÈñ¢Êï∞\n",
        "* „Åª„Å®„Çì„Å©„ÅÆÂá¶ÁêÜ„ÅåÂÖ®„Å¶„ÅÆ„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å´ÂÖ±ÈÄö„Åó„Å¶„ÅÑ„Çã„ÄÇ\n",
        "* „Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆÊñπÊ≥ï„Å†„Åë„ÅåÁï∞„Å™„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTDkrVdutCk1"
      },
      "outputs": [],
      "source": [
        "def decoding(ids, length, num_beams, sampling, temperature=0.1, top_k=20, nucleus_p=0.5):\n",
        "\n",
        "  input_ids = ids\n",
        "\n",
        "  # „Éé„Éº„Éâ„Åå‰∏Ä„Å§„ÅÆ„Ç∞„É©„Éï„Çí‰ΩúÊàê\n",
        "  graph = nx.DiGraph()\n",
        "  graph.add_node(graph.number_of_nodes())\n",
        "  node = graph.number_of_nodes() - 1\n",
        "\n",
        "  # „Ç∞„É©„Éï„ÅÆÊ†π„Éé„Éº„Éâ„ÅåÊåÅ„Å§ÊÉÖÂ†±„ÇíÂàùÊúüÂåñ„Åô„Çã\n",
        "  graph.nodes[node]['tokenscore'] = 100\n",
        "  graph.nodes[node]['score'] = 0\n",
        "  graph.nodes[node]['sequencescore'] = 0\n",
        "  graph.nodes[node]['token'] = text\n",
        "\n",
        "  # Ê†π„Éé„Éº„Éâ„ÇíÂá∫Áô∫ÁÇπ„Å´Ë®≠ÂÆö„Åô„Çã\n",
        "  nodes = [node]\n",
        "\n",
        "  for n in range(length):\n",
        "    # „É¢„Éá„É´„ÅåÂá∫Âäõ„Åô„Çãlogits„ÇíÂæó„Çã\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "    # Ê¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Çí‰∫àÊ∏¨„Åô„Çãlogit„Å†„Åë„ÇíÂèñ„ÇäÂá∫„Åô\n",
        "    logits = predictions[:,-1]\n",
        "\n",
        "    if sampling == 'greedy':\n",
        "      top_token_ids = torch.topk(logits, num_beams, dim=-1).indices\n",
        "    elif sampling == 'top_k':\n",
        "      top_token_ids = top_k_sampling(logits, top_k, num_beams, temperature=temperature)\n",
        "    elif sampling == 'nucleus':\n",
        "      top_token_ids = nucleus_sampling(logits, nucleus_p, num_beams, temperature=temperature)\n",
        "\n",
        "    assert len(nodes) == input_ids.shape[0]\n",
        "\n",
        "    # ÂêÑ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÊú´Â∞æ„Å´ÈÅ∏„Å∞„Çå„Åü„Éà„Éº„ÇØ„É≥„ÇíËøΩÂä†„Åô„Çã\n",
        "    node_scores = list()\n",
        "    for i in range(input_ids.shape[0]):\n",
        "      node = nodes[i]\n",
        "      for j, token_id in enumerate(top_token_ids[i]):\n",
        "\n",
        "        # ‰∫àÊ∏¨„Åï„Çå„Åü„Éà„Éº„ÇØ„É≥„ÅÆÂØæÊï∞Á¢∫Áéá„ÇíÊ±Ç„ÇÅ„Å¶„ÄÅ„Åì„Åì„Åæ„Åß„ÅÆ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÂØæÊï∞Á¢∫Áéá„Å´Âä†ÁÆó„Åô„Çã\n",
        "        token_score = get_log_prob(logits[i], token_id)\n",
        "\n",
        "        graph.add_node(graph.number_of_nodes())\n",
        "        new_node = graph.number_of_nodes() - 1\n",
        "        graph.add_edge(node, new_node)\n",
        "\n",
        "        cumulative_score = graph.nodes[node]['score'] + token_score\n",
        "        graph.nodes[new_node]['score'] = cumulative_score\n",
        "        graph.nodes[new_node]['tokenscore'] = np.exp(token_score) * 100\n",
        "        graph.nodes[new_node]['sequencescore'] = cumulative_score / (input_ids.shape[1] + 1)\n",
        "        token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "        graph.nodes[new_node]['token'] = token + f\"_{n}_{i}\"\n",
        "        node_scores.append((new_node, cumulative_score, i, token_id, token_score))\n",
        "\n",
        "    # „Çπ„Ç≥„Ç¢„ÅÆÈôçÈ†Ü„Å´„Éé„Éº„Éâ„Çí„ÇΩ„Éº„Éà\n",
        "    node_scores.sort(key=lambda a: a[1], reverse=True)\n",
        "\n",
        "    new_nodes = list()\n",
        "    new_input_ids = list()\n",
        "    for i, node_score in enumerate(node_scores[:num_beams]):\n",
        "      new_node, _, sequence_id, token_id, token_score = node_score\n",
        "      print(f\"node id: {new_node}, token score:{token_score:.3f},\",\n",
        "            \"token:\", tokenizer.decode(token_id, skip_special_tokens=True))\n",
        "      # ‰∫àÊ∏¨„Åï„Çå„Åü„Éà„Éº„ÇØ„É≥„Çí„ÄÅ„Åì„Åì„Åæ„Åß„ÅÆ„Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÊú´Â∞æ„Å´Áπã„Åí„Çã„ÄÇ\n",
        "      new_nodes.append(new_node)\n",
        "      new_input_ids.append(\n",
        "          torch.cat([input_ids[sequence_id], token_id.unsqueeze(0)],\n",
        "                    dim=-1)\n",
        "          )\n",
        "\n",
        "    nodes = new_nodes\n",
        "    input_ids = torch.stack(new_input_ids)\n",
        "\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgiRt9aSGTQ"
      },
      "source": [
        "### „Éá„Éº„Çø„Å®„Éë„É©„É°„Éº„Çø„ÅÆË®≠ÂÆö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQb-UURwQYTZ"
      },
      "outputs": [],
      "source": [
        "# „Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥Áî®„ÅÆ„Éá„Éº„Çø\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# ÂèØË¶ñÂåñ„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "length = 5\n",
        "\n",
        "# „Éì„Éº„É†„Çµ„Éº„ÉÅ„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxTnt6nPT5Kv"
      },
      "source": [
        "### decode„ÅÆÂÆüË°å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7_Ui3msGFdf"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'greedy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suVgbNiYHs0f"
      },
      "source": [
        "### „Çπ„Ç≥„Ç¢ÊúÄÂ§ß„ÅÆ„Éà„Éº„ÇØ„É≥Âàó„ÇíÂèñÂæó„Åô„Çã„Éò„É´„ÉëÈñ¢Êï∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V38YsPv8F1Fy"
      },
      "outputs": [],
      "source": [
        "def get_best_sequence(graph):\n",
        "  depths = nx.shortest_path_length(graph, 0)\n",
        "  max_depth = max(depths.values())\n",
        "  leaf_nodes = [node for node in graph.nodes if depths[node] == max_depth]\n",
        "  max_score_node = None\n",
        "  max_score = float('-inf')\n",
        "  for node in leaf_nodes:\n",
        "    if graph.nodes[node]['sequencescore'] > max_score:\n",
        "      max_score = graph.nodes[node]['sequencescore']\n",
        "      max_score_node = node\n",
        "    path = nx.shortest_path(graph, source=0, target=max_score_node)\n",
        "    sequence = \"\".join([graph.nodes[node]['token'].split('_')[0] for node in path])\n",
        "  return sequence, max_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HrqbxZzGWu-"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoOyhqpcVWI3"
      },
      "source": [
        "### „Ç∞„É©„Éï„ÅÆÊèèÁîª\n",
        "* Á§∫„Åï„Çå„Å¶„ÅÑ„Çã„Çπ„Ç≥„Ç¢„ÅØ„ÄÅ„Åù„ÅÆ„Éé„Éº„Éâ„Å´Ëá≥„Çã„Åæ„Åß„ÅÆ„Éà„Éº„ÇØ„É≥Âàó„ÅÆ„Çπ„Ç≥„Ç¢„ÄÇ\n",
        "* „Éì„Éº„É†„Çµ„Éº„ÉÅ„ÅØ„ÄÅÊúÄ„ÇÇ„Çπ„Ç≥„Ç¢„ÅÆÂ§ß„Åç„ÅÑ„Éà„Éº„ÇØ„É≥Âàó„ÇíÈÅ∏„Å∂„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPbd4dw0J9k"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmObi_P6bR0"
      },
      "source": [
        "## üé≤ Top-k sampling\n",
        "* top-k„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅØ„ÄÅÁ¢∫Áéá„ÅåÈ´ò„ÅÑkÂÄã„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„Åã„Çâ„Éª„Éª„Éª\n",
        "* „Åù„Çå„Çâ„ÅÆÁ¢∫Áéá„Å´Âæì„Å£„Å¶„É©„É≥„ÉÄ„É†„Å´Ê¨°„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÈÅ∏„Å∂„ÄÇ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzeaPdKJV5XY"
      },
      "source": [
        "### „Çµ„É≥„Éó„É™„É≥„Ç∞„ÇíË°å„ÅÜÈñ¢Êï∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzZLWF_4V2cn"
      },
      "outputs": [],
      "source": [
        "def top_k_sampling(logits, top_k, num_beams, temperature=1.0, plot=False):\n",
        "  assert top_k >= 1\n",
        "  assert num_beams <= top_k\n",
        "\n",
        "  logit_k = torch.topk(logits, top_k, dim=-1).values[:,-1]\n",
        "  indices_to_remove = logits < logit_k.unsqueeze(-1)\n",
        "  new_logits = torch.clone(logits)\n",
        "  new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # logit„ÇíÁ¢∫Áéá„Å´Â§âÊèõ„Åô„Çã\n",
        "  probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "  # top-k„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´beamsÂÄã„ÇíÈÅ∏„Å∂\n",
        "  next_tokens = torch.multinomial(probabilities, num_beams)\n",
        "\n",
        "  if plot:\n",
        "    # „Çµ„Éñ„ÉØ„Éº„Éâ„ÇíÈÅ∏„Å∂„Å®„Åç„Å´‰Ωø„Å£„ÅüÁ¢∫ÁéáÂàÜÂ∏É„ÇíÊèèÁîª„Åô„Çã\n",
        "    total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)\n",
        "\n",
        "  return next_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bxyiGsVUYy9"
      },
      "source": [
        "### „Çµ„Éñ„ÉØ„Éº„Éâ„ÅÆÁ¢∫ÁéáÂàÜÂ∏É„ÇíÊèèÁîª„Åô„Çã„Éò„É´„ÉëÈñ¢Êï∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsDJWQs-tav"
      },
      "outputs": [],
      "source": [
        "def plot_prob_distribution(total_prob, next_tokens, sampling, potential_nb, total_nb=50):\n",
        "  for i in range(total_prob.shape[0]):\n",
        "    probabilities = total_prob[i]\n",
        "    next_token = next_tokens[i]\n",
        "    # top k„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„ÇíÂèñÂæó\n",
        "    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)\n",
        "    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]\n",
        "\n",
        "    # Ê¨°„ÅÆ„Éà„Éº„ÇØ„É≥„Å®„Åó„Å¶ÈÅ∏„Å∞„Çå„Åü„Çµ„Éñ„ÉØ„Éº„Éâ„ÅÆÁ¢∫Áéá„ÇíÂèñÂæó\n",
        "    next_token_list = [tokenizer.decode([idx]) for idx in next_token.tolist()]\n",
        "    next_token_prob = probabilities[next_token].tolist()\n",
        "\n",
        "    # Á¢∫ÁéáÂàÜÂ∏É„ÅÆ„ÇíÊ£í„Ç∞„É©„Éï„Å®„Åó„Å¶Êèè„Åè\n",
        "    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid(axis='y', linestyle='-', alpha=0.5)\n",
        "    if potential_nb < total_nb:\n",
        "      plt.axvline(x=potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')\n",
        "    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')\n",
        "    plt.bar(next_token_list, next_token_prob, color='red', label='Selected tokens')\n",
        "    plt.xticks(rotation=45, ha='right', va='top')\n",
        "    plt.gca().spines['top'].set_visible(False)\n",
        "    plt.gca().spines['right'].set_visible(False)\n",
        "    if sampling == 'top_k':\n",
        "      plt.title('Probability distribution of predicted tokens with top-k sampling')\n",
        "    elif sampling == 'nucleus':\n",
        "      plt.title('Probability distribution of predicted tokens with nucleus sampling')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilh2KsT5UU0v"
      },
      "source": [
        "### decode„ÅÆÂÆüË°å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd3kkhVS82gQ"
      },
      "outputs": [],
      "source": [
        "# „Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥Áî®„ÅÆ„Éá„Éº„Çø\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# ÂèØË¶ñÂåñ„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "length = 5\n",
        "\n",
        "# top-k„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "temperature = 5\n",
        "top_k = 20\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTdzJimoISJG"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'top_k', temperature=temperature, top_k=top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVE3LnnaIA16"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZeffMvEVCEf"
      },
      "source": [
        "### „Ç∞„É©„Éï„ÅÆÊèèÁîª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyop3NTp--4G"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDd_PePe_Itd"
      },
      "source": [
        "## üî¨ Nucleus sampling\n",
        "* ÂèÇËÄÉË≥áÊñô\n",
        " * https://www.youtube.com/watch?v=JETxaSaj6_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exsgQ3NQ8oTS"
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(logits, nucleus_p, num_beams, temperature=1.0, top_k=100, plot=False):\n",
        "  assert nucleus_p > 0\n",
        "  assert nucleus_p <= 1\n",
        "\n",
        "  # „Åæ„Åötop k„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„Å∏„Å®„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆÂØæË±°„ÇíÁµû„ÇäËæº„ÇÄ\n",
        "  logit_k = torch.topk(logits, top_k, dim=-1).values[:,-1]\n",
        "  indices_to_remove = logits < logit_k.unsqueeze(-1)\n",
        "  new_logits = torch.clone(logits)\n",
        "  new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # Ê¨°„Å´top p„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„Å∏„Å®„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆÂØæË±°„ÇíÁµû„ÇäËæº„ÇÄ\n",
        "  sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n",
        "  probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n",
        "  cumulative_probabilities = torch.cumsum(probabilities, dim=-1)\n",
        "\n",
        "  # Á¢∫Áéá„ÅÆÂíå„Åånucleus_p„ÇíË∂Ö„Åà„Çã„Å®„Åì„Çç„Åã„Çâ„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞„Åã„ÇâÈô§Â§ñ„Åô„Çã\n",
        "  # „Åü„Å†„Åó„ÄÅÊúÄ‰Ωé„Åß„ÇÇnum_beamsÂÄã„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„ÅØÊÆã„Åô\n",
        "  mask = cumulative_probabilities >= cumulative_probabilities[:,num_beams]\n",
        "  mask = torch.logical_and(mask, cumulative_probabilities > nucleus_p)\n",
        "  for i in range(logits.shape[0]):\n",
        "    new_logits[i,sorted_indices[i,mask[i]]] = float('-inf')\n",
        "\n",
        "  mask_p = cumulative_probabilities > nucleus_p\n",
        "\n",
        "  # logit„ÇíÁ¢∫Áéá„Å´Â§âÊèõ„Åô„Çã\n",
        "  probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "  # top p„ÅÆ„Çµ„Éñ„ÉØ„Éº„Éâ„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´num_beamsÂÄã„ÇíÈÅ∏„Å∂\n",
        "  next_tokens = torch.multinomial(probabilities, num_beams)\n",
        "\n",
        "  # Plot distribution\n",
        "  if plot:\n",
        "    total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)\n",
        "\n",
        "  return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv04nWH1LuFB"
      },
      "outputs": [],
      "source": [
        "# „Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥Áî®„ÅÆ„Éá„Éº„Çø\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# ÂèØË¶ñÂåñ„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "length = 5\n",
        "\n",
        "# top-k„Çµ„É≥„Éó„É™„É≥„Ç∞„ÅÆ„Éë„É©„É°„Éº„Çø\n",
        "temperature = 1\n",
        "nucleus_p = 0.5\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyBau3K3LeGY"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'nucleus', temperature=temperature, nucleus_p=nucleus_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6uTqcMnk0qb"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPHx7ZRL2641"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVr4_nPsqwVj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}