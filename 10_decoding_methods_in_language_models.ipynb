{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/10_decoding_methods_in_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgSATSBHqe_W"
      },
      "source": [
        "# テキスト生成におけるdecoding\n",
        "* 参考になる資料\n",
        "  * https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/\n",
        "* decodingアルゴリズムの可視化に関して参考にした資料\n",
        "  * https://mlabonne.github.io/blog/decoding/\n",
        "  * ↑この資料は、アルゴリズムの説明は正確でないため、注意。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**今日動かすコードの大部分は、decodingを説明するための可視化を実現するだけのコードです。**"
      ],
      "metadata": {
        "id": "zH_-rcLTP02r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8VKedOjlt_W"
      },
      "source": [
        "## 言語モデルを使ったテキスト生成\n",
        "* 今回は、言語モデルをテキスト生成に使う。\n",
        "* テキストを生成するアルゴリズム（decodingのアルゴリズム）は複数あることを学ぶ。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zApYZfF3c3"
      },
      "source": [
        "### テキストはどのように生成されるか？\n",
        "* 生成に使う言語モデルの最終的な出力は、語彙集合（subwordの集合）上に定義された確率分布である。\n",
        "  * つまり、各subwordに数値が割り振られていて、それらを合計すると1になるような数値の組。\n",
        "  * 分布の名前は、カテゴリカル分布 https://en.wikipedia.org/wiki/Categorical_distribution\n",
        "* 確率分布のパラメータの値は、次のトークンがそれぞれのsubwordのトークンになる確率を表している。\n",
        "  * トークン列の$i$番目の位置での、$w$番目の語彙の確率を、\n",
        "  $\\phi_{i,w}$と書くことする。\n",
        "  * 語彙集合上に定義された確率分布なので、$\\sum_{w=1}^W \\phi_{i,w} = 1$が成り立っている。\n",
        "* しかし、次の一つのトークンを生成するために、この確率分布をどのように使えばいいのだろうか？\n",
        "  * 例えば、確率最大のsubwordを、$\\mathop{\\arg\\max}_w \\phi_{i,w}$と、すべての場所$i=1,\\ldots,$で次々に選んでいく、というアルゴリズムは、良いアルゴリズムだろうか？\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 次の一つのトークンを選ぶアルゴリズムに、複数あることを今日は学ぶ。\n",
        "  * いずれのアルゴリズムを使うかで、生成されるテキストの様子が違ってくる。"
      ],
      "metadata": {
        "id": "3-qmcOnsMlw0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbhuFLTG_RW"
      },
      "source": [
        "**ランタイムのタイプはGPUにしておいてください。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW1IYEw0sBtc"
      },
      "source": [
        "## 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-TkVrykSCQz"
      },
      "source": [
        "### インストール"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKY1FX7beLim"
      },
      "source": [
        "* graphvizをインストールしようとして、utf8関係のエラーが出たら、以下のセルを実行してからインストールする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7aCeNp2eIQp"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iKAfvyNrYd9"
      },
      "source": [
        "* graphviz関係のライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As0ruNRPrV-S"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install graphviz graphviz-dev\n",
        "!pip install pygraphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVNRkLVHSFkk"
      },
      "source": [
        "### インポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhFveia6JmQH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R4IKg4lbNcN"
      },
      "source": [
        "## 言語モデル\n",
        "* 今回はGPT2を使う。\n",
        "  * 最初だけモデルのダウンロードに時間がかかる。\n",
        "* GPT2LMHeadModelとは・・・\n",
        "> The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "  * https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel\n",
        "\n",
        "* テキストを生成させるだけである（finetuningはしない）ので`eval()`メソッドを呼ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt6NhGjANTEm"
      },
      "source": [
        "### モデルの取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsvrVwdJsZuT"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_num = 0\n",
        "for param in model.parameters():\n",
        "  total_num += param.numel()\n",
        "print(total_num)"
      ],
      "metadata": {
        "id": "vjAGwokDU-NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHRHB4xJt7SY"
      },
      "source": [
        "* 語彙サイズを調べる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4osQpOPVt6PL"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H75oB9oouAqf"
      },
      "source": [
        "* 適当な単語のidを調べる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4hdLwJIuCr7"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab()[\"hello\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqMyKWKJuMol"
      },
      "source": [
        "* 適当なテキストをidの列に変換してみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Gd3xOjuPs-"
      },
      "outputs": [],
      "source": [
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeOVit1JstZg"
      },
      "source": [
        "* デフォルトの設定のもとでテキストの続きを生成させてみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJn0V5mmvK4c"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      max_length=100,\n",
        "      )\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB5Fns4SvOlO"
      },
      "source": [
        "* 生成されたid列をトークン列に変換する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LS5sCUPwzaD"
      },
      "outputs": [],
      "source": [
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Generated text:\\n\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB-nF44SxLnU"
      },
      "source": [
        "### モデルが出力する予測logitsの確認"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4boscktHxCz"
      },
      "source": [
        "* テキストを生成させるのではなく、モデルが直接出力する単語確率を取得する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJAvgFzewwNN"
      },
      "outputs": [],
      "source": [
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYTu8I3Pwy6b"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(input_ids)\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HteyJvl-IBx4"
      },
      "source": [
        "* `logit`は確率に変換される前の値。\n",
        " * その`shape`は[シーケンス数, シーケンス長, 語彙サイズ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j2Hg6M8w14E"
      },
      "outputs": [],
      "source": [
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0mAesGuyUIc"
      },
      "source": [
        "* 次のsubwordの予測logitを取得する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LnsYQ4dyBAr"
      },
      "outputs": [],
      "source": [
        "logits = outputs.logits[0,-1].cpu()\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "id": "uxTEYcp1PI8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPpNC7QSyjhm"
      },
      "source": [
        "* 確率最大のsubwordを調べる。\n",
        " * 確率最大のsubwordを次のトークンに選ぶのが、後で説明するgreedy searchアルゴリズム。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egCYdRHfKbXs"
      },
      "outputs": [],
      "source": [
        "torch.argmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVKOns7hysag"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(torch.argmax(logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKtQqhJNPtnM"
      },
      "source": [
        "## decodingの可視化の準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D7Orsd4L33c"
      },
      "source": [
        "### 対数確率を求めるヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEgpaxpSL22C"
      },
      "outputs": [],
      "source": [
        "def get_log_prob(logits, token_id):\n",
        "  log_probabilities = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "  token_log_probability = log_probabilities[token_id].item()\n",
        "  return token_log_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkrbJCG5OeQh"
      },
      "source": [
        "### グラフ構造を可視化するヘルパ関数\n",
        "* この関数は、networkxライブラリを使って作られたグラフ構造を、可視化する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMNyl5maqTc"
      },
      "outputs": [],
      "source": [
        "def plot_graph(graph, length, beams, score_type):\n",
        "  fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')\n",
        "\n",
        "  # ノードのレイアウトを確定させる\n",
        "  pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
        "\n",
        "  # 確率に応じてノードの色を決める\n",
        "  assert score_type in ['token', 'sequence']\n",
        "  scores = [\n",
        "      data[score_type + 'score']\n",
        "      for _, data in graph.nodes(data=True)\n",
        "      if data['token'] is not None\n",
        "      ]\n",
        "  vmin = min(scores)\n",
        "  vmax = max(scores)\n",
        "  norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
        "  cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256)\n",
        "\n",
        "  # ノードを描く\n",
        "  nx.draw_networkx_nodes(\n",
        "      graph, pos, node_size=2000, node_shape='o',\n",
        "      alpha=1, linewidths=4,\n",
        "      node_color=scores, cmap=cmap,\n",
        "      )\n",
        "\n",
        "  # エッジを描く\n",
        "  nx.draw_networkx_edges(graph, pos)\n",
        "\n",
        "  # ラベルを描く\n",
        "  suffix = \"%\" if score_type == \"token\" else \"\"\n",
        "  labels = {\n",
        "      node: data['token'].split('_')[0] + f\"\\n{data[score_type + 'score']:.2f}\" + suffix\n",
        "      for node, data in graph.nodes(data=True)\n",
        "      if data['token'] is not None\n",
        "      }\n",
        "  nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n",
        "  plt.box(False)\n",
        "\n",
        "  # 確率の高低を表すカラーバーを追加する\n",
        "  sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "  sm.set_array([])\n",
        "  if score_type == 'token':\n",
        "    label = 'Token probability (%)'\n",
        "  elif score_type == 'sequence':\n",
        "    label = 'Sequence score'\n",
        "  fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label=label)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7mElwjbPYW"
      },
      "source": [
        "## (0) Greedy Search\n",
        "* その都度確率最大のトークンを選ぶという、最もシンプルなdecoding。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy3681h6AJlS"
      },
      "source": [
        "### データとパラメータの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu9hB8smMD-Z"
      },
      "outputs": [],
      "source": [
        "# デモンストレーション用のデータ\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# 可視化のパラメータ\n",
        "length = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxS7QzKMQn1r"
      },
      "source": [
        "### decodeの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm_AiUhK8yrP"
      },
      "outputs": [],
      "source": [
        "# ノードが一つのグラフを作成\n",
        "graph = nx.DiGraph()\n",
        "graph.add_node(graph.number_of_nodes())\n",
        "node = graph.number_of_nodes() - 1\n",
        "\n",
        "# グラフの根ノードが持つ情報を初期化する\n",
        "graph.nodes[node]['tokenscore'] = 100\n",
        "graph.nodes[node]['token'] = text\n",
        "\n",
        "# 根ノードを出発点に設定する\n",
        "node = 0\n",
        "\n",
        "for n in range(length):\n",
        "\n",
        "  # モデルが出力するlogitsを得る\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "  predictions = outputs.logits\n",
        "\n",
        "  # 次のトークンを予測するlogitだけを取り出す\n",
        "  logits = predictions[0][-1]\n",
        "\n",
        "  # 確率最大のサブワードを得て、その確率の対数をとったものを求める。\n",
        "  token_id = torch.argmax(logits).unsqueeze(0)\n",
        "  token_score = get_log_prob(logits, token_id)\n",
        "\n",
        "  # 可視化用のグラフに選ばれたトークンの情報を追加する。\n",
        "  next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "  graph.add_node(graph.number_of_nodes())\n",
        "  new_node = graph.number_of_nodes() - 1\n",
        "  graph.add_edge(node, new_node)\n",
        "  node = new_node\n",
        "  graph.nodes[node]['tokenscore'] = np.exp(token_score) * 100\n",
        "  graph.nodes[node]['token'] = next_token + f\"_{length}\"\n",
        "\n",
        "  # 選ばれたサブワードを入力のトークン列に継ぎ足す。\n",
        "  input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "output = tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "print(f\"Generated text: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BS_w2dZWG5y"
      },
      "source": [
        "### グラフの描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSt2J6qBOlPf"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, 1.5, 'token')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J16z-Mw295Jx"
      },
      "source": [
        "## (1) Beam Search\n",
        "* 複数の候補を残しつつトークンを生成していく。\n",
        "* ある程度生成を進めたところでベストなトークン列を選ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfhkM6-ULkq"
      },
      "source": [
        "### テキスト生成のヘルパ関数\n",
        "* ほとんどの処理が全てのアルゴリズムに共通している。\n",
        "* サンプリングの方法だけが異なる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTDkrVdutCk1"
      },
      "outputs": [],
      "source": [
        "def decoding(ids, length, num_beams, sampling, plot=False, temperature=0.1, top_k=20, nucleus_p=0.5):\n",
        "\n",
        "  input_ids = ids\n",
        "\n",
        "  # ノードが一つのグラフを作成\n",
        "  graph = nx.DiGraph()\n",
        "  graph.add_node(graph.number_of_nodes())\n",
        "  node = graph.number_of_nodes() - 1\n",
        "\n",
        "  # グラフの根ノードが持つ情報を初期化する\n",
        "  graph.nodes[node]['tokenscore'] = 100\n",
        "  graph.nodes[node]['score'] = 0\n",
        "  graph.nodes[node]['sequencescore'] = 0\n",
        "  graph.nodes[node]['token'] = text\n",
        "\n",
        "  # 根ノードを出発点に設定する\n",
        "  nodes = [node]\n",
        "\n",
        "  for n in range(length):\n",
        "    # モデルが出力するlogitsを得る\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "    # 次のトークンを予測するlogitだけを取り出す\n",
        "    logits = predictions[:,-1]\n",
        "\n",
        "    if sampling == 'deterministic':\n",
        "      top_token_ids = torch.topk(logits, num_beams, dim=-1).indices\n",
        "    elif sampling == 'top_k':\n",
        "      top_token_ids = top_k_sampling(logits, top_k, num_beams, plot=plot, temperature=temperature)\n",
        "    elif sampling == 'nucleus':\n",
        "      top_token_ids = nucleus_sampling(logits, nucleus_p, num_beams, plot=plot, temperature=temperature)\n",
        "\n",
        "    assert len(nodes) == input_ids.shape[0]\n",
        "\n",
        "    # 各シーケンスの末尾に選ばれたトークンを追加する\n",
        "    node_scores = list()\n",
        "    for i in range(input_ids.shape[0]):\n",
        "      node = nodes[i]\n",
        "      for j, token_id in enumerate(top_token_ids[i]):\n",
        "\n",
        "        # 予測されたトークンの対数確率を求めて、ここまでのシーケンスの対数確率に加算する\n",
        "        token_score = get_log_prob(logits[i], token_id)\n",
        "\n",
        "        graph.add_node(graph.number_of_nodes())\n",
        "        new_node = graph.number_of_nodes() - 1\n",
        "        graph.add_edge(node, new_node)\n",
        "\n",
        "        cumulative_score = graph.nodes[node]['score'] + token_score\n",
        "        graph.nodes[new_node]['score'] = cumulative_score\n",
        "        graph.nodes[new_node]['tokenscore'] = np.exp(token_score) * 100\n",
        "        graph.nodes[new_node]['sequencescore'] = cumulative_score / (input_ids.shape[1] + 1)\n",
        "        token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "        graph.nodes[new_node]['token'] = token + f\"_{n}_{i}\"\n",
        "        node_scores.append((new_node, cumulative_score, i, token_id, token_score))\n",
        "\n",
        "    # スコアの降順にノードをソート\n",
        "    node_scores.sort(key=lambda a: a[1], reverse=True)\n",
        "\n",
        "    new_nodes = list()\n",
        "    new_input_ids = list()\n",
        "    for i, node_score in enumerate(node_scores[:num_beams]):\n",
        "      new_node, _, sequence_id, token_id, token_score = node_score\n",
        "      print(f\"node id: {new_node}, token score:{token_score:.3f},\",\n",
        "            \"token:\", tokenizer.decode(token_id, skip_special_tokens=True))\n",
        "      # 予測されたトークンを、ここまでのシーケンスの末尾に繋げる。\n",
        "      new_nodes.append(new_node)\n",
        "      new_input_ids.append(\n",
        "          torch.cat([input_ids[sequence_id], token_id.unsqueeze(0)],\n",
        "                    dim=-1)\n",
        "          )\n",
        "\n",
        "    nodes = new_nodes\n",
        "    input_ids = torch.stack(new_input_ids)\n",
        "\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgiRt9aSGTQ"
      },
      "source": [
        "### データとパラメータの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQb-UURwQYTZ"
      },
      "outputs": [],
      "source": [
        "# デモンストレーション用のデータ\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# 可視化のパラメータ\n",
        "length = 5\n",
        "\n",
        "# ビームサーチのパラメータ\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxTnt6nPT5Kv"
      },
      "source": [
        "### decodeの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7_Ui3msGFdf"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'deterministic', plot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suVgbNiYHs0f"
      },
      "source": [
        "### スコア最大のトークン列を取得するヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V38YsPv8F1Fy"
      },
      "outputs": [],
      "source": [
        "def get_best_sequence(graph):\n",
        "  depths = nx.shortest_path_length(graph, 0)\n",
        "  max_depth = max(depths.values())\n",
        "  leaf_nodes = [node for node in graph.nodes if depths[node] == max_depth]\n",
        "  max_score_node = None\n",
        "  max_score = float('-inf')\n",
        "  for node in leaf_nodes:\n",
        "    if graph.nodes[node]['sequencescore'] > max_score:\n",
        "      max_score = graph.nodes[node]['sequencescore']\n",
        "      max_score_node = node\n",
        "    path = nx.shortest_path(graph, source=0, target=max_score_node)\n",
        "    sequence = \"\".join([graph.nodes[node]['token'].split('_')[0] for node in path])\n",
        "  return sequence, max_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HrqbxZzGWu-"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoOyhqpcVWI3"
      },
      "source": [
        "### グラフの描画\n",
        "* 示されているスコアは、そのノードに至るまでのトークン列のスコア。\n",
        "* ビームサーチは、最もスコアの大きいトークン列を選ぶ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPbd4dw0J9k"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmObi_P6bR0"
      },
      "source": [
        "## (2) Top-k sampling\n",
        "* top-kサンプリングは、確率が高いk個のサブワードから・・・\n",
        "* それらの確率に従ってランダムに次のトークンを選ぶ。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzeaPdKJV5XY"
      },
      "source": [
        "### サンプリングを行う関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzZLWF_4V2cn"
      },
      "outputs": [],
      "source": [
        "def top_k_sampling(logits, top_k, num_beams, temperature=1.0, plot=False):\n",
        "  assert top_k >= 1\n",
        "  assert num_beams <= top_k\n",
        "\n",
        "  logit_k = torch.topk(logits, top_k, dim=-1).values[:,-1]\n",
        "  indices_to_remove = logits < logit_k.unsqueeze(-1)\n",
        "  new_logits = torch.clone(logits)\n",
        "  new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # logitを確率に変換する\n",
        "  probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "  # top-kのサブワードからランダムにbeams個を選ぶ\n",
        "  next_tokens = torch.multinomial(probabilities, num_beams)\n",
        "\n",
        "  if plot:\n",
        "    # サブワードを選ぶときに使った確率分布を描画する\n",
        "    total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)\n",
        "\n",
        "  return next_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bxyiGsVUYy9"
      },
      "source": [
        "### サブワードの確率分布を描画するヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsDJWQs-tav"
      },
      "outputs": [],
      "source": [
        "def plot_prob_distribution(total_prob, next_tokens, sampling, potential_nb, total_nb=50):\n",
        "  for i in range(total_prob.shape[0]):\n",
        "    probabilities = total_prob[i]\n",
        "    next_token = next_tokens[i]\n",
        "    # top kのサブワードを取得\n",
        "    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)\n",
        "    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]\n",
        "\n",
        "    # 次のトークンとして選ばれたサブワードの確率を取得\n",
        "    next_token_list = [tokenizer.decode([idx]) for idx in next_token.tolist()]\n",
        "    next_token_prob = probabilities[next_token].tolist()\n",
        "\n",
        "    # 確率分布のを棒グラフとして描く\n",
        "    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid(axis='y', linestyle='-', alpha=0.5)\n",
        "    temp_potential_nb = potential_nb\n",
        "    if isinstance(potential_nb, list):\n",
        "      temp_potential_nb = potential_nb[i]\n",
        "    if temp_potential_nb < total_nb:\n",
        "      plt.axvline(x=temp_potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')\n",
        "    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')\n",
        "    plt.bar(next_token_list, next_token_prob, color='red', label='Selected tokens')\n",
        "    plt.xticks(rotation=45, ha='right', va='top')\n",
        "    plt.gca().spines['top'].set_visible(False)\n",
        "    plt.gca().spines['right'].set_visible(False)\n",
        "    if sampling == 'top_k':\n",
        "      plt.title('Probability distribution of predicted tokens with top-k sampling')\n",
        "    elif sampling == 'nucleus':\n",
        "      plt.title('Probability distribution of predicted tokens with nucleus sampling')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilh2KsT5UU0v"
      },
      "source": [
        "### decodeの実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd3kkhVS82gQ"
      },
      "outputs": [],
      "source": [
        "# デモンストレーション用のデータ\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# 可視化のパラメータ\n",
        "length = 5\n",
        "\n",
        "temperature = 5\n",
        "num_beams = 2\n",
        "\n",
        "# top-kサンプリングのパラメータ\n",
        "top_k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTdzJimoISJG"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'top_k', plot=True, temperature=temperature, top_k=top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVE3LnnaIA16"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZeffMvEVCEf"
      },
      "source": [
        "### グラフの描画"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyop3NTp--4G"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDd_PePe_Itd"
      },
      "source": [
        "## (3) Nucleus sampling\n",
        "* 参考資料\n",
        " * https://www.youtube.com/watch?v=JETxaSaj6_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exsgQ3NQ8oTS"
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(logits, nucleus_p, num_beams, temperature=1.0, top_k=100, plot=False):\n",
        "  assert nucleus_p > 0\n",
        "  assert nucleus_p <= 1\n",
        "\n",
        "  # まずtop kのサブワードへとサンプリングの対象を絞り込む\n",
        "  logit_k = torch.topk(logits, top_k, dim=-1).values[:,-1]\n",
        "  indices_to_remove = logits < logit_k.unsqueeze(-1)\n",
        "  new_logits = torch.clone(logits)\n",
        "  new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # 次にtop pのサブワードへとサンプリングの対象を絞り込む\n",
        "  sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n",
        "  probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n",
        "  cumulative_probabilities = torch.cumsum(probabilities, dim=-1)\n",
        "  print(cumulative_probabilities.shape)\n",
        "\n",
        "  # 確率の和がnucleus_pを超えるところからのサブワードをサンプリングから除外する\n",
        "  # ただし、最低でもnum_beams個のサブワードは残す\n",
        "  mask = cumulative_probabilities >= cumulative_probabilities[:,num_beams:num_beams+1]\n",
        "  mask = torch.logical_and(mask, cumulative_probabilities > nucleus_p)\n",
        "  top_p_index_to_keep = list()\n",
        "  for i in range(logits.shape[0]):\n",
        "    new_logits[i,sorted_indices[i,mask[i]]] = float('-inf')\n",
        "    top_p_index_to_keep.append(len(mask[i]) - mask[i].sum().item())\n",
        "\n",
        "  # logitを確率に変換する\n",
        "  probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "  # top pのサブワードからランダムにnum_beams個を選ぶ\n",
        "  next_tokens = torch.multinomial(probabilities, num_beams)\n",
        "\n",
        "  # Plot distribution\n",
        "  if plot:\n",
        "    total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)\n",
        "\n",
        "  return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv04nWH1LuFB"
      },
      "outputs": [],
      "source": [
        "# デモンストレーション用のデータ\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# 可視化のパラメータ\n",
        "length = 5\n",
        "\n",
        "temperature = 1\n",
        "num_beams = 2\n",
        "\n",
        "# nucleusサンプリングのパラメータ\n",
        "nucleus_p = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyBau3K3LeGY"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'nucleus', plot=True, temperature=temperature, nucleus_p=nucleus_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6uTqcMnk0qb"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPHx7ZRL2641"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVr4_nPsqwVj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}