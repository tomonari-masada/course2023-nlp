{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/10_decoding_methods_in_language_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgSATSBHqe_W"
      },
      "source": [
        "# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãŠã‘ã‚‹decoding\n",
        "* ãã®ä»–ã®å‚è€ƒã«ãªã‚‹è³‡æ–™\n",
        " * https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/\n",
        "* å¯è¦–åŒ–ã«é–¢ã—ã¦å‚è€ƒã«ã—ãŸè³‡æ–™\n",
        " * https://mlabonne.github.io/blog/decoding/\n",
        " * ã“ã®è³‡æ–™ã¯ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®èª¬æ˜Žã¯æ­£ç¢ºã§ãªã„ãŸã‚ã€æ³¨æ„ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8VKedOjlt_W"
      },
      "source": [
        "## è¨€èªžãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
        "* ä»Šå›žã¯ã€è¨€èªžãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ä½¿ã†ã€‚\n",
        "* ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆdecodingã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ã¯è¤‡æ•°ã‚ã‚‹ã“ã¨ã‚’å­¦ã¶ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zApYZfF3c3"
      },
      "source": [
        "### ãƒ†ã‚­ã‚¹ãƒˆã¯ã©ã®ã‚ˆã†ã«ç”Ÿæˆã•ã‚Œã‚‹ã‹ï¼Ÿ\n",
        "* ç”Ÿæˆã«ä½¿ã†è¨€èªžãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚çš„ãªå‡ºåŠ›ã¯ã€èªžå½™é›†åˆä¸Šã«å®šç¾©ã•ã‚ŒãŸç¢ºçŽ‡åˆ†å¸ƒã€‚\n",
        " * ã¤ã¾ã‚Šã€å„å˜èªžã«æ•°å€¤ãŒå‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ã„ã¦ã€ãã‚Œã‚‰ã‚’åˆè¨ˆã™ã‚‹ã¨1ã«ãªã‚‹ã€‚\n",
        " * ã“ã‚Œã‚‰ã®æ•°å€¤ã¯ã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒãã®å˜èªžã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ãªã‚‹ç¢ºçŽ‡ã‚’è¡¨ã—ã¦ã„ã‚‹ã€‚\n",
        "* ã—ã‹ã—ã€æ¬¡ã®ä¸€ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã€ã“ã®ç¢ºçŽ‡åˆ†å¸ƒã‚’ã©ã®ã‚ˆã†ã«ä½¿ã†ã®ã ã‚ã†ã‹ï¼Ÿ\n",
        " * ç¢ºçŽ‡æœ€å¤§ã®å˜èªžã‚’æ¬¡ã€…ã«é¸ã¶ã¨ã„ã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€è‰¯ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã ã‚ã†ã‹ï¼Ÿ\n",
        "* æ¬¡ã®ä¸€ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸ã¶ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã€è¤‡æ•°ã‚ã‚‹ã“ã¨ã‚’ä»Šæ—¥ã¯å­¦ã¶ã€‚\n",
        " * ã„ãšã‚Œã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ã†ã‹ã§ã€ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æ§˜å­ãŒé•ã£ã¦ãã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbhuFLTG_RW"
      },
      "source": [
        "**ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã¯GPUã«ã—ã¦ãŠã„ã¦ãã ã•ã„ã€‚**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW1IYEw0sBtc"
      },
      "source": [
        "## æº–å‚™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-TkVrykSCQz"
      },
      "source": [
        "### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKY1FX7beLim"
      },
      "source": [
        "* graphvizã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã‚ˆã†ã¨ã—ã¦ã€utf8é–¢ä¿‚ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7aCeNp2eIQp"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iKAfvyNrYd9"
      },
      "source": [
        "* graphvizã¨transformersã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As0ruNRPrV-S"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install graphviz graphviz-dev\n",
        "!pip install transformers pygraphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVNRkLVHSFkk"
      },
      "source": [
        "### ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhFveia6JmQH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R4IKg4lbNcN"
      },
      "source": [
        "## è¨€èªžãƒ¢ãƒ‡ãƒ«\n",
        "* ä»Šå›žã¯GPT2ã‚’ä½¿ã†ã€‚\n",
        " * æœ€åˆã ã‘ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã€‚\n",
        "* GPT2LMHeadModelã¨ã¯\n",
        "\"The GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\"\n",
        " * https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel\n",
        "\n",
        "* ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã•ã›ã‚‹ã ã‘ãªã®ã§`eval()`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã¶ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt6NhGjANTEm"
      },
      "source": [
        "### ãƒ¢ãƒ‡ãƒ«ã®å–å¾—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsvrVwdJsZuT"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHRHB4xJt7SY"
      },
      "source": [
        "* èªžå½™ã‚µã‚¤ã‚ºã‚’èª¿ã¹ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4osQpOPVt6PL"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H75oB9oouAqf"
      },
      "source": [
        "* é©å½“ãªå˜èªžã®idã‚’èª¿ã¹ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4hdLwJIuCr7"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab()[\"hello\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqMyKWKJuMol"
      },
      "source": [
        "* é©å½“ãªãƒ†ã‚­ã‚¹ãƒˆã‚’idã®åˆ—ã«å¤‰æ›ã—ã¦ã¿ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Gd3xOjuPs-"
      },
      "outputs": [],
      "source": [
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeOVit1JstZg"
      },
      "source": [
        "* ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨­å®šã®ã‚‚ã¨ã§ãƒ†ã‚­ã‚¹ãƒˆã®ç¶šãã‚’ç”Ÿæˆã•ã›ã¦ã¿ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJn0V5mmvK4c"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    max_length=100,\n",
        "    )\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB5Fns4SvOlO"
      },
      "source": [
        "* ç”Ÿæˆã•ã‚ŒãŸidåˆ—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¤‰æ›ã™ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LS5sCUPwzaD"
      },
      "outputs": [],
      "source": [
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Generated text:\\n\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB-nF44SxLnU"
      },
      "source": [
        "### ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã™ã‚‹äºˆæ¸¬logitsã®ç¢ºèª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJAvgFzewwNN"
      },
      "outputs": [],
      "source": [
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4boscktHxCz"
      },
      "source": [
        "* ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã•ã›ã‚‹ã®ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ãŒç›´æŽ¥å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’èª¿ã¹ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYTu8I3Pwy6b"
      },
      "outputs": [],
      "source": [
        "outputs = model(input_ids)\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HteyJvl-IBx4"
      },
      "source": [
        "* `logit`ã¯ç¢ºçŽ‡ã«å¤‰æ›ã•ã‚Œã‚‹å‰ã®å€¤ã€‚\n",
        " * ãã®`shape`ã¯[ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°, ãƒˆãƒ¼ã‚¯ãƒ³æ•°, èªžå½™ã‚µã‚¤ã‚º]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j2Hg6M8w14E"
      },
      "outputs": [],
      "source": [
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0mAesGuyUIc"
      },
      "source": [
        "* æ¬¡ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®äºˆæ¸¬logitã‚’å–å¾—ã™ã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LnsYQ4dyBAr"
      },
      "outputs": [],
      "source": [
        "logits = outputs.logits[0,-1]\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPpNC7QSyjhm"
      },
      "source": [
        "* ç¢ºçŽ‡æœ€å¤§ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’èª¿ã¹ã‚‹ã€‚\n",
        " * ç¢ºçŽ‡æœ€å¤§ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é¸ã¶ã®ãŒã€greedy searchã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egCYdRHfKbXs"
      },
      "outputs": [],
      "source": [
        "torch.argmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVKOns7hysag"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(torch.argmax(logits))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKtQqhJNPtnM"
      },
      "source": [
        "## decodingã®å¯è¦–åŒ–ã®æº–å‚™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D7Orsd4L33c"
      },
      "source": [
        "### å¯¾æ•°ç¢ºçŽ‡ã‚’æ±‚ã‚ã‚‹ãƒ˜ãƒ«ãƒ‘é–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEgpaxpSL22C"
      },
      "outputs": [],
      "source": [
        "def get_log_prob(logits, token_id):\n",
        "  log_probabilities = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "  token_log_probability = log_probabilities[token_id].item()\n",
        "  return token_log_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkrbJCG5OeQh"
      },
      "source": [
        "### ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’å¯è¦–åŒ–ã™ã‚‹ãƒ˜ãƒ«ãƒ‘é–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeMNyl5maqTc"
      },
      "outputs": [],
      "source": [
        "def plot_graph(graph, length, beams, score_type):\n",
        "  fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')\n",
        "\n",
        "  # ãƒŽãƒ¼ãƒ‰ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ç¢ºå®šã•ã›ã‚‹\n",
        "  pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
        "\n",
        "  # ç¢ºçŽ‡ã«å¿œã˜ã¦ãƒŽãƒ¼ãƒ‰ã®è‰²ã‚’æ±ºã‚ã‚‹\n",
        "  assert score_type in ['token', 'sequence']\n",
        "  scores = [\n",
        "      data[score_type + 'score']\n",
        "      for _, data in graph.nodes(data=True)\n",
        "      if data['token'] is not None\n",
        "      ]\n",
        "  vmin = min(scores)\n",
        "  vmax = max(scores)\n",
        "  norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
        "  cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256)\n",
        "\n",
        "  # ãƒŽãƒ¼ãƒ‰ã‚’æã\n",
        "  nx.draw_networkx_nodes(\n",
        "      graph, pos, node_size=2000, node_shape='o',\n",
        "      alpha=1, linewidths=4,\n",
        "      node_color=scores, cmap=cmap,\n",
        "      )\n",
        "\n",
        "  # ã‚¨ãƒƒã‚¸ã‚’æã\n",
        "  nx.draw_networkx_edges(graph, pos)\n",
        "\n",
        "  # ãƒ©ãƒ™ãƒ«ã‚’æã\n",
        "  suffix = \"%\" if score_type == \"token\" else \"\"\n",
        "  labels = {\n",
        "      node: data['token'].split('_')[0] + f\"\\n{data[score_type + 'score']:.2f}\" + suffix\n",
        "      for node, data in graph.nodes(data=True)\n",
        "      if data['token'] is not None\n",
        "      }\n",
        "  nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n",
        "  plt.box(False)\n",
        "\n",
        "  # ç¢ºçŽ‡ã®é«˜ä½Žã‚’è¡¨ã™ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã‚’è¿½åŠ ã™ã‚‹\n",
        "  sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "  sm.set_array([])\n",
        "  if score_type == 'token':\n",
        "    label = 'Token probability (%)'\n",
        "  elif score_type == 'sequence':\n",
        "    label = 'Sequence score'\n",
        "  fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label=label)\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw7mElwjbPYW"
      },
      "source": [
        "## ðŸƒâ€â™‚ï¸ Greedy Search\n",
        "* ãã®éƒ½åº¦ç¢ºçŽ‡æœ€å¤§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸ã¶ã¨ã„ã†ã€æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªdecodingã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy3681h6AJlS"
      },
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu9hB8smMD-Z"
      },
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# å¯è¦–åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "length = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxS7QzKMQn1r"
      },
      "source": [
        "### decodeã®å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm_AiUhK8yrP"
      },
      "outputs": [],
      "source": [
        "# ãƒŽãƒ¼ãƒ‰ãŒä¸€ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
        "graph = nx.DiGraph()\n",
        "graph.add_node(graph.number_of_nodes())\n",
        "node = graph.number_of_nodes() - 1\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã®æ ¹ãƒŽãƒ¼ãƒ‰ãŒæŒã¤æƒ…å ±ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
        "graph.nodes[node]['tokenscore'] = 100\n",
        "graph.nodes[node]['token'] = text\n",
        "\n",
        "# æ ¹ãƒŽãƒ¼ãƒ‰ã‚’å‡ºç™ºç‚¹ã«è¨­å®šã™ã‚‹\n",
        "node = 0\n",
        "\n",
        "for n in range(length):\n",
        "\n",
        "  # ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã™ã‚‹logitsã‚’å¾—ã‚‹\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "  predictions = outputs.logits\n",
        "\n",
        "  # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹logitã ã‘ã‚’å–ã‚Šå‡ºã™\n",
        "  logits = predictions[0][-1]\n",
        "\n",
        "  # ç¢ºçŽ‡æœ€å¤§ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’å¾—ã¦ã€ãã®ç¢ºçŽ‡ã®å¯¾æ•°ã‚’ã¨ã£ãŸã‚‚ã®ã‚’æ±‚ã‚ã‚‹ã€‚\n",
        "  token_id = torch.argmax(logits).unsqueeze(0)\n",
        "  token_score = get_log_prob(logits, token_id)\n",
        "\n",
        "  # å¯è¦–åŒ–ç”¨ã®ã‚°ãƒ©ãƒ•ã«é¸ã°ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹ã€‚\n",
        "  next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "  graph.add_node(graph.number_of_nodes())\n",
        "  new_node = graph.number_of_nodes() - 1\n",
        "  graph.add_edge(node, new_node)\n",
        "  node = new_node\n",
        "  graph.nodes[node]['tokenscore'] = np.exp(token_score) * 100\n",
        "  graph.nodes[node]['token'] = next_token + f\"_{length}\"\n",
        "\n",
        "  # é¸ã°ã‚ŒãŸã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«ç¶™ãŽè¶³ã™ã€‚\n",
        "  input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "output = tokenizer.decode(input_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "print(f\"Generated text: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BS_w2dZWG5y"
      },
      "source": [
        "### ã‚°ãƒ©ãƒ•ã®æç”»"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSt2J6qBOlPf"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, 1.5, 'token')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J16z-Mw295Jx"
      },
      "source": [
        "## âš–ï¸ Beam Search\n",
        "* è¤‡æ•°ã®å€™è£œã‚’æ®‹ã—ã¤ã¤ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ã¦ã„ãã€‚\n",
        "* ã‚ã‚‹ç¨‹åº¦ç”Ÿæˆã‚’é€²ã‚ãŸã¨ã“ã‚ã§ãƒ™ã‚¹ãƒˆãªãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’é¸ã¶ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTfhkM6-ULkq"
      },
      "source": [
        "### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãƒ˜ãƒ«ãƒ‘é–¢æ•°\n",
        "* ã»ã¨ã‚“ã©ã®å‡¦ç†ãŒå…¨ã¦ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å…±é€šã—ã¦ã„ã‚‹ã€‚\n",
        "* ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æ–¹æ³•ã ã‘ãŒç•°ãªã‚‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTDkrVdutCk1"
      },
      "outputs": [],
      "source": [
        "def decoding(ids, length, num_beams, sampling, temperature=0.1, top_k=20, nucleus_p=0.5):\n",
        "\n",
        "  input_ids = ids\n",
        "\n",
        "  # ãƒŽãƒ¼ãƒ‰ãŒä¸€ã¤ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
        "  graph = nx.DiGraph()\n",
        "  graph.add_node(graph.number_of_nodes())\n",
        "  node = graph.number_of_nodes() - 1\n",
        "\n",
        "  # ã‚°ãƒ©ãƒ•ã®æ ¹ãƒŽãƒ¼ãƒ‰ãŒæŒã¤æƒ…å ±ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
        "  graph.nodes[node]['tokenscore'] = 100\n",
        "  graph.nodes[node]['score'] = 0\n",
        "  graph.nodes[node]['sequencescore'] = 0\n",
        "  graph.nodes[node]['token'] = text\n",
        "\n",
        "  # æ ¹ãƒŽãƒ¼ãƒ‰ã‚’å‡ºç™ºç‚¹ã«è¨­å®šã™ã‚‹\n",
        "  nodes = [node]\n",
        "\n",
        "  for n in range(length):\n",
        "    # ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã™ã‚‹logitsã‚’å¾—ã‚‹\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "    # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹logitã ã‘ã‚’å–ã‚Šå‡ºã™\n",
        "    logits = predictions[:,-1]\n",
        "\n",
        "    if sampling == 'greedy':\n",
        "      top_token_ids = torch.topk(logits, num_beams, dim=-1).indices\n",
        "    elif sampling == 'top_k':\n",
        "      top_token_ids = top_k_sampling(logits, top_k, num_beams, temperature=temperature)\n",
        "    elif sampling == 'nucleus':\n",
        "      top_token_ids = nucleus_sampling(logits, nucleus_p, num_beams, temperature=temperature)\n",
        "\n",
        "    assert len(nodes) == input_ids.shape[0]\n",
        "\n",
        "    # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ«å°¾ã«é¸ã°ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã™ã‚‹\n",
        "    node_scores = list()\n",
        "    for i in range(input_ids.shape[0]):\n",
        "      node = nodes[i]\n",
        "      for j, token_id in enumerate(top_token_ids[i]):\n",
        "\n",
        "        # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®å¯¾æ•°ç¢ºçŽ‡ã‚’æ±‚ã‚ã¦ã€ã“ã“ã¾ã§ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å¯¾æ•°ç¢ºçŽ‡ã«åŠ ç®—ã™ã‚‹\n",
        "        token_score = get_log_prob(logits[i], token_id)\n",
        "\n",
        "        graph.add_node(graph.number_of_nodes())\n",
        "        new_node = graph.number_of_nodes() - 1\n",
        "        graph.add_edge(node, new_node)\n",
        "\n",
        "        cumulative_score = graph.nodes[node]['score'] + token_score\n",
        "        graph.nodes[new_node]['score'] = cumulative_score\n",
        "        graph.nodes[new_node]['tokenscore'] = np.exp(token_score) * 100\n",
        "        graph.nodes[new_node]['sequencescore'] = cumulative_score / (input_ids.shape[1] + 1)\n",
        "        token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "        graph.nodes[new_node]['token'] = token + f\"_{n}_{i}\"\n",
        "        node_scores.append((new_node, cumulative_score, i, token_id, token_score))\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢ã®é™é †ã«ãƒŽãƒ¼ãƒ‰ã‚’ã‚½ãƒ¼ãƒˆ\n",
        "    node_scores.sort(key=lambda a: a[1], reverse=True)\n",
        "\n",
        "    new_nodes = list()\n",
        "    new_input_ids = list()\n",
        "    for i, node_score in enumerate(node_scores[:num_beams]):\n",
        "      new_node, _, sequence_id, token_id, token_score = node_score\n",
        "      print(f\"node id: {new_node}, token score:{token_score:.3f},\",\n",
        "            \"token:\", tokenizer.decode(token_id, skip_special_tokens=True))\n",
        "      # äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€ã“ã“ã¾ã§ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ«å°¾ã«ç¹‹ã’ã‚‹ã€‚\n",
        "      new_nodes.append(new_node)\n",
        "      new_input_ids.append(\n",
        "          torch.cat([input_ids[sequence_id], token_id.unsqueeze(0)],\n",
        "                    dim=-1)\n",
        "          )\n",
        "\n",
        "    nodes = new_nodes\n",
        "    input_ids = torch.stack(new_input_ids)\n",
        "\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PgiRt9aSGTQ"
      },
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQb-UURwQYTZ"
      },
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# å¯è¦–åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "length = 5\n",
        "\n",
        "# ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxTnt6nPT5Kv"
      },
      "source": [
        "### decodeã®å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7_Ui3msGFdf"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'greedy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suVgbNiYHs0f"
      },
      "source": [
        "### ã‚¹ã‚³ã‚¢æœ€å¤§ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’å–å¾—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘é–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V38YsPv8F1Fy"
      },
      "outputs": [],
      "source": [
        "def get_best_sequence(graph):\n",
        "  depths = nx.shortest_path_length(graph, 0)\n",
        "  max_depth = max(depths.values())\n",
        "  leaf_nodes = [node for node in graph.nodes if depths[node] == max_depth]\n",
        "  max_score_node = None\n",
        "  max_score = float('-inf')\n",
        "  for node in leaf_nodes:\n",
        "    if graph.nodes[node]['sequencescore'] > max_score:\n",
        "      max_score = graph.nodes[node]['sequencescore']\n",
        "      max_score_node = node\n",
        "    path = nx.shortest_path(graph, source=0, target=max_score_node)\n",
        "    sequence = \"\".join([graph.nodes[node]['token'].split('_')[0] for node in path])\n",
        "  return sequence, max_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HrqbxZzGWu-"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoOyhqpcVWI3"
      },
      "source": [
        "### ã‚°ãƒ©ãƒ•ã®æç”»\n",
        "* ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚¹ã‚³ã‚¢ã¯ã€ãã®ãƒŽãƒ¼ãƒ‰ã«è‡³ã‚‹ã¾ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ã‚¹ã‚³ã‚¢ã€‚\n",
        "* ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã¯ã€æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®å¤§ãã„ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’é¸ã¶ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPbd4dw0J9k"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmObi_P6bR0"
      },
      "source": [
        "## ðŸŽ² Top-k sampling\n",
        "* top-kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€ç¢ºçŽ‡ãŒé«˜ã„kå€‹ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒ»ãƒ»ãƒ»\n",
        "* ãã‚Œã‚‰ã®ç¢ºçŽ‡ã«å¾“ã£ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸ã¶ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzeaPdKJV5XY"
      },
      "source": [
        "### ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†é–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzZLWF_4V2cn"
      },
      "outputs": [],
      "source": [
        "def top_k_sampling(logits, top_k, num_beams, temperature=1.0, plot=False):\n",
        "  assert top_k >= 1\n",
        "  assert num_beams <= top_k\n",
        "\n",
        "  logit_k = torch.topk(logits, top_k, dim=-1).values[:,-1]\n",
        "  indices_to_remove = logits < logit_k.unsqueeze(-1)\n",
        "  new_logits = torch.clone(logits)\n",
        "  new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # logitã‚’ç¢ºçŽ‡ã«å¤‰æ›ã™ã‚‹\n",
        "  probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "  # top-kã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«beamså€‹ã‚’é¸ã¶\n",
        "  next_tokens = torch.multinomial(probabilities, num_beams)\n",
        "\n",
        "  if plot:\n",
        "    # ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’é¸ã¶ã¨ãã«ä½¿ã£ãŸç¢ºçŽ‡åˆ†å¸ƒã‚’æç”»ã™ã‚‹\n",
        "    total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)\n",
        "\n",
        "  return next_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bxyiGsVUYy9"
      },
      "source": [
        "### ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®ç¢ºçŽ‡åˆ†å¸ƒã‚’æç”»ã™ã‚‹ãƒ˜ãƒ«ãƒ‘é–¢æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsDJWQs-tav"
      },
      "outputs": [],
      "source": [
        "def plot_prob_distribution(total_prob, next_tokens, sampling, potential_nb, total_nb=50):\n",
        "  for i in range(total_prob.shape[0]):\n",
        "    probabilities = total_prob[i]\n",
        "    next_token = next_tokens[i]\n",
        "    # top kã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—\n",
        "    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)\n",
        "    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]\n",
        "\n",
        "    # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦é¸ã°ã‚ŒãŸã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã®ç¢ºçŽ‡ã‚’å–å¾—\n",
        "    next_token_list = [tokenizer.decode([idx]) for idx in next_token.tolist()]\n",
        "    next_token_prob = probabilities[next_token].tolist()\n",
        "\n",
        "    # ç¢ºçŽ‡åˆ†å¸ƒã®ã‚’æ£’ã‚°ãƒ©ãƒ•ã¨ã—ã¦æã\n",
        "    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid(axis='y', linestyle='-', alpha=0.5)\n",
        "    if potential_nb < total_nb:\n",
        "      plt.axvline(x=potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')\n",
        "    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')\n",
        "    plt.bar(next_token_list, next_token_prob, color='red', label='Selected tokens')\n",
        "    plt.xticks(rotation=45, ha='right', va='top')\n",
        "    plt.gca().spines['top'].set_visible(False)\n",
        "    plt.gca().spines['right'].set_visible(False)\n",
        "    if sampling == 'top_k':\n",
        "      plt.title('Probability distribution of predicted tokens with top-k sampling')\n",
        "    elif sampling == 'nucleus':\n",
        "      plt.title('Probability distribution of predicted tokens with nucleus sampling')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilh2KsT5UU0v"
      },
      "source": [
        "### decodeã®å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd3kkhVS82gQ"
      },
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# å¯è¦–åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "length = 5\n",
        "\n",
        "# top-kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "temperature = 5\n",
        "top_k = 20\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTdzJimoISJG"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'top_k', temperature=temperature, top_k=top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVE3LnnaIA16"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZeffMvEVCEf"
      },
      "source": [
        "### ã‚°ãƒ©ãƒ•ã®æç”»"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyop3NTp--4G"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDd_PePe_Itd"
      },
      "source": [
        "## ðŸ”¬ Nucleus sampling\n",
        "* å‚è€ƒè³‡æ–™\n",
        " * https://www.youtube.com/watch?v=JETxaSaj6_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exsgQ3NQ8oTS"
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(logits, nucleus_p, num_beams, temperature=1.0, top_k=100, plot=False):\n",
        "  assert nucleus_p > 0\n",
        "  assert nucleus_p <= 1\n",
        "\n",
        "  # ã¾ãštop kã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã¸ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å¯¾è±¡ã‚’çµžã‚Šè¾¼ã‚€\n",
        "  logit_k = torch.topk(logits, top_k, dim=-1).values[:,-1]\n",
        "  indices_to_remove = logits < logit_k.unsqueeze(-1)\n",
        "  new_logits = torch.clone(logits)\n",
        "  new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "  # æ¬¡ã«top pã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã¸ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å¯¾è±¡ã‚’çµžã‚Šè¾¼ã‚€\n",
        "  sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n",
        "  probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n",
        "  cumulative_probabilities = torch.cumsum(probabilities, dim=-1)\n",
        "\n",
        "  # ç¢ºçŽ‡ã®å’ŒãŒnucleus_pã‚’è¶…ãˆã‚‹ã¨ã“ã‚ã‹ã‚‰ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‹ã‚‰é™¤å¤–ã™ã‚‹\n",
        "  # ãŸã ã—ã€æœ€ä½Žã§ã‚‚num_beamså€‹ã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã¯æ®‹ã™\n",
        "  for i in range(logits.shape[0]):\n",
        "    mask = cumulative_probabilities[i] > nucleus_p\n",
        "    if (1 - mask * 1).sum() < num_beams:\n",
        "      indices_to_remove = sorted_indices[i,num_beams:]\n",
        "    else:\n",
        "      indices_to_remove = sorted_indices[i,mask]\n",
        "    new_logits[i,indices_to_remove] = float('-inf')\n",
        "\n",
        "  # logitã‚’ç¢ºçŽ‡ã«å¤‰æ›ã™ã‚‹\n",
        "  probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "  # top pã®ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«num_beamså€‹ã‚’é¸ã¶\n",
        "  next_tokens = torch.multinomial(probabilities, num_beams)\n",
        "\n",
        "  # Plot distribution\n",
        "  if plot:\n",
        "    total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "    plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)\n",
        "\n",
        "  return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv04nWH1LuFB"
      },
      "outputs": [],
      "source": [
        "# ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ‡ãƒ¼ã‚¿\n",
        "text = \"I work as a data scientist\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "# å¯è¦–åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "length = 5\n",
        "\n",
        "# top-kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "temperature = 1\n",
        "nucleus_p = 0.5\n",
        "num_beams = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyBau3K3LeGY"
      },
      "outputs": [],
      "source": [
        "graph = decoding(input_ids, length, num_beams, 'nucleus', temperature=temperature, nucleus_p=nucleus_p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6uTqcMnk0qb"
      },
      "outputs": [],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPHx7ZRL2641"
      },
      "outputs": [],
      "source": [
        "plot_graph(graph, length, num_beams, 'sequence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVr4_nPsqwVj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}