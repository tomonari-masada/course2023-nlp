{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPs/3pOk1k1QBtt9o+9zCdh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/08_sentiment_analysis_with_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMを使ってみる\n",
        "* 今日は、とりあえず、LLMを使ってみる。"
      ],
      "metadata": {
        "id": "s7UHVLvsl89T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 今日の授業の目的\n",
        "* いまどのくらい手軽にLLMを使えるようになっているかを、とりあえず体感する。\n",
        "* 技術的な詳細は次回以降学んでいくことにして、とにかく使ってみる。"
      ],
      "metadata": {
        "id": "7HPNt4EWmD-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **ランタイムのタイプをGPUに設定しておくこと。**"
      ],
      "metadata": {
        "id": "R0tqelrXmhun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 例題: LLMによる感情分析(sentiment analysis)\n",
        "* 今日は、WRIMEというデータセットを使って、LLMに感情分析させてみる。\n",
        "* 感情分析とは、テキストが表す感情を分析するタスク。\n",
        " * ポジティブな感情か、ネガティブな感情かの2値分類タスクとして解くことが多い。\n",
        " * 今日は、ニュートラルな感情も含めた3値分類問題として解くことにする。\n",
        "* LLMとしてはELYZA-japanese-Llama-2-7b-instructを使う。\n",
        " * プロンプトを使ったテキスト生成によって感情分析の問題を解く。"
      ],
      "metadata": {
        "id": "7PtjYmygnqm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備\n",
        "* Hugging Faceの各種ライブラリを使えば、簡単なコードを書くだけでLLMを使える。"
      ],
      "metadata": {
        "id": "3GIHAP_Rrgkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformersライブラリのインストール\n",
        "* https://huggingface.co/docs/transformers/index"
      ],
      "metadata": {
        "id": "3FEPliNumzPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "2W5-9kWlm1qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasetsライブラリのインストール\n",
        "* https://huggingface.co/docs/datasets/index"
      ],
      "metadata": {
        "id": "eGHG8EeTnPQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "OXa6Thx-nGVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accelerateライブラリのインストール\n",
        "* https://huggingface.co/docs/accelerate/index"
      ],
      "metadata": {
        "id": "aJKj8VXHrauA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "GCTSWrIsrWaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 量子化されたモデルを使うためのライブラリAutoGPTQのインストール\n",
        "* https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization\n",
        " * https://huggingface.co/blog/gptq-integration"
      ],
      "metadata": {
        "id": "wORDq1mJ07Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq"
      ],
      "metadata": {
        "id": "sHQ6_PlWxQf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ここでランタイムを再起動する。**"
      ],
      "metadata": {
        "id": "ntPSyIO8r83H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インポート"
      ],
      "metadata": {
        "id": "eX6zotwEoK2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "9A6Euk7gmXNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* Ver. 2 の方を使う。\n",
        " * WRIME: 主観と客観の感情分析データセット https://github.com/ids-cv/wrime\n",
        "* Hugging Faceのdatasets hubに登録されているので、簡単に扱うことができる。"
      ],
      "metadata": {
        "id": "ELjoQ6q_oTAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WRIMEデータセットの取得\n",
        "* training 30,000件、validation 2,500件、test 2,500件。\n",
        "* 最初だけ、ダウンロードに時間がかかる。"
      ],
      "metadata": {
        "id": "oC5RDmUWosMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"shunk031/wrime\", \"ver2\")"
      ],
      "metadata": {
        "id": "4A0Qp1JsoRaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "julWQgfCox2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"validation\"][0]"
      ],
      "metadata": {
        "id": "f6ssdeeOo7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 正解ラベルの確認"
      ],
      "metadata": {
        "id": "h4lUC6lprDdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、`avg_readers`の`sentiment`を正解ラベルとして使用する。"
      ],
      "metadata": {
        "id": "V-uvFcwepEzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "labels = []\n",
        "for example in dataset[\"validation\"]:\n",
        "  labels.append(example[\"avg_readers\"][\"sentiment\"])\n",
        "Counter(labels)"
      ],
      "metadata": {
        "id": "BMLNCNk_pLhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 元のデータでは５値。\n",
        "* 今回は、-2と-1をnegativeとして、2と1をpositiveとして、それぞれまとめることにする。\n",
        " * これで3値分類の問題になる。\n",
        " * このための前処理は後で行う。"
      ],
      "metadata": {
        "id": "sSlTKQelp3ZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM\n"
      ],
      "metadata": {
        "id": "ACC0ED27qZtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、ELYZA-japanese-Llama-2-7b-instructを使う。\n",
        " * https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct\n",
        "* だが、Google Colab無料版では、この元のモデルは大きすぎて使えない・・・。\n",
        " * 20GB以上のメモリがあるGPUなら使える。\n",
        "* そこで、量子化された下記のモデルを代わりに使う。\n",
        " * https://huggingface.co/TFMC/ELYZA-japanese-Llama-2-7b-instruct-GPTQ-4bit-64g"
      ],
      "metadata": {
        "id": "kIsSj4bzq7Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELYZA-japanese-Llama-2-7b-instruct-GPTQ-4bit-64gの取得\n",
        "* モデルのダウンロードに少し時間がかかる。\n",
        "* `AutoGPTQForCausalLM`クラスについては、以下を参照。\n",
        " * https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/auto.py"
      ],
      "metadata": {
        "id": "qVJVYtaKrGwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* safetensorsについては、以下を参照。\n",
        " * https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors"
      ],
      "metadata": {
        "id": "sFvxES91acnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `trust_remote_code`については、[ここ](https://huggingface.co/docs/transformers/model_doc/auto)に以下のような説明がある。\n",
        "\n",
        "> Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to True for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
        "\n"
      ],
      "metadata": {
        "id": "4m_BJPRmbxSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "model_name = \"TFMC/ELYZA-japanese-Llama-2-7b-instruct-GPTQ-4bit-64g\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_name,\n",
        "    use_safetensors=True,\n",
        "    inject_fused_attention=False,\n",
        "    device=\"cuda:0\",\n",
        "    #trust_remote_code=True,\n",
        "    )\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "PYR1TtgLrGcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## プロンプト\n",
        "* LLMがうまく感情分析をしてくれそうなプロンプトを考える。\n",
        " * 下はあくまで一つの例。"
      ],
      "metadata": {
        "id": "i5-ymVqjvFDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### プロンプト作成用のヘルパ関数"
      ],
      "metadata": {
        "id": "c33BfNxjypMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]答え：\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントで、人の感情を察するのが得意です。\"\n",
        "\n",
        "def make_prompt(example):\n",
        "  sentence = example['sentence']\n",
        "  text = \"「\" + sentence + \"」と言っている人の気持ちは、どのように説明できますか。\"\n",
        "  text += \"また、その気持ちを１単語で言うと、次のどちらですか：「嬉しい」、「悲しい」。\"\n",
        "  text += \"どちらでもなければ、「どちらでもない」と答えてください。\"\n",
        "  prompt = \"{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "      b_inst=B_INST,\n",
        "      system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "      prompt=text,\n",
        "      e_inst=E_INST,\n",
        "      )\n",
        "  example['sentence'] = prompt\n",
        "  return example"
      ],
      "metadata": {
        "id": "Q7gEATn-y3RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 元のテキストをプロンプトに一括変換する。"
      ],
      "metadata": {
        "id": "sU-cGHd2zRBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set = dataset[\"validation\"].map(make_prompt)"
      ],
      "metadata": {
        "id": "aJcIVmkjzPkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(validation_set[0][\"sentence\"])"
      ],
      "metadata": {
        "id": "aqp3lDmbzWJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* プロンプトをあらかじめトークン化しておく。"
      ],
      "metadata": {
        "id": "kJ0e3l3lziue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_set = validation_set.map(lambda samples: tokenizer(samples['sentence']), batched=True)"
      ],
      "metadata": {
        "id": "yYTlet7Gzn3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `input_ids`というフィールドにトークン化の結果が格納されている。"
      ],
      "metadata": {
        "id": "GN_FVAqCz2Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(validation_set[0][\"input_ids\"])"
      ],
      "metadata": {
        "id": "P-1YpzxwzsGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(validation_set[0][\"sentence\"])"
      ],
      "metadata": {
        "id": "AC0No0VuYqrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens([1])"
      ],
      "metadata": {
        "id": "gSCcZ88eY2hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 感情分析"
      ],
      "metadata": {
        "id": "6PahFnk2z-lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5値を3値に変換するヘルパ関数"
      ],
      "metadata": {
        "id": "wOpsD35O0C5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(example):\n",
        "  sentiment_dic = {-2:'悲しい', -1:'悲しい', 0:'どちらでもない', 1:'嬉しい', 2:'嬉しい'}\n",
        "  sentiment = example['avg_readers']['sentiment']\n",
        "  return sentiment_dic[sentiment]"
      ],
      "metadata": {
        "id": "vPpQGdxx0J0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 感情分析の実行"
      ],
      "metadata": {
        "id": "QRYi_49i1Gdt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKPWp6jdW9fF"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  print(f'[{i+1}]' + '-'*80)\n",
        "  instance = validation_set[i]\n",
        "  prompt = instance[\"sentence\"]\n",
        "  with torch.no_grad():\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids=token_ids.to(model.device),\n",
        "        max_new_tokens=256,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "  output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
        "  print(f\"{prompt}\\nprediction:\\t{output}\")\n",
        "  print(f\"ground truth:\\t「{get_sentiment(instance)}」\")\n",
        "  print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* もっとうまくLLMに感情分析をさせるプロンプトを考えてみよう。"
      ],
      "metadata": {
        "id": "-cXKf5dF0vRp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUXcB4axyCQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}