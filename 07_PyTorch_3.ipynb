{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Ju5Lc6g45t9qjI0R2cFUAA1wHBFm2IeB",
      "authorship_tag": "ABX9TyOEHdEnQXZoV4oeKgqPRyIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/07_PyTorch_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hx2zN0IvyqF"
      },
      "source": [
        "# PyTorch入門 (3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 本日のお題\n",
        "* テキスト分類をPyTorchで実装する。\n",
        "\n",
        "* PyTorchのチュートリアルを参考にした。\n",
        "  * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "\n",
        "* ただし、モデルを定義するところ以外は、大幅に変えている。\n",
        "  * まず、トークナイザの訓練から自前でおこなうことにした。\n",
        "  * また、データセットはHugging Faceのdatasetsライブラリから使うようにした。\n"
      ],
      "metadata": {
        "id": "pXenjmLeAJjA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv8yYt8-Yn0"
      },
      "source": [
        "**ランタイムの設定でGPUを選択しておこう。**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "iIHq0CF9oMqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 必要なライブラリのインストール"
      ],
      "metadata": {
        "id": "J5Lnd-ekgblc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hugging Faceのdatasetsライブラリとtokenizersライブラリをインストール"
      ],
      "metadata": {
        "id": "_esbFOZ9pZso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tokenizers"
      ],
      "metadata": {
        "id": "MbjXLZStpYp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 再現性の確保\n"
      ],
      "metadata": {
        "id": "_0jLWNWjk1DO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このくらいやっておけば、完璧？\n",
        "* 参考にしたリポジトリ\n",
        "  * https://github.com/ericwtodd/function_vectors/blob/main/src/utils/model_utils.py"
      ],
      "metadata": {
        "id": "M2v7WucbEcF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "\n",
        "  # Random seed\n",
        "  random.seed(seed)\n",
        "\n",
        "  # Numpy seed\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Torch seed\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "  # os seed\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "EecUGZKspEal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用するデバイスの設定"
      ],
      "metadata": {
        "id": "5KPNSH_uKI7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "ATicCrfrKIXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット"
      ],
      "metadata": {
        "id": "7MU1kOIZpGuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AG Newsデータセット\n",
        "* 今回はAG_NEWSというテキスト分類用のデータセットを使う。\n",
        "  * 4クラス分類問題を解く。"
      ],
      "metadata": {
        "id": "PvExI-tmpI_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ag_news\")"
      ],
      "metadata": {
        "id": "V7p6LtyekseI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "Q5Q3qFfre1Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ラベルの意味は、以下の通り。（ https://huggingface.co/datasets/ag_news を参照。）"
      ],
      "metadata": {
        "id": "8EdVC8h0V7Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ag_news_label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tec\"}"
      ],
      "metadata": {
        "id": "UWE6VFnqV0f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 中身を少し見てみる。"
      ],
      "metadata": {
        "id": "PwIOEuIbWDWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "ALJZeVnYEo2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"text\"][1]"
      ],
      "metadata": {
        "id": "bTQAsjRKIWgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"label\"][1]"
      ],
      "metadata": {
        "id": "rHz5r5qHVO7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"number of different labels: {len(set(dataset['train']['label']))}\")"
      ],
      "metadata": {
        "id": "aa9KBFMYP7iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練/検証/テストデータへ分割"
      ],
      "metadata": {
        "id": "H6htLjG3feGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_valid = dataset[\"train\"].train_test_split(test_size=0.05)\n",
        "train_valid"
      ],
      "metadata": {
        "id": "O18EBvE5OSZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_valid[\"train\"],\n",
        "    \"valid\": train_valid[\"test\"],\n",
        "    \"test\": dataset[\"test\"],\n",
        "})\n",
        "dataset"
      ],
      "metadata": {
        "id": "q9HDarWfOVZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トークナイザ\n",
        "* 以下の説明は、ほぼ次のHugging Faceのdocumentationそのまま。\n",
        "  * https://huggingface.co/docs/tokenizers/pipeline"
      ],
      "metadata": {
        "id": "cDcs6sxBqWLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トークン化アルゴリズム\n",
        "* 今回はWordPieceアルゴリズムを使う。\n",
        "  * https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt\n",
        "  * https://huggingface.co/docs/tokenizers/api/models#tokenizers.models.BPE\n",
        "* 見たことがない文字列は、unknownトークンとして検出する。\n",
        "  * unknownトークンを避けるには、byteレベルでトークン化すれば良い。\n",
        "  * だが、今回は、このような高度なトークン化は行わない。\n",
        "  * byteレベルのトークン化については、\n",
        "  [ここ](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)の緑色のコメント部分を参照。"
      ],
      "metadata": {
        "id": "hDHxdhmfF2L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "JdtZhLnpF5Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model.unk_token"
      ],
      "metadata": {
        "id": "kwlr_649UeVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストの正規化\n",
        "* NFDについては、例えば、下の記事を参照。\n",
        "  * https://qiita.com/fury00812/items/b98a7f9428d1395fc230\n",
        "* Lowercase()は小文字化、StripAccents()はアクセント記号の除去。"
      ],
      "metadata": {
        "id": "yBinPZyBYnWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
        "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
      ],
      "metadata": {
        "id": "KGOZg8SaWjEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このnormalizerがどんな正規化をするか、見てみる。"
      ],
      "metadata": {
        "id": "_MfvNaxudAId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\")"
      ],
      "metadata": {
        "id": "GpRm7nFec9cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### プレトークナイザ\n",
        "* トークナイザを訓練させるとき、最初に無条件に実行するトークン化を設定する。\n",
        "* 例えば、英語の場合、まずは無条件に空白文字でトークン化するのが普通。\n",
        "  * https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.Whitespace"
      ],
      "metadata": {
        "id": "W9SeBf0iGv4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "4xzIt-AGHCiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トークナイザのトレーナ\n",
        "* ここで語彙サイズなども設定できる。\n",
        "* 今回は、特殊トークンの設定を除いて、デフォルトの設定を使う。\n",
        "  * 特殊トークンは、今回は実際には`[UNK]`しか使わない。\n",
        "  * このように書けば良いという例として、他の特殊トークンも示しておく。"
      ],
      "metadata": {
        "id": "ioZHJU-yGTx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "FLQ12LPHGa3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.vocab_size"
      ],
      "metadata": {
        "id": "CzfpLppxGngC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.special_tokens"
      ],
      "metadata": {
        "id": "-P-wl_ikUsWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トークナイザの訓練\n",
        "* 語彙集合を決めるときは、訓練データ部分だけを使う。\n",
        "* trainerを与えるのを忘れないように。\n",
        "  * trainerを与えるのを忘れると、デフォルトの設定で訓練されてしまう。\n"
      ],
      "metadata": {
        "id": "uISKPGWHHmO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(dataset[\"train\"][\"text\"], trainer)"
      ],
      "metadata": {
        "id": "8AG3u8G-HeRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練したトークナイザは、JSON形式で保存もできる。"
      ],
      "metadata": {
        "id": "mMGW8almdhOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"my-tokenizer.json\")"
      ],
      "metadata": {
        "id": "VS_Q39gJZMOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙サイズ"
      ],
      "metadata": {
        "id": "Inu8JAr9djWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "HugHDvHv3mjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙の取得"
      ],
      "metadata": {
        "id": "WiSCXpKSdlIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tokenizer.get_vocab()\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "k_TXMjnNIh-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 念の為`[UNK]`トークンが語彙に入っているか確認する。"
      ],
      "metadata": {
        "id": "wg537L4udnQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model.unk_token in vocab"
      ],
      "metadata": {
        "id": "eQs9G2RxU4L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 単語トークンの列が整数の列に変換されることを確認する。"
      ],
      "metadata": {
        "id": "KwPfiU3Xq8qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(dataset[\"train\"][\"text\"][0])\n",
        "output"
      ],
      "metadata": {
        "id": "9JxplX7Eq5kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][\"text\"][0])"
      ],
      "metadata": {
        "id": "oSTOkdNFQ92x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.ids)"
      ],
      "metadata": {
        "id": "k3F6t2bDIwvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.type_ids)"
      ],
      "metadata": {
        "id": "QPOBod3vJB2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.tokens)"
      ],
      "metadata": {
        "id": "ueoaEp9aJV1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* offsetsは各トークンが何文字目から何文字目までかを表す。"
      ],
      "metadata": {
        "id": "yiSXO-noJcEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.offsets)"
      ],
      "metadata": {
        "id": "oivhonJfyOqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 次に、わざと、トークナイザが見たことなさそうなトークンを含むテキストをトークン化させてみる。"
      ],
      "metadata": {
        "id": "U7Cp3Yw8aCP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(\"Welcome to the 🤗 Tokenizers library.\")"
      ],
      "metadata": {
        "id": "3xVPZrvPZlqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 絵文字が`[UNK]`トークンとしてトークン化されている。"
      ],
      "metadata": {
        "id": "3Uwi0FTcdvqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.tokens)"
      ],
      "metadata": {
        "id": "KNIFXqR8aJn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "60xaGMzv3_XW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### collate関数\n",
        "* サンプルに前処理を施してミニバッチを作ることを、collateする、と言う。\n",
        "* collate関数の中でトークナイザを呼び出している。\n",
        "* 今回は、同じミニバッチに含まれるテキストをすべてつなげてしまう。\n",
        "  * `offsets`は、各テキストが、先頭から数えて何トークン目から始まるかを表す。\n",
        "  * 正確には、先頭から数えて何トークン目から始まるか、マイナス１、がオフセット。\n",
        "* このcollate関数は、後でDataLoaderを作るときに使う。"
      ],
      "metadata": {
        "id": "QUaiRtBo4VKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "  label_list, text_list, offsets = [], [], [0]\n",
        "  for instance in batch:\n",
        "    _label, _text = instance[\"label\"], instance[\"text\"]\n",
        "    # ラベルはラベルで集める\n",
        "    label_list.append(_label)\n",
        "    token_ids = torch.tensor(tokenizer.encode(_text).ids, dtype=torch.int64)\n",
        "    # トークンidの列も集める\n",
        "    text_list.append(token_ids)\n",
        "    # オフセットも集める\n",
        "    offsets.append(token_ids.size(0))\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "metadata": {
        "id": "NRrfbx4W4T2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データ、検証データ、テストデータのDataLoaderを作る。\n",
        "* collate関数の使い方に注目。"
      ],
      "metadata": {
        "id": "HiPXZZyim7Zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoaderのインスタンスの作成"
      ],
      "metadata": {
        "id": "8Odw2RSygNw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ミニバッチのサイズを適当に決める\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    dataset[\"valid\"], batch_size=BATCH_SIZE, collate_fn=collate_batch\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    dataset[\"test\"], batch_size=BATCH_SIZE, collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "RQ-6jH0h4E-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "PkuKQtIWK3s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル"
      ],
      "metadata": {
        "id": "VFuDSuMR52hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `nn.EmbeddingBag`\n",
        "* 全トークンのembeddingの平均を一挙に求めるlayer。"
      ],
      "metadata": {
        "id": "LP6WNSYVeUfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# 説明目的で、見やすいように低次元で埋め込み層を作ってみる。\n",
        "embedding = nn.EmbeddingBag(len(vocab), 8, sparse=False)"
      ],
      "metadata": {
        "id": "BtrYIvX0eS3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 本当に平均を求めているかを確認する。"
      ],
      "metadata": {
        "id": "0BVikXBriB_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"language models\"\n",
        "input = torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
        "offsets = torch.tensor([0], dtype=torch.int64)\n",
        "embedding(input=input, offsets=offsets)"
      ],
      "metadata": {
        "id": "JpdkpArLeqj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text).tokens"
      ],
      "metadata": {
        "id": "CaAmoerAPj8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"language\"\n",
        "input = torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
        "offsets = torch.tensor([0], dtype=torch.int64)\n",
        "output1 = embedding(input=input, offsets=offsets)\n",
        "output1"
      ],
      "metadata": {
        "id": "F_3fX1GCg_73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"models\"\n",
        "input = torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
        "offsets = torch.tensor([0], dtype=torch.int64)\n",
        "output2 = embedding(input=input, offsets=offsets)\n",
        "output2"
      ],
      "metadata": {
        "id": "OnTSVqsPhCEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(output1 + output2) / 2"
      ],
      "metadata": {
        "id": "z5mc0lLghNOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* offsetsはテキストの切れ目を表す。\n",
        "  * offsetsを利用すれば、複数のテキストをつなげたままベクトル化できる。\n",
        "  * メモリの効率も時間的な効率も良い。"
      ],
      "metadata": {
        "id": "9Ig_SXomh8oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"language models text classification\"\n",
        "tokenizer.encode(text).tokens"
      ],
      "metadata": {
        "id": "ggS8gooLP0B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* オフセットを指定してembedする。\n",
        "  * この例では、\"text classification\"が二つ目のテキストとなる。"
      ],
      "metadata": {
        "id": "k-IAl8zER_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"language models text classification\"\n",
        "input = torch.tensor(tokenizer.encode(text).ids, dtype=torch.int64)\n",
        "offsets = torch.tensor([0, 2], dtype=torch.int64)\n",
        "embedding(input=input, offsets=offsets)"
      ],
      "metadata": {
        "id": "qGHLsNTWh0mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分類モデル\n",
        "* `nn.Module`を継承して自前のクラスを定義する。\n"
      ],
      "metadata": {
        "id": "hFnexLcZ6Pkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "    super(TextClassificationModel, self).__init__()\n",
        "    # 埋め込み層\n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
        "    # 分類用の全結合層\n",
        "    self.fc = nn.Linear(embed_dim, num_class)\n",
        "    # 自前の重み初期化関数を呼び出す\n",
        "    self.init_weights()\n",
        "\n",
        "  # 自前の重み初期化関数\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.bias.data.zero_()\n",
        "\n",
        "  # forward pass\n",
        "  def forward(self, text, offsets):\n",
        "    embedded = self.embedding(text, offsets)\n",
        "    return self.fc(embedded)"
      ],
      "metadata": {
        "id": "pRJysWR_r9cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データを使ってクラスの個数を調べる。"
      ],
      "metadata": {
        "id": "dtVoatKc7vBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set([label for label in dataset[\"train\"][\"label\"]])\n",
        "print(unique_labels)\n",
        "num_class = len(unique_labels)"
      ],
      "metadata": {
        "id": "x4kpm5Ie7tMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 重要な定数を変数にセットする。"
      ],
      "metadata": {
        "id": "1u1m1i7e748S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 語彙サイズ\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 埋め込みベクトルの次元（これは適当に決める）\n",
        "emsize = 64"
      ],
      "metadata": {
        "id": "UcPLpHkXsAag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルのインスタンスを作成しGPUへ送る。\n",
        " * 上で値をセットした変数を使って初期化している。"
      ],
      "metadata": {
        "id": "q2UFwoEW71LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "metadata": {
        "id": "0BXZ2xKg7zJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練に使うヘルパ関数"
      ],
      "metadata": {
        "id": "xxK-Sx4A7eFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "  model.train()\n",
        "  total_acc, total_count = 0, 0\n",
        "  log_interval = 500 # ログ情報を表示する間隔\n",
        "  start_time = time.time()\n",
        "\n",
        "  for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    predicted_label = model(text, offsets)\n",
        "    loss = criterion(predicted_label, label)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "    optimizer.step()\n",
        "    total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "    if idx % log_interval == 0 and idx > 0:\n",
        "      elapsed = time.time() - start_time\n",
        "      print(\n",
        "          \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "          \"| accuracy {:8.3f}\".format(\n",
        "              epoch, idx, len(dataloader), total_acc / total_count\n",
        "          )\n",
        "      )\n",
        "      total_acc, total_count = 0, 0\n",
        "      start_time = time.time()"
      ],
      "metadata": {
        "id": "BeGMjdPd7dXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価に使うヘルパ関数"
      ],
      "metadata": {
        "id": "aa0nVABe8GGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader):\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "      predicted_label = model(text, offsets)\n",
        "      loss = criterion(predicted_label, label)\n",
        "      total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "  return total_acc / total_count"
      ],
      "metadata": {
        "id": "8JEgh9O3sAPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの訓練"
      ],
      "metadata": {
        "id": "iPLsWoz88SrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* エポック数と学習率の設定\n",
        "  * SGDを使うので、学習率は大きい目の値にしている。"
      ],
      "metadata": {
        "id": "7Yw5vNAlSq-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10  # epoch\n",
        "LR = 5  # learning rate"
      ],
      "metadata": {
        "id": "TJ8vB0_t8bAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 損失関数、最適化アルゴリズム、スケジューラの設定"
      ],
      "metadata": {
        "id": "EHJtiyQqSy9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n"
      ],
      "metadata": {
        "id": "rKjAt42oS7kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 学習の実行"
      ],
      "metadata": {
        "id": "G8otKmBlT2vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_accu = None\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(train_dataloader)\n",
        "  accu_val = evaluate(valid_dataloader)\n",
        "  if total_accu is not None and total_accu > accu_val:\n",
        "    # 検証データの正解率が前のエポックより下がったらスケジューラを動かす\n",
        "    scheduler.step()\n",
        "  else:\n",
        "    total_accu = accu_val\n",
        "  print(\"-\" * 59)\n",
        "  print(\n",
        "      \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "      f\"| lr = {optimizer.param_groups[0]['lr']:.3f}\"\n",
        "      \"| valid accuracy {:8.3f} \".format(\n",
        "          epoch, time.time() - epoch_start_time, accu_val\n",
        "      )\n",
        "  )\n",
        "  print(\"-\" * 59)"
      ],
      "metadata": {
        "id": "vixuo6u2r__K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 最後にテストセットで評価"
      ],
      "metadata": {
        "id": "1rToWRgOeJI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking the results of test dataset...\")\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print(\"test accuracy {:8.3f}\".format(accu_test))"
      ],
      "metadata": {
        "id": "o45KtyPZsF68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def predict(text):\n",
        "  with torch.no_grad():\n",
        "    text = torch.tensor(tokenizer.encode(text).ids)\n",
        "    output = model(text, torch.tensor([0]))\n",
        "    return output.argmax(1).item() + 1\n",
        "\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str)])"
      ],
      "metadata": {
        "id": "8FMoLIJ5sF3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzVkJ2v-tt_"
      },
      "source": [
        "# 本日の課題\n",
        "* アーキテクチャやoptimizerやschedulerを変更して、validation set上で評価しつつモデルをチューニングしよう。\n",
        "* 余裕があれば、トークナイザもチューニングしよう。\n",
        "  * 例: トークン化アルゴリズムをBPEに変えてみる。\n",
        "* 最後に、自分でチューニングした設定を使って、test set上で評価しよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26RYXnmnbbSc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}