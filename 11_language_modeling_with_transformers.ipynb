{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/11_language_modeling_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI5WU1A-dedt"
      },
      "source": [
        "# Transformerを使った言語モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 参考資料: 言語モデルに関するPyTorchのチュートリアル\n",
        "  * https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "  * モデル以外の部分は大きく変更している。"
      ],
      "metadata": {
        "id": "c5T5a3euRYPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 今回の問題設定\n",
        "* トランスフォーマを使って自分で言語モデルをtrainingする。\n",
        "* 言語モデルにテキストを生成させてみる。"
      ],
      "metadata": {
        "id": "-CQsCew8AHx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ランタイムのタイプをGPUにしておく。**"
      ],
      "metadata": {
        "id": "btXTiJr4fuwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インストール"
      ],
      "metadata": {
        "id": "fjdVdxIgRvPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回、データセットはHugging Faceのdatasetsライブラリを使って取得する。"
      ],
      "metadata": {
        "id": "ZBYf79oFK9H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Vq3-Gw_LLn0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "Vh2_VQHKLDMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPax_f4Pdedv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from datasets import load_dataset\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* Hugging Face Hubからwikitext-103-v1を取得する。\n",
        "  * https://huggingface.co/datasets/wikitext\n",
        "  * 約1分待つ。"
      ],
      "metadata": {
        "id": "dCdlTLwx6ic_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
      ],
      "metadata": {
        "id": "6MbS-KnB6ldd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "MkpklJvP7HB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['text'][3]"
      ],
      "metadata": {
        "id": "tDTAHiN_92A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = 1000000\n",
        "max_len = 0\n",
        "for text in dataset[\"train\"][\"text\"]:\n",
        "  max_len = max(len(text), max_len)\n",
        "  min_len = min(len(text), min_len)\n",
        "print(min_len, max_len)"
      ],
      "metadata": {
        "id": "8ZcQFLrgAGTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* すべてのテキストを繋いで、それを固定長のテキストに分けることにする。"
      ],
      "metadata": {
        "id": "ZODtWKjkMQYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トークナイザの学習\n",
        "* BPEアルゴリズムを使ってみる。\n",
        "  * https://huggingface.co/docs/tokenizers/main/en/quicktour\n",
        "* データの渡し方については、下記のリンク先を参照。\n",
        "  * https://huggingface.co/learn/nlp-course/chapter6/2\n"
      ],
      "metadata": {
        "id": "dgF-xArr8Hxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* BPEトークナイザのインスタンスを作成"
      ],
      "metadata": {
        "id": "hziHVjyoMjDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "CvDxr78L7fP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は必要でない特殊トークンもあるが、参考までに。"
      ],
      "metadata": {
        "id": "flXjXpOIMazq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
      ],
      "metadata": {
        "id": "ayqdwHpu7grh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* プレトークナイザの設定"
      ],
      "metadata": {
        "id": "h1nRcgbiMlpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ],
      "metadata": {
        "id": "2NnvA0fY7ncL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 1000個ずつテキストを取得するヘルパ関数\n",
        "  * 処理を速めるため。"
      ],
      "metadata": {
        "id": "nksoKnLcMonZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_corpus():\n",
        "  for start_idx in range(0, len(dataset[\"train\"]), 1000):\n",
        "    samples = dataset[\"train\"][start_idx:start_idx+1000]\n",
        "    yield samples[\"text\"]"
      ],
      "metadata": {
        "id": "IQPcMoep8W7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データを使って、トークナイザに語彙を学習させる。\n",
        "  * 90秒ぐらい待つ。"
      ],
      "metadata": {
        "id": "vmsR3JIsA5FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(get_training_corpus(), trainer)"
      ],
      "metadata": {
        "id": "eb82h5N17p6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トークナイザをJSON形式のファイルとして保存。"
      ],
      "metadata": {
        "id": "JWWBz_zhNSUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer-wiki.json\")"
      ],
      "metadata": {
        "id": "LFm45tBUICsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙サイズのチェック\n",
        "  * デフォルトの設定をそのまま使っただけ。"
      ],
      "metadata": {
        "id": "rrHtVYeyNVtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "7MEY704e_O3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トークン化を試してみる。"
      ],
      "metadata": {
        "id": "Uw3ydkc4NdqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tokenizer.encode(dataset[\"train\"][3][\"text\"])"
      ],
      "metadata": {
        "id": "7f8PeGw4IIni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.tokens)"
      ],
      "metadata": {
        "id": "EecXyMYCHTii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.ids)"
      ],
      "metadata": {
        "id": "-VclIT-n-Yqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sNu1jEjdedu"
      },
      "source": [
        "## モデル\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9cKzXcdedu"
      },
      "source": [
        "* PyTorchの`nn.TransformerEncoder`を使う\n",
        "  * https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "  * デフォルトの設定で``batch_first=False``になっていることに注意。\n",
        "  * デフォルトの設定がこうなっているのは、少し前までのトークン列の扱い方の名残。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの定義"
      ],
      "metadata": {
        "id": "DqaNBA8HNi2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout=0.5):\n",
        "    super().__init__()\n",
        "    # 入力されるベクトルの次元（今回はtoken embeddingの次元）\n",
        "    self.d_model = d_model\n",
        "    # 位置エンコーディング\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    # 多層のエンコーダを作成\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout,\n",
        "        batch_first=True, # batch_firstはTrueに変えておく。\n",
        "        )\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    # 入力の埋め込み層\n",
        "    self.encoder = nn.Embedding(ntoken, d_model)\n",
        "    # 単語ロジットを出力する全結合層（ntokenは語彙サイズ）\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "    # 今回は、自前の初期化を使ってみる\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(- initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(- initrange, initrange)\n",
        "\n",
        "  def forward(self, src):\n",
        "    src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
        "    Unmasked positions are filled with float(0.0).\n",
        "    \"\"\"\n",
        "    seq_len = src.shape[1]\n",
        "    src_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(device)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "WGotCgURRszq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jep-B49dedv"
      },
      "source": [
        "## 位置エンコーディング\n",
        "* シーケンス内でのトークンの絶対的な位置をベクトルで表現する。\n",
        "  * 参考資料: https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/positional-encoding/\n",
        "* 最近のLLMでは、絶対的な位置の情報は使わないことが多い。\n",
        "  * 参考資料: https://pub.towardsai.net/the-quest-to-have-endless-conversations-with-llama-and-chatgpt-%EF%B8%8F-81360b9b34b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6JPtiRhdedw"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:,0,0::2] = torch.sin(position * div_term)\n",
        "    pe[:,0,1::2] = torch.cos(position * div_term)\n",
        "    # `register_buffer()`を使ってpeをこのモジュールのパラメータの一部にする。\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # テンソルxの形は[seq_len, batch_size, embedding_dim]\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットのミニバッチ化"
      ],
      "metadata": {
        "id": "QNqK_0lMOMl7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トークン化をおこなうヘルパ関数"
      ],
      "metadata": {
        "id": "JsoDg_oiU5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* テキストをトークン化しながら、一つの長いトークンIDの列を作る。\n",
        "  * 訓練データのテキストは最初の10万件だけを使う。\n",
        "  * これは単に説明の時間を短縮するため。（本当は訓練データ全体を使う。）\n",
        "* トークンIDの列は、PyTorchのテンソルへ変換しておく。"
      ],
      "metadata": {
        "id": "0R-v98hxCA9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(data_slice, seq_len):\n",
        "  token_ids = []\n",
        "  # training setは大きいので、100000テキストだけ使うことにする。\n",
        "  for text in tqdm(data_slice[\"text\"][:100000]):\n",
        "    token_ids += tokenizer.encode(text).ids\n",
        "  truncated_length = (len(token_ids) // seq_len) * seq_len\n",
        "  token_ids = torch.tensor(token_ids[:truncated_length])\n",
        "  # `t()`は転置をとる操作\n",
        "  return token_ids.reshape(-1, seq_len)"
      ],
      "metadata": {
        "id": "skqC_xvBC5JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットのトークン化"
      ],
      "metadata": {
        "id": "DYxaq9L4VnwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4f7NfSededx"
      },
      "outputs": [],
      "source": [
        "# シーケンスの最大長\n",
        "max_seq_len = 128\n",
        "\n",
        "sequences = {}\n",
        "for key in dataset:\n",
        "  # 1を足しているのは、入力とターゲットのペアを作るとき、\n",
        "  # それぞれ、最初の1トークンと、最後の1トークンを、削除するため。\n",
        "  sequences[key] = data_process(dataset[key], max_seq_len + 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[\"train\"][0]"
      ],
      "metadata": {
        "id": "X1JKi9xDPlMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(list(sequences[\"train\"][0]))"
      ],
      "metadata": {
        "id": "8g2y7wlvKVBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "loader = {}\n",
        "for key in sequences:\n",
        "  loader[key] = DataLoader(\n",
        "      sequences[key],\n",
        "      batch_size=batch_size,\n",
        "      shuffle=(key == \"train\"),\n",
        "      )"
      ],
      "metadata": {
        "id": "mAHjtWVuNCZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(loader[\"train\"]))\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "N5MjZz8aNU7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token_ids in batch:\n",
        "  print(tokenizer.decode(list(token_ids)))"
      ],
      "metadata": {
        "id": "YOWzKYt9QEQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 入力は、後でこうやって作る。"
      ],
      "metadata": {
        "id": "leMzycQZQN-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch[:,:-1]"
      ],
      "metadata": {
        "id": "T1P7lTvcRvzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ターゲットは、後でこうやって作る。"
      ],
      "metadata": {
        "id": "2i3ThmrJQQYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch[:,1:]"
      ],
      "metadata": {
        "id": "703WIgEuRw2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLTBk6IVdedy"
      },
      "source": [
        "## モデルの作成\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNpAHFMvdedz"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.get_vocab_size()  # 語彙サイズ\n",
        "embed_size = 256  # トークンembeddingの次元\n",
        "hidden_dim = 256  # nn.TransformerEncoderの隠れ状態のサイズ\n",
        "n_layers = 2  # nn.TransformerEncoderLayerの層の数\n",
        "n_head = 2  # nn.MultiheadAttentionのヘッドの数\n",
        "dropout = 0.1  # dropoutの確率\n",
        "model = TransformerModel(vocab_size, embed_size, n_head, hidden_dim, n_layers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CY7Bl3ededz"
      },
      "source": [
        "## モデルの訓練\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ],
      "metadata": {
        "id": "uzdyAGprYjL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "metadata": {
        "id": "P-SbgdUwXJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練のためのヘルパ関数"
      ],
      "metadata": {
        "id": "c3ekonXRXsZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luKNMEyidedz"
      },
      "outputs": [],
      "source": [
        "def train(model):\n",
        "  model.train()  # 訓練モード\n",
        "  total_loss = 0.0\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "\n",
        "  num_batches = len(loader[\"train\"])\n",
        "  for i, batch in enumerate(loader[\"train\"]):\n",
        "    batch = batch.to(device)\n",
        "    input, target = batch[:,:-1], batch[:,1:]\n",
        "    output = model(input)\n",
        "    loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    if i % log_interval == 0 and i > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| epoch {epoch:3d} | {i:5d}/{num_batches:5d} batches | '\n",
        "            f'lr {lr:.3e} | ms/batch {ms_per_batch:5.2f} | '\n",
        "            f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "      total_loss = 0\n",
        "      start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 評価のためのヘルパ関数"
      ],
      "metadata": {
        "id": "XGhKe36qXxgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, eval_loader):\n",
        "  model.eval()  # 評価モード\n",
        "  total_loss = 0.0\n",
        "  total_seq_len = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "      batch = batch.to(device)\n",
        "      seq_len = batch.shape[0] - 1\n",
        "      input, target = batch[:,:-1], batch[:,1:]\n",
        "      output = model(input)\n",
        "      loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "      total_loss += seq_len * loss.item()\n",
        "      total_seq_len += seq_len\n",
        "  return total_loss / total_seq_len"
      ],
      "metadata": {
        "id": "lPha2fJ_XwoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2LJA57tdedz"
      },
      "source": [
        "### 学習の実行\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルを保存するパスの設定"
      ],
      "metadata": {
        "id": "mZifzbxOdcBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_directory = os.getcwd() # ここを自分のGoogle Driveのフォルダに変更\n",
        "best_model_params_path = os.path.join(working_directory, \"best_model_params.pt\")\n",
        "print(f\"save path: {best_model_params_path}\")"
      ],
      "metadata": {
        "id": "s1qWwKbEc0Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* trainingのループを動かす。"
      ],
      "metadata": {
        "id": "pkUwr4kKfhLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2SGi1C3ded0"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = model\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, loader[\"validation\"])\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(\n",
        "      f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "      f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}'\n",
        "      )\n",
        "  print('-' * 89)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLNGCgNmded0"
      },
      "source": [
        "## テストセット上での評価\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BiXfr0Mded0"
      },
      "outputs": [],
      "source": [
        "test_loss = evaluate(best_model, loader[\"test\"])\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(\n",
        "    f'| End of training | test loss {test_loss:5.2f} | '\n",
        "    f'test ppl {test_ppl:8.2f}'\n",
        "    )\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テキストの生成"
      ],
      "metadata": {
        "id": "lgAnVsswkE97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I couldn't sleep last night. Because I was\"\n",
        "token_ids = torch.tensor([tokenizer.encode(text).ids]).to(device)\n",
        "output = model(token_ids)"
      ],
      "metadata": {
        "id": "UoV3uczKgSOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "k-VvGXK4gsf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0,-1,:].argmax()"
      ],
      "metadata": {
        "id": "7dT1GUiCjCZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([output[0,-1,:].argmax().item()])"
      ],
      "metadata": {
        "id": "iF5ssGvugvSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I couldn't sleep last night. Because I was\"\n",
        "token_ids = torch.tensor([tokenizer.encode(text).ids]).to(device)\n",
        "for _ in range(10):\n",
        "  output = model(token_ids)\n",
        "  token_ids = torch.cat([token_ids, output[0,-1,:].argmax().reshape(1,-1)], dim=1)\n",
        "  print(tokenizer.decode(list(token_ids[0])))"
      ],
      "metadata": {
        "id": "U_5lF10qi09d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題\n",
        "* 最低限、上のコードの動作確認をしよう。\n",
        "* 余裕があれば、validation perplexityの値をどこまで減らせるか、チューニングしてみよう。"
      ],
      "metadata": {
        "id": "ulmTvMemYuzb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QT_kugVcYFf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}