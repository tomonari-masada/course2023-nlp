{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/11_language_modeling_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI5WU1A-dedt"
      },
      "source": [
        "# Transformerを使った言語モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 参考資料: 言語モデルに関するPyTorchのチュートリアル\n",
        "  * https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
      ],
      "metadata": {
        "id": "c5T5a3euRYPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 今回の課題設定\n",
        "* 英語のテキストのembeddingを得るために、トランスフォーマを一からtrainingする。"
      ],
      "metadata": {
        "id": "-CQsCew8AHx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ランタイムのタイプをGPUにしておく。**"
      ],
      "metadata": {
        "id": "btXTiJr4fuwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "fjdVdxIgRvPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 必要なライブラリのインストール\n",
        "  * あとでデータセットを取得するために必要なライブラリ。"
      ],
      "metadata": {
        "id": "i1YOmzHfSa-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "id": "Vq3-Gw_LLn0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `transformers`のインストール\n",
        "* `transformers`はtokenizerを用意するために使う。\n",
        "  * 今回は、モデルを作るために`transformers`は使わない。"
      ],
      "metadata": {
        "id": "qFoSDqwyTdZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "g5MzlBXtE9vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPax_f4Pdedv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sNu1jEjdedu"
      },
      "source": [
        "## モデルの定義\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer encoderモデル"
      ],
      "metadata": {
        "id": "tcusOPvMYOvk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9cKzXcdedu"
      },
      "source": [
        "* PyTorchの`nn.TransformerEncoder`を使う\n",
        "  * https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "* デフォルトの設定で``batch_first=False``になっていることに注意。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout=0.5):\n",
        "    super().__init__()\n",
        "    # 入力されるベクトルの次元（今回はtoken embeddingの次元）\n",
        "    self.d_model = d_model\n",
        "    # 位置エンコーディング\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    # 多層のエンコーダを作成\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    # 入力の埋め込み層\n",
        "    self.encoder = nn.Embedding(ntoken, d_model)\n",
        "    # 単語ロジットを出力する全結合層（ntokenは語彙サイズ）\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "    # 今回は、自前の初期化を使ってみる\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(- initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(- initrange, initrange)\n",
        "\n",
        "  def forward(self, src, src_mask):\n",
        "    src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "WGotCgURRszq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 上三角行列のマスク\n",
        "* 言語モデルは、次のトークンを予測するモデル。\n",
        "* よって、過去のトークンだけを見るようにしないといけない。\n",
        "* そのため、self-attentionの計算に、上三角行列のマスクをかける。"
      ],
      "metadata": {
        "id": "i9VarWt-SSR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(mask_size):\n",
        "  #上三角行列を生成する。上三角が-inf、対角成分含めた残りはゼロ。\n",
        "  return torch.triu(torch.ones(mask_size, mask_size) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "ll5b1z_RSQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jep-B49dedv"
      },
      "source": [
        "## 位置エンコーディング\n",
        "* シーケンス内でのトークンの絶対的な位置をベクトルで表現する。\n",
        "* 参考資料\n",
        "  * https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/positional-encoding/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6JPtiRhdedw"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:,0,0::2] = torch.sin(position * div_term)\n",
        "    pe[:,0,1::2] = torch.cos(position * div_term)\n",
        "    # `register_buffer()`を使ってpeをこのモジュールのパラメータの一部にする。\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # テンソルxの形は[seq_len, batch_size, embedding_dim]\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トークナイザ"
      ],
      "metadata": {
        "id": "ITCgJ3u-Q3sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はGPT-2のトークナイザを使う。"
      ],
      "metadata": {
        "id": "c__yHF_wS7RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "oyNYpwSWMKfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"This is a pen.\")"
      ],
      "metadata": {
        "id": "2VMUswjvAXWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"This is a pen.\")"
      ],
      "metadata": {
        "id": "EfTCYlEZh5j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "B1wR6cjzAGHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トークン化をおこなうヘルパ関数"
      ],
      "metadata": {
        "id": "JsoDg_oiU5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* テキスト集合から、一つの長いトークンIDの列を作る。\n",
        "  * 最大でも1000万トークンまでしか読まないことにする。"
      ],
      "metadata": {
        "id": "0R-v98hxCA9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def data_process(raw_text_iter, length=10000000):\n",
        "  ids = list()\n",
        "  for item in tqdm(raw_text_iter):\n",
        "    ids += tokenizer(item.strip())['input_ids']\n",
        "    if len(ids) > length:\n",
        "      break\n",
        "  return torch.tensor(ids[:length], dtype=torch.long)"
      ],
      "metadata": {
        "id": "DLrL0Mr1U4HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlI-ArN4dedw"
      },
      "source": [
        "## データセット\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy1UiXiFdedx"
      },
      "source": [
        "### ``torchtext``を使ったWikitext-2データセットの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText103\n",
        "\n",
        "train_iter, val_iter, test_iter = WikiText103()"
      ],
      "metadata": {
        "id": "qBT8KptLQt3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットのトークン化"
      ],
      "metadata": {
        "id": "DYxaq9L4VnwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4f7NfSededx"
      },
      "outputs": [],
      "source": [
        "train_token_ids = data_process(train_iter)\n",
        "val_token_ids = data_process(val_iter)\n",
        "test_token_ids = data_process(test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データセットはトークンIDの長い列として表されている。"
      ],
      "metadata": {
        "id": "ndiwnMTNXUTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_token_ids.shape, val_token_ids.shape, test_token_ids.shape"
      ],
      "metadata": {
        "id": "nhmJiPWLiZoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(train_token_ids[:50]))"
      ],
      "metadata": {
        "id": "j9dndkw7OYdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(train_token_ids[:50])))"
      ],
      "metadata": {
        "id": "-QTPwuocAndw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットのミニバッチ化"
      ],
      "metadata": {
        "id": "r1SGaTwJVp02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トークン列を固定長の列に切り分けるヘルパ関数を定義する。\n",
        "  * トークン列を、バッチサイズの本数に分割する。\n",
        "  * ただし、分割されたトークン列は、縦向きに並べる。\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & ... & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "* これを後で縦方向に細かく分割する。\n",
        "  * shapeが[入力シーケンス長, バッチサイズ]のミニバッチが得られる。"
      ],
      "metadata": {
        "id": "XneOrqUY_R14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, batch_size):\n",
        "  data_len = data.size(0)\n",
        "  # 割り切れずに余る分の末尾のトークンは捨てる。\n",
        "  full_seq_len = data_len // batch_size\n",
        "  data = data[:full_seq_len * batch_size]\n",
        "  # `t()`は転置をとる操作\n",
        "  data = data.reshape(batch_size, full_seq_len).t()\n",
        "  return data.to(device)"
      ],
      "metadata": {
        "id": "V6xTYLO9VIDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データセットをミニバッチ化する。"
      ],
      "metadata": {
        "id": "PRK4nNvCVSYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "eval_batch_size = 16\n",
        "train_data = batchify(train_token_ids, batch_size)\n",
        "val_data = batchify(val_token_ids, eval_batch_size)\n",
        "test_data = batchify(test_token_ids, eval_batch_size)"
      ],
      "metadata": {
        "id": "WnTb_E9ChquE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "w8VOQ_-Xiup6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(train_data[:50,0])))"
      ],
      "metadata": {
        "id": "Zpcr0ThMPfTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r65rHzSGdedy"
      },
      "source": [
        "### 入力列とターゲットのペアの作成\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ersX1ab1dedy"
      },
      "source": [
        "* ``get_batch()`` は、入力列とターゲットのペアを作る関数。\n",
        "* 変数``bptt``で指定された長さの短い列に、元のトークン列を分割する。\n",
        "* ターゲットは、一次元に潰しておく。\n",
        " * 損失関数の計算にはこの方が都合がいいため。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpE0wCk2dedy"
      },
      "outputs": [],
      "source": [
        "# シーケンスの最大長\n",
        "max_seq_len = 128\n",
        "\n",
        "def get_batch(source, i):\n",
        "  # i はオフセットを表す。\n",
        "  # sourceの形は[full_seq_len, batch_size]\n",
        "  # dataの形は[max_seq_len, batch_size]\n",
        "  # targetの形は[max_seq_len * batch_size]\n",
        "  seq_len = min(max_seq_len, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  target = source[i+1:i+1+seq_len]\n",
        "  return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLTBk6IVdedy"
      },
      "source": [
        "## モデルの作成\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNpAHFMvdedz"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.vocab_size  # 語彙サイズ\n",
        "embed_size = 256  # トークンembeddingの次元\n",
        "hidden_dim = 256  # nn.TransformerEncoderの隠れ層のサイズ\n",
        "n_layers = 2  # nn.TransformerEncoderLayerの層の数\n",
        "n_head = 2  # nn.MultiheadAttentionのヘッドの数\n",
        "dropout = 0.1  # dropoutの確率\n",
        "model = TransformerModel(vocab_size, embed_size, n_head, hidden_dim, n_layers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CY7Bl3ededz"
      },
      "source": [
        "## モデルの訓練\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ],
      "metadata": {
        "id": "uzdyAGprYjL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 1e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "metadata": {
        "id": "P-SbgdUwXJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練のためのヘルパ関数"
      ],
      "metadata": {
        "id": "c3ekonXRXsZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luKNMEyidedz"
      },
      "outputs": [],
      "source": [
        "def train(model):\n",
        "  model.train()  # 訓練モード\n",
        "  total_loss = 0.\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "  src_mask = generate_square_subsequent_mask(max_seq_len).to(device)\n",
        "\n",
        "  num_batches = len(train_data) // max_seq_len\n",
        "  full_seq_len = len(train_data)\n",
        "  for batch, i in enumerate(range(0, full_seq_len - 1, max_seq_len)):\n",
        "    data, target = get_batch(train_data, i)\n",
        "    seq_len = data.size(0)\n",
        "    if seq_len != max_seq_len:  # 最後のミニバッチだけ長さが短い\n",
        "      src_mask = src_mask[:seq_len, :seq_len]\n",
        "    output = model(data, src_mask)\n",
        "    loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "            f'lr {lr:.3e} | ms/batch {ms_per_batch:5.2f} | '\n",
        "            f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "      total_loss = 0\n",
        "      start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 評価のためのヘルパ関数"
      ],
      "metadata": {
        "id": "XGhKe36qXxgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, eval_data):\n",
        "  model.eval()  # 評価モード\n",
        "  total_loss = 0.\n",
        "  src_mask = generate_square_subsequent_mask(max_seq_len).to(device)\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, eval_data.size(0) - 1, max_seq_len):\n",
        "      data, target = get_batch(eval_data, i)\n",
        "      seq_len = data.size(0)\n",
        "      if seq_len != max_seq_len:\n",
        "        src_mask = src_mask[:seq_len, :seq_len]\n",
        "      output = model(data, src_mask)\n",
        "      loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "      total_loss += seq_len * loss.item()\n",
        "  return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "lPha2fJ_XwoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2LJA57tdedz"
      },
      "source": [
        "### 学習の実行\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルを保存するパスの設定"
      ],
      "metadata": {
        "id": "mZifzbxOdcBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "working_directory = os.getcwd() # ここを自分のGoogle Driveのフォルダに変更\n",
        "best_model_params_path = os.path.join(working_directory, \"best_model_params.pt\")\n",
        "print(f\"save path: {best_model_params_path}\")"
      ],
      "metadata": {
        "id": "s1qWwKbEc0Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* trainingのループを動かす。"
      ],
      "metadata": {
        "id": "pkUwr4kKfhLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2SGi1C3ded0"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = model\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, val_data)\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(\n",
        "      f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "      f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}'\n",
        "      )\n",
        "  print('-' * 89)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLNGCgNmded0"
      },
      "source": [
        "## テストセット上での評価\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BiXfr0Mded0"
      },
      "outputs": [],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(\n",
        "    f'| End of training | test loss {test_loss:5.2f} | '\n",
        "    f'test ppl {test_ppl:8.2f}'\n",
        "    )\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テキストの生成"
      ],
      "metadata": {
        "id": "lgAnVsswkE97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I couldn't sleep last night. Because I was\"\n",
        "token_ids = torch.tensor([tokenizer(text)[\"input_ids\"]], dtype=torch.int).to(device)\n",
        "src_mask = generate_square_subsequent_mask(len(token_ids)).to(device)\n",
        "output = model(token_ids, src_mask)"
      ],
      "metadata": {
        "id": "UoV3uczKgSOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "k-VvGXK4gsf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0,-1,:].argmax()"
      ],
      "metadata": {
        "id": "7dT1GUiCjCZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(output[0,-1,:].argmax().reshape(-1))"
      ],
      "metadata": {
        "id": "iF5ssGvugvSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "  token_ids = torch.cat([token_ids, output[0,-1,:].argmax().reshape(1,-1)], dim=1)\n",
        "  src_mask = generate_square_subsequent_mask(len(token_ids)).to(device)\n",
        "  output = model(token_ids, src_mask)\n",
        "  print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(token_ids.reshape(-1))))"
      ],
      "metadata": {
        "id": "U_5lF10qi09d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* 最低限、上のコードの動作確認をしよう。\n",
        "* 余裕があれば、validation perplexityの値をどこまで減らせるか、チューニングしてみよう。"
      ],
      "metadata": {
        "id": "ulmTvMemYuzb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QT_kugVcYFf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}