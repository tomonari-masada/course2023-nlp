{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/11_language_modeling_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI5WU1A-dedt"
      },
      "source": [
        "# Transformerを使った言語モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、PyTorchのチュートリアルをなぞるだけ。\n",
        " * https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "* このコードは時系列データにも流用できる。\n"
      ],
      "metadata": {
        "id": "c5T5a3euRYPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ランタイムのタイプをGPUにしておく。**"
      ],
      "metadata": {
        "id": "btXTiJr4fuwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "fjdVdxIgRvPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `torchdata`のインストール\n",
        "* `torchtext`からデータセットをロードする際に必要となる。"
      ],
      "metadata": {
        "id": "qFoSDqwyTdZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "id": "O1s1fs0kTc9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ランタイムを再起動する。**"
      ],
      "metadata": {
        "id": "3BjizPuYZlCj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPax_f4Pdedv"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sNu1jEjdedu"
      },
      "source": [
        "## モデルの定義\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer encoderモデル"
      ],
      "metadata": {
        "id": "tcusOPvMYOvk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y9cKzXcdedu"
      },
      "source": [
        "* PyTorchの``nn.TransformerEncoder``を使う\n",
        " * https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "\n",
        "* デフォルトの設定で``batch_first=False``になっていることに注意。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "  def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "               nlayers: int, dropout: float = 0.5):\n",
        "    super().__init__()\n",
        "    # 入力されるベクトルの次元（今回はtoken embeddingの次元）\n",
        "    self.d_model = d_model\n",
        "    # 位置エンコーディング\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "    # 多層のエンコーダを作成\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    # 入力の埋め込み層\n",
        "    self.encoder = nn.Embedding(ntoken, d_model)\n",
        "    # 単語ロジットを出力する全結合層（ntokenは語彙サイズ）\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "    # 今回は、自前の初期化を使ってみる\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "  def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "    src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "    src = self.pos_encoder(src)\n",
        "    output = self.transformer_encoder(src, src_mask)\n",
        "    output = self.decoder(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "WGotCgURRszq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 上三角行列のマスクを作る関数\n",
        "* 言語モデルは、次のトークンを予測するモデル。\n",
        "* よって、過去のトークンだけを見るようにしないといけない。"
      ],
      "metadata": {
        "id": "i9VarWt-SSR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "  #上三角行列を生成する。上三角が-inf、対角成分含めた残りはゼロ。\n",
        "  return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "ll5b1z_RSQUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jep-B49dedv"
      },
      "source": [
        "### 位置エンコーディング\n",
        "* シーケンス内でのトークンの絶対的な位置をベクトルで表現する。\n",
        " * 参考資料 https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/positional-encoding/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6JPtiRhdedw"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position = torch.arange(max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(max_len, 1, d_model)\n",
        "    pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "    # 下のように書くと、上で作成したpeがこのモジュールのパラメータの一部になる。\n",
        "    self.register_buffer('pe', pe)\n",
        "\n",
        "  def forward(self, x: Tensor) -> Tensor:\n",
        "    # テンソルxの形は[seq_len, batch_size, embedding_dim]\n",
        "    x = x + self.pe[:x.size(0)]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlI-ArN4dedw"
      },
      "source": [
        "## データセット\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy1UiXiFdedx"
      },
      "source": [
        "### ``torchtext``を使ったWikitext-2データセットの読み込み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eejyY3Dqdedx"
      },
      "source": [
        "* 語彙集合は訓練データから作る。（テストデータを混ぜてはいけない。）\n",
        " * https://pytorch.org/text/stable/vocab.html\n",
        "* 未知語（低頻度語）は`<unk>`で表す。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# 語彙集合を作る\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "\n",
        "# 未知語を表す特殊トークンを追加する\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "3fHQIgyDhKOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"this is a pen.\")"
      ],
      "metadata": {
        "id": "2VMUswjvAXWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(tokenizer(\"this is a pen.\"))"
      ],
      "metadata": {
        "id": "EfTCYlEZh5j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.lookup_tokens(vocab(tokenizer(\"this is a pen.\")))"
      ],
      "metadata": {
        "id": "B1wR6cjzAGHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トークン化の関数を定義しておく。"
      ],
      "metadata": {
        "id": "JsoDg_oiU5ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter):\n",
        "  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ],
      "metadata": {
        "id": "DLrL0Mr1U4HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットのトークン化"
      ],
      "metadata": {
        "id": "DYxaq9L4VnwH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4f7NfSededx"
      },
      "outputs": [],
      "source": [
        "# train_iter は、上で語彙集合を作るときに消費されてしまっている。\n",
        "# そのため、もう一度ここで作成する。\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "\n",
        "# トークン化の実行\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データは一本の長いトークン列として表されている。"
      ],
      "metadata": {
        "id": "ndiwnMTNXUTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "nhmJiPWLiZoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:20]"
      ],
      "metadata": {
        "id": "-QTPwuocAndw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab.lookup_tokens(train_data[:20].tolist()))"
      ],
      "metadata": {
        "id": "3x7uR7hiAqYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットのミニバッチ化"
      ],
      "metadata": {
        "id": "r1SGaTwJVp02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トークン列を固定長の列に切り分け、ミニバッチ化し、GPUに送る関数を定義する。"
      ],
      "metadata": {
        "id": "i045lJ-kVCce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* トークン列が与えられると、``batchify()``は固定長の列へとトークン列を分割する。\n",
        " * 列の数がミニバッチのサイズになるように分割する。\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & ... & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "* こうすると複数のシーケンスを同時に入力できる。\n",
        "* しかし、上の例で言うと``G``が``F``に後続するという関係性が無視されてしまう。\n",
        " * どうすればいい？"
      ],
      "metadata": {
        "id": "XneOrqUY_R14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "  \"\"\"長いシーケンスを長さbszの短いシーケンスへ分割する。\n",
        "  Args:\n",
        "      data: Tensor, shape [N]\n",
        "      bsz: int, batch size\n",
        "  Returns:\n",
        "      Tensor of shape [N // bsz, bsz]\n",
        "  \"\"\"\n",
        "  seq_len = data.size(0) // bsz\n",
        "  data = data[:seq_len * bsz]\n",
        "  # t()は転置の操作\n",
        "  data = data.view(bsz, seq_len).t().contiguous()\n",
        "  return data.to(device)"
      ],
      "metadata": {
        "id": "V6xTYLO9VIDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データセットをミニバッチ化する。"
      ],
      "metadata": {
        "id": "PRK4nNvCVSYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # テンソルの形は [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "WnTb_E9ChquE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "id": "w8VOQ_-Xiup6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r65rHzSGdedy"
      },
      "source": [
        "### 入力列とターゲットのペアの作成\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ersX1ab1dedy"
      },
      "source": [
        "* ``get_batch()`` は、入力列とターゲットのペアを作る関数。\n",
        "* 変数``bptt``で指定された長さの短い列に、元のトークン列を分割する。\n",
        "* ターゲットは、一次元に潰しておく。\n",
        " * 損失関数の計算にはこの方が都合がいいため。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpE0wCk2dedy"
      },
      "outputs": [],
      "source": [
        "# シーケンスの最大長\n",
        "bptt = 35\n",
        "\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "  # i はオフセットを表す。\n",
        "  # sourceの形は[full_seq_len, batch_size]\n",
        "  # dataの形は[seq_len, batch_size]\n",
        "  # targetの形は[seq_len * batch_size]\n",
        "  seq_len = min(bptt, len(source) - 1 - i)\n",
        "  data = source[i:i+seq_len]\n",
        "  target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "  return data, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLTBk6IVdedy"
      },
      "source": [
        "## モデルの作成\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNpAHFMvdedz"
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab)  # 語彙サイズ\n",
        "emsize = 200  # トークンembeddingの次元\n",
        "d_hid = 200  # nn.TransformerEncoderの隠れ層のサイズ\n",
        "nlayers = 2  # nn.TransformerEncoderLayerの層の数\n",
        "nhead = 2  # nn.MultiheadAttentionのヘッドの数\n",
        "dropout = 0.2  # dropoutの確率\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CY7Bl3ededz"
      },
      "source": [
        "## モデルの訓練\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数と最適化アルゴリズム"
      ],
      "metadata": {
        "id": "uzdyAGprYjL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # 学習率\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "metadata": {
        "id": "P-SbgdUwXJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練のためのヘルパ関数"
      ],
      "metadata": {
        "id": "c3ekonXRXsZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "m0FwzQes4KJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luKNMEyidedz"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module) -> None:\n",
        "  model.train()  # 訓練モード\n",
        "  total_loss = 0.\n",
        "  log_interval = 200\n",
        "  start_time = time.time()\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "  num_batches = len(train_data) // bptt\n",
        "  for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    seq_len = data.size(0)\n",
        "    if seq_len != bptt:  # 最後のミニバッチだけ長さが短い\n",
        "      src_mask = src_mask[:seq_len, :seq_len]\n",
        "    output = model(data, src_mask)\n",
        "    loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    if batch % log_interval == 0 and batch > 0:\n",
        "      lr = scheduler.get_last_lr()[0]\n",
        "      ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "      cur_loss = total_loss / log_interval\n",
        "      ppl = math.exp(cur_loss)\n",
        "      print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "            f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "            f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "      total_loss = 0\n",
        "      start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 評価のためのヘルパ関数"
      ],
      "metadata": {
        "id": "XGhKe36qXxgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "  model.eval()  # 評価モード\n",
        "  total_loss = 0.\n",
        "  src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "  with torch.no_grad():\n",
        "    for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "      data, targets = get_batch(eval_data, i)\n",
        "      seq_len = data.size(0)\n",
        "      if seq_len != bptt:\n",
        "        src_mask = src_mask[:seq_len, :seq_len]\n",
        "      output = model(data, src_mask)\n",
        "      output_flat = output.view(-1, ntokens)\n",
        "      total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "  return total_loss / (len(eval_data) - 1)"
      ],
      "metadata": {
        "id": "lPha2fJ_XwoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2LJA57tdedz"
      },
      "source": [
        "### 訓練の実行\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2SGi1C3ded0"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  epoch_start_time = time.time()\n",
        "  train(model)\n",
        "  val_loss = evaluate(model, val_data)\n",
        "  val_ppl = math.exp(val_loss)\n",
        "  elapsed = time.time() - epoch_start_time\n",
        "  print('-' * 89)\n",
        "  print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "        f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "  print('-' * 89)\n",
        "\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLNGCgNmded0"
      },
      "source": [
        "## テストセット上での評価\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BiXfr0Mded0"
      },
      "outputs": [],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* 最低限、上のコードの動作確認をしよう。\n",
        "* 余裕があれば、validation perplexityの値をどこまで減らせるか、チューニングしてみよう。"
      ],
      "metadata": {
        "id": "ulmTvMemYuzb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QT_kugVcYFf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}