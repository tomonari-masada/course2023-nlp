{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14QeLWJQtU0qmXn4kA0yuFoitqh9vK0tO",
      "authorship_tag": "ABX9TyMe/z1Yd/gvybSMnSktRuCt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/09_finetuning_BERT_for_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 感情分析のための日本語BERTのfine-tuning\n",
        "* 事前学習済みモデルをfine-tuningによってカスタマイズする方法を学ぶ。"
      ],
      "metadata": {
        "id": "gn77EKx-AQ4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transformersアーキテクチャについては、以下を参照。\n",
        " * https://arxiv.org/abs/1706.03762\n",
        "* 以下は解説記事。\n",
        " * http://jalammar.github.io/illustrated-transformer/\n",
        " * http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ],
      "metadata": {
        "id": "4KAt7vAHMJHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はHugging FaceのTransformersライブラリからBERTを使う。\n",
        " * https://huggingface.co/docs/transformers/\n",
        "* BERTについては、以下を参照。\n",
        " * https://arxiv.org/abs/1810.04805"
      ],
      "metadata": {
        "id": "ZPo8hQCQJvPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ランタイムのタイプはGPUにしておくこと。**"
      ],
      "metadata": {
        "id": "WlO5x3aiJ7t5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要なライブラリのインストール"
      ],
      "metadata": {
        "id": "MIAX9AypAOD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "id": "P1RLfEe7IGBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### トークナイザを動かすために必要なモジュールのインストール"
      ],
      "metadata": {
        "id": "4KA_-2z3x7vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi ipadic"
      ],
      "metadata": {
        "id": "uZo3L7KdNO5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インポート"
      ],
      "metadata": {
        "id": "haWKediZe0Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 日本語BERTとしては、今回、下記のモデルを使う。\n",
        "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\""
      ],
      "metadata": {
        "id": "WgU9hUOwZxUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット"
      ],
      "metadata": {
        "id": "leR994WFKDhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WRIME: 主観と客観の感情分析データセット\n",
        "* https://github.com/ids-cv/wrime"
      ],
      "metadata": {
        "id": "gijzr-XxBV0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このデータセットは、元々は-2, -1, 0, ,1 2の５値で感情極性を表現している。\n",
        "* 今回は、**ネガティブ、ニュートラル、ポジティブの3値**に単純化することにする。\n",
        " * 余裕がある方は、5値分類のままファインチューニングを実施してみてください。"
      ],
      "metadata": {
        "id": "llM-7ozzEuGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"shunk031/wrime\", \"ver2\")"
      ],
      "metadata": {
        "id": "0hxETcaWJeU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "RG6UEJm3JlQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、ポジティヴ／ニュートラル／ネガティヴの3値分類問題として、感情分析を行なう。\n",
        " * ポジティブのラベルを2、ニュートラルのラベルを1、ネガティブのラベルを0と設定する。"
      ],
      "metadata": {
        "id": "j-R1QXWvMdcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_sentiment_label(example):\n",
        "  sentiment_dic = {-2:0, -1:0, 0:1, 1:2, 2:2}\n",
        "  sentiment = example['avg_readers']['sentiment']\n",
        "  example['label'] = sentiment_dic[sentiment]\n",
        "  return example\n",
        "\n",
        "for key in dataset:\n",
        "  dataset[key] = dataset[key].map(modify_sentiment_label)"
      ],
      "metadata": {
        "id": "zL-mTWmPgaGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 3つのクラスのサイズを調べてみる。"
      ],
      "metadata": {
        "id": "9_9zgDbaiFTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "for key in dataset:\n",
        "  labels = []\n",
        "  for example in dataset[key]:\n",
        "    labels.append(example[\"label\"])\n",
        "  print(key, Counter(labels))"
      ],
      "metadata": {
        "id": "SuExVoK_hG8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トークナイザ"
      ],
      "metadata": {
        "id": "1jh6cchmNEI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 日本語BERTのトークナイザをダウンロードしておく。"
      ],
      "metadata": {
        "id": "EDMdqhAgBlzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "SBCg50HXIRH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "U4GEI0_aTKfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しに、一つのテキストをトークナイズしてみる。"
      ],
      "metadata": {
        "id": "yNxZq5fsObhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(dataset[\"train\"][\"sentence\"][0], padding=True, return_tensors=\"pt\")\n",
        "print(model_inputs)"
      ],
      "metadata": {
        "id": "kj2aZlG5NczW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `token_type_ids`については、下記URLを参照。\n",
        " * https://huggingface.co/docs/transformers/main/en/glossary#token-type-ids"
      ],
      "metadata": {
        "id": "YWFIeg4yUJca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* インデックスの列をトークン列に戻してみる。\n",
        " * 先頭と末尾のspecial tokensに注意。"
      ],
      "metadata": {
        "id": "eIYqSN9-3DOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(model_inputs['input_ids'][0]))"
      ],
      "metadata": {
        "id": "moJS6cdKrKpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "f6Ho1E7rWr6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PyTorchのDataLoaderを利用して、ミニバッチをランダムな順で取ってこれるようにする。"
      ],
      "metadata": {
        "id": "n0RDAGmoB0S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* どんなミニバッチにしたいかをcollation用関数で定義する。\n",
        " * テキストはトークナイザを通した状態でミニバッチに含ませる。\n",
        " * ラベルはPyTorchのTensorに変換してミニバッチに含ませる。"
      ],
      "metadata": {
        "id": "jNf8o2eJCC84"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回使っているトークナイザは、テキストの長さが揃っていないときのpaddingの処理までおこなってくれる。"
      ],
      "metadata": {
        "id": "4utDx1e3K2u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  texts = list()\n",
        "  labels = list()\n",
        "  for instance in batch:\n",
        "    texts.append(instance[\"sentence\"])\n",
        "    labels.append(instance[\"label\"])\n",
        "  model_input = tokenizer(texts, padding=True, return_tensors=\"pt\")\n",
        "  return model_input.to(device), torch.tensor(labels).long().to(device)"
      ],
      "metadata": {
        "id": "tZD0FofKPONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train/Dev/Testの各スプリットについて、DataLoaderのインスタンスを作成する。\n",
        " * ミニバッチのサイズは、チューニングした方が良い。"
      ],
      "metadata": {
        "id": "0GVbKonPCXPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "loaders = {}\n",
        "for key in dataset:\n",
        "  loaders[key] = DataLoader(\n",
        "      dataset[key],\n",
        "      batch_size=BATCH_SIZE,\n",
        "      shuffle=(key == \"train\"),\n",
        "      collate_fn=collate_fn,\n",
        "      )"
      ],
      "metadata": {
        "id": "yIrrVaEvP0m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しに、一個、ミニバッチを訓練データから取得してみる。"
      ],
      "metadata": {
        "id": "jL7SHnvKCd4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, label = next(iter(loaders[\"train\"]))\n",
        "print(input)\n",
        "print(label)"
      ],
      "metadata": {
        "id": "vK8LcLZtPZAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 事前学習済み日本語BERT"
      ],
      "metadata": {
        "id": "_gxtPrEhQMwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの取得"
      ],
      "metadata": {
        "id": "SYsit6SoCpxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = AutoModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "N0X5cByZQC_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert"
      ],
      "metadata": {
        "id": "O27vm9nHUeFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このBERTの単語埋め込みの次元数を調べる。"
      ],
      "metadata": {
        "id": "imDADk2A8PJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert.embeddings.word_embeddings.weight.shape"
      ],
      "metadata": {
        "id": "gyTjlNuq4Icd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このBERTのlayer数を調べる。"
      ],
      "metadata": {
        "id": "qRGPM1-EDHxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(bert.encoder.layer)"
      ],
      "metadata": {
        "id": "5JUKGE0MCz-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 最後のlayerだけ表示する。"
      ],
      "metadata": {
        "id": "3swqsR0rDQGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert.encoder.layer[-1]"
      ],
      "metadata": {
        "id": "rEkVHql_DPvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pooler\n",
        "* poolerは、最初のトークンに対応する出力を受け取っている。\n",
        "* 活性化関数tanhを使っている。\n",
        " * https://github.com/huggingface/transformers/blob/e3a4bd2bee212a2d0fd9f03b27fe7bfc1debe42d/src/transformers/models/bert/modeling_bert.py#L654\n"
      ],
      "metadata": {
        "id": "zBoqcnQ7PjSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert.pooler"
      ],
      "metadata": {
        "id": "1p7tpw0yMjVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 分類モデルの定義"
      ],
      "metadata": {
        "id": "YATFp98-3wEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* [CLS]トークンに対応する出力だけでなく、全トークンに対応する出力の平均も併せて使うことにする。"
      ],
      "metadata": {
        "id": "HhfIsjLHkrYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTTextSentiment(nn.Module):\n",
        "  def __init__(self, bert, num_class):\n",
        "    super().__init__()\n",
        "    self.bert = bert\n",
        "    self.embdim = bert.embeddings.word_embeddings.weight.size(1)\n",
        "    self.num_class = num_class\n",
        "    self.fc = nn.Linear(self.embdim * 2, self.num_class)\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.bert(**input)\n",
        "    pooler_output = output.pooler_output\n",
        "    # poolerに合わせて、tanhを適用しておく。\n",
        "    mean_output = torch.tanh(output.last_hidden_state).mean(1)\n",
        "    logit = self.fc(torch.cat([pooler_output, mean_output], -1))\n",
        "    return logit"
      ],
      "metadata": {
        "id": "a3gToTKN3ufq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = 3\n",
        "model = BERTTextSentiment(bert, num_class).to(device)"
      ],
      "metadata": {
        "id": "lERVrkI86NH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 最後のlayerと、poolerだけfinetuningするよう、設定する。"
      ],
      "metadata": {
        "id": "tLSLnVzmDZUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.bert.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in model.bert.encoder.layer[-1].parameters():\n",
        "  param.requires_grad = True\n",
        "for param in model.bert.pooler.parameters():\n",
        "  param.requires_grad = True"
      ],
      "metadata": {
        "id": "Nv1BgNpmbg-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 試しに、ミニバッチを入力してみる。"
      ],
      "metadata": {
        "id": "X-6yHjJFDq_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input, _ = next(iter(loaders[\"train\"]))\n",
        "logit = model(input)\n",
        "print(logit)"
      ],
      "metadata": {
        "id": "PsxbvBbSQX2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 最適化アルゴリズムと損失関数"
      ],
      "metadata": {
        "id": "kFs0CPh4D08X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 学習率はチューニングする必要あり。"
      ],
      "metadata": {
        "id": "Jq08EG_mDQ4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "4CSgk32RQxYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練を行なう関数"
      ],
      "metadata": {
        "id": "eB8UZcGp-w8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader):\n",
        "  model.train()\n",
        "  total_loss, total_acc, total_count = 0, 0, 0\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    input, label = batch\n",
        "    logit = model(input)\n",
        "    loss = criterion(logit, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    n_instances = label.size(0)\n",
        "    total_loss += loss.item() * n_instances\n",
        "    total_acc += (logit.argmax(-1) == label).sum().item()\n",
        "    total_count += n_instances\n",
        "    if (i + 1) % 100 == 0:\n",
        "      print(f\"===>{i+1} | acc {total_acc/total_count:.4f}\")\n",
        "  return total_loss / total_count, total_acc / total_count\n"
      ],
      "metadata": {
        "id": "Dc_BQQO-oJnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価を行なう関数"
      ],
      "metadata": {
        "id": "ThfayewS-0yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(dataloader):\n",
        "  model.eval()\n",
        "  total_loss, total_acc, total_count = 0, 0, 0\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    input, label = batch\n",
        "    with torch.no_grad():\n",
        "      logit = model(input)\n",
        "      loss = criterion(logit, label)\n",
        "    n_instances = label.size(0)\n",
        "    total_loss += loss.item() * n_instances\n",
        "    total_acc += (logit.argmax(-1) == label).sum().item()\n",
        "    total_count += n_instances\n",
        "    if (i + 1) % 100 == 0:\n",
        "      print(f\"===>{i+1} | acc {total_acc/total_count:.4f}\")\n",
        "  return total_loss / total_count, total_acc / total_count"
      ],
      "metadata": {
        "id": "HwFn1DRr-2li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fine-tuningの実行"
      ],
      "metadata": {
        "id": "ImjC2Y6XD9EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 6):\n",
        "  loss, acc = train(loaders[\"train\"])\n",
        "  dev_loss, dev_acc = evaluation(loaders[\"validation\"])\n",
        "  print(f'> epoch {epoch} | train loss {loss:.3f} | train acc {acc:.4f} || '\n",
        "      f'dev loss {dev_loss:.3f} | dev acc {dev_acc:.4f}')"
      ],
      "metadata": {
        "id": "RVBoXE8VRr11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* 分類性能が上がるよう、ハイパーパラメータをチューニングしてみよう。\n",
        " * 学習率、ミニバッチのサイズ、分類用の全結合層の層数、etc...\n",
        "* [発展] 今回のデータセットの、元々の5値分類で、fine-tuningを実行してみよう。"
      ],
      "metadata": {
        "id": "-VIhmiiwBCMm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsps8jPKbBY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}