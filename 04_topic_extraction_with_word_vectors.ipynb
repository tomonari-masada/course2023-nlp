{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO6dDGfvMMNXBqFHt324zE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/04_topic_extraction_with_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 単語ベクトルを利用したトピック抽出"
      ],
      "metadata": {
        "id": "NZMvfq2188Jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 手順\n",
        "* テキストを埋め込む。\n",
        "* embeddingsをクラスタリングする。\n",
        "  * トピック抽出のつもり。\n",
        "* クラスタの重心に近い順に単語をソートする。\n",
        "  * 各トピックを表す単語のつもり。"
      ],
      "metadata": {
        "id": "8huBm0K89od7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "Ngkx_JnS98Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ランタイムのタイプはCPUでよい。"
      ],
      "metadata": {
        "id": "8Eo3OusU84W_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1W0THra8tTb"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ja_core_news_md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット"
      ],
      "metadata": {
        "id": "aSyyhewg99z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### livedoorニュースコーパスのダウンロード"
      ],
      "metadata": {
        "id": "TFq2AMXHK7vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ],
      "metadata": {
        "id": "73yRC-Uc9BXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前処理"
      ],
      "metadata": {
        "id": "EbQe8mthLBmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tarfile\n",
        "\n",
        "tar_fname = \"ldcc-20140209.tar.gz\"\n",
        "\n",
        "def read_title(f):\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  title = next(f) # 3行目を返す：タイトル\n",
        "  title = title.decode('utf-8')\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  return re.sub(brackets_head, \"\", re.sub(brackets_tail, \"\", title))[:-1]\n",
        "\n",
        "corpus = []\n",
        "with tarfile.open(tar_fname) as tf:\n",
        "  for item in tf:\n",
        "    if \"LICENSE.txt\" in item.name:\n",
        "      continue\n",
        "    if len(item.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not item.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    fname = item.name\n",
        "    # 今回はクラス名は要らない\n",
        "    #class_name = fname.split('/')[1]\n",
        "    f = tf.extractfile(fname)\n",
        "    title = read_title(f)\n",
        "    corpus.append(title)"
      ],
      "metadata": {
        "id": "1CXFH8CI9D3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "ZLjqFArD9Gxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テキストの埋め込み"
      ],
      "metadata": {
        "id": "SQXJZ-nr9_4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_core_news_md')\n",
        "\n",
        "X = []\n",
        "for text in tqdm(corpus):\n",
        "  tokens = nlp(text)\n",
        "  X.append(tokens.vector)\n",
        "X = np.array(X)"
      ],
      "metadata": {
        "id": "L8f713la9I4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "HRAuPNIb9mga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 語彙集合の作成"
      ],
      "metadata": {
        "id": "xFoQJDsh--LT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙集合を作る目的で、形態素解析を行う。\n",
        "* 今回は、名詞、固有名詞のみを残す。"
      ],
      "metadata": {
        "id": "Q1Gmfhu8-9Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list = [\"NOUN\", \"PROPN\"]\n",
        "\n",
        "vocabulary = list()\n",
        "for text in tqdm(corpus):\n",
        "  vocabulary += [token.lemma_ for token in nlp(text) if token.pos_ in pos_list]\n",
        "vocabulary = set(vocabulary)"
      ],
      "metadata": {
        "id": "LmOvLJgQ-vX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_words = list()\n",
        "word_embeddings = list()\n",
        "for word in tqdm(vocabulary):\n",
        "  wv = nlp(word).vector\n",
        "  if (wv != 0.0).sum() == 0:\n",
        "    continue\n",
        "  embedded_words.append(word)\n",
        "  word_embeddings.append(wv)"
      ],
      "metadata": {
        "id": "mlk5VBPW_1dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = np.array(word_embeddings)\n",
        "embedded_words = np.array(embedded_words)"
      ],
      "metadata": {
        "id": "sGqRo77OAu5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings.shape"
      ],
      "metadata": {
        "id": "5yYwuqJoA7ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## クラスタリング"
      ],
      "metadata": {
        "id": "GLazRuNB-C_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はk-meansでクラスタリングする。"
      ],
      "metadata": {
        "id": "LhroFHQV_D-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=10, random_state=0, n_init=\"auto\").fit(X)"
      ],
      "metadata": {
        "id": "_LkFhbqy9VvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.cluster_centers_.shape"
      ],
      "metadata": {
        "id": "ib_8HyE5-Clp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "for label, count in sorted(list(zip(labels, counts)), key=lambda x: - x[1]):\n",
        "  print(label, count)"
      ],
      "metadata": {
        "id": "1OViUC0sBtAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = np.array(corpus)\n",
        "corpus[kmeans.labels_ == 5][:10]"
      ],
      "metadata": {
        "id": "eGHxxVP0IcRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## クラスタのラベル付け"
      ],
      "metadata": {
        "id": "1tSPSmLuLHh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* クラスタの重心に近い単語を20個選ぶ。\n",
        "  * 各トピックを表す単語のつもり。"
      ],
      "metadata": {
        "id": "p9M5-WavLMUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 内積で単語を選ぶ。"
      ],
      "metadata": {
        "id": "u59tOI5PLl5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(kmeans.n_clusters):\n",
        "  similarities = np.dot(word_embeddings, kmeans.cluster_centers_[i])\n",
        "  print(' '.join(list(embedded_words[(- similarities).argsort()[:20]])))"
      ],
      "metadata": {
        "id": "orS7xtMk-Hng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlNXTLDf-edo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}