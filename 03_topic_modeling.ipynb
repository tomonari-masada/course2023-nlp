{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/03_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPj0E52Uf-se"
      },
      "source": [
        "# トピックモデリング (topic modeling)\n",
        "\n",
        "* BoW (bag-of-words) の範囲で実現できる優れたEDA (exploratory data analysis)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io0B8e-vad-8"
      },
      "source": [
        "## 解説"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4aVnL4U4nlN"
      },
      "source": [
        "### 使いみち\n",
        "* テキストの集合から、多数の異なる話題を、それぞれの話題を端的に表す単語リストとして取り出せる。\n",
        "* 単語の出現頻度を要素とするベクトルの次元圧縮にも使えるが・・・\n",
        " * トピックモデルは、次元圧縮の手法としての性能はあまり良くない。\n",
        "* トピックモデルは、あくまでEDAの手法として使うのが吉。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZHGzh64gax"
      },
      "source": [
        "### 入力データの形式\n",
        "* 入力データは各文書における各単語の出現回数。\n",
        "* BoWとしてテキストをモデリングするので、**語順は考慮されない**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwofNZGC3bTe"
      },
      "source": [
        "### 代表的な手法: 潜在的ディリクレ配分法 (LDA; latent Dirichlet allocation)\n",
        "* LDAはテキスト集合のモデリングに使えるベイズ的な確率モデル。\n",
        " * LDAの理屈については「統計モデリング2」で。\n",
        "* 今回はsklearnの実装を使う。\n",
        " * https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gensimのLDAは非推奨\n",
        "* https://radimrehurek.com/gensim/models/ldamodel.html はお勧めしない。\n",
        "* 最大の理由: デフォルトの設定で`passes=1`\n",
        " * 深層学習の言葉で言えばepoch数が1ということ。\n",
        " * これでは、ほとんどの状況で学習が途中までしか進まない。\n",
        " * [「gensim lda トピック」でググって](https://www.google.com/search?q=gensim+lda+%E3%83%88%E3%83%94%E3%83%83%E3%82%AF)見つかるほとんどの記事でpassesを変更していない。つまり・・・\n",
        "* トピックモデルの日本語の解説記事でgensimを使っているものは、LDAの本来の性能を紹介できていない。\n",
        "* 他の理由: perplexityを底2の対数で求めている。\n",
        " * 多くの論文のconventionに反するので、gensimの出力を論文の値と比較できない。"
      ],
      "metadata": {
        "id": "aJKS311Cmwf0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTyeErX63ahO"
      },
      "source": [
        "### LDAのモデル構成\n",
        "* LDAは、テキスト集合から、$K$個のトピックを抽出する。\n",
        "* 各トピックは、$W$個の語彙の上に定義された確率分布として得られる。\n",
        " * 各トピックについて、全語彙にわたって和をとると1になる数値の集まりが得られる。\n",
        " * $\\phi_k = \\{ \\phi_{k,1}, \\ldots, \\phi_{k,W} \\}$ s.t. $\\sum_{w=1}^W \\phi_{k,w} = 1$ for $k=1, \\ldots, K$\n",
        "* LDAを使うと、各テキストにおけるトピックの混合率も分かる。\n",
        " * 各テキストについて、全てのトピックにわたって和を求めると1になる数値の集まりが得られる。\n",
        " * $\\theta_d = \\{ \\theta_{d,1}, \\ldots, \\theta_{d,K} \\}$ s.t. $\\sum_{k=1}^K \\theta_{d,k} = 1$ for each document $d$\n",
        "* 今回は、各トピックにおいて確率の高い単語を、ワードクラウドで可視化する。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "o8N1h_K8723D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwDmQK7a7ZUo"
      },
      "source": [
        "### spaCy日本語モデルのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q84eYYPP5yYh"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GS2EbCt7k-r"
      },
      "source": [
        "### データセット\n",
        "* liverdoorニュースコーパスを使う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygbsoSd36DXf"
      },
      "outputs": [],
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bj6ghpj7pki"
      },
      "source": [
        "* 前回と同じ前処理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Ugn6t46Pdr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tarfile\n",
        "\n",
        "tar_fname = \"ldcc-20140209.tar.gz\"\n",
        "\n",
        "def read_title(f):\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  title = next(f) # 3行目を返す：タイトル\n",
        "  title = title.decode('utf-8')\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  return re.sub(brackets_head, \"\", re.sub(brackets_tail, \"\", title))[:-1]\n",
        "\n",
        "corpus = []\n",
        "with tarfile.open(tar_fname) as tf:\n",
        "  for item in tf:\n",
        "    if \"LICENSE.txt\" in item.name:\n",
        "      continue\n",
        "    if len(item.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not item.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    fname = item.name\n",
        "    # 今回はクラス名は要らない\n",
        "    #class_name = fname.split('/')[1]\n",
        "    f = tf.extractfile(fname)\n",
        "    title = read_title(f)\n",
        "    corpus.append(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXircI6x6yLH"
      },
      "outputs": [],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrdP9HBb7uMK"
      },
      "source": [
        "* 形態素解析し、活用語は原形に戻す。\n",
        "* 今回は、名詞、固有名詞、動詞、形容詞、副詞のみを残す。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mj2EIpW60ZU"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "\n",
        "pos_list = [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
        "\n",
        "lemmatized = []\n",
        "for text in tqdm(corpus):\n",
        "  words = [token.lemma_ for token in nlp(text) if token.pos_ in pos_list]\n",
        "  lemmatized.append(' '.join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2QJLHI7725X"
      },
      "outputs": [],
      "source": [
        "lemmatized[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 前処理したテキスト集合をファイルとして保存しておく。"
      ],
      "metadata": {
        "id": "rJ1bja08rxC5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvbtLJoh7yK-"
      },
      "outputs": [],
      "source": [
        "with open(\"lemmatized_livedoor_corpus.txt\", \"w\") as f:\n",
        "  for text in lemmatized:\n",
        "    f.write(f\"{text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9KGwXSP5zCt"
      },
      "source": [
        "## word cloudを作る練習\n",
        "* livedoorニュースコーパス全体で一つのword cloudを作ってみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HyIraAW9ltX"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRMihsPFChug"
      },
      "source": [
        "### テキストの準備\n",
        "* 全てのテキストをつなげた長い文字列を作る。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONskFBbg91ib"
      },
      "outputs": [],
      "source": [
        "with open(\"lemmatized_livedoor_corpus.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "long_text = ' '.join([line.strip() for line in lines])\n",
        "long_text[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Gk8rV9CuwH"
      },
      "source": [
        "### 単語のフィルタリング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RPEgavqB0Fj"
      },
      "source": [
        "* 単語を出現頻度の降順にソートする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3aOqKxQ_x6S"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_freqs = Counter(long_text.split()).items()\n",
        "sorted_word_freqs = sorted(word_freqs, key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhhuDtxrAn_u"
      },
      "outputs": [],
      "source": [
        "print(sorted_word_freqs[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6JfpmopCRZa"
      },
      "source": [
        "* 適当な条件を設定してフィルタリングする。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTjCGT14Atr4"
      },
      "outputs": [],
      "source": [
        "reduced_sorted_word_freqs = [\n",
        "    (word, freq)\n",
        "    for word, freq in sorted_word_freqs\n",
        "    if freq < 320 and freq >= 5 and len(word) > 1\n",
        "    ]\n",
        "print(reduced_sorted_word_freqs[:10])\n",
        "print(reduced_sorted_word_freqs[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4wdOt9yC0HK"
      },
      "source": [
        "### word cloudの描画"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 日本語フォントのインストール"
      ],
      "metadata": {
        "id": "J_p5-5bzpREM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install fonts-ipafont-gothic"
      ],
      "metadata": {
        "id": "fZ6hkAgmoQQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP0Lj45z-fM0"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(\n",
        "    font_path=\"/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\",\n",
        "    background_color=\"white\",\n",
        "    width=1600,\n",
        "    height=900,\n",
        "    )\n",
        "wordcloud.generate_from_frequencies(dict(reduced_sorted_word_freqs))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"word_cloud.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIEqjxRTcnan"
      },
      "source": [
        "* 多数のテキストに対して、たった一つword cloudを作ったところで、何が分かるというのだろうか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd8RjBObDASl"
      },
      "source": [
        "## LDAによるEDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nu0eT7OXCzD"
      },
      "source": [
        "### データ行列の作成"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LDAの場合、単語の出現頻度をそのまま使って各文書をベクトル化する。\n",
        " * TF-IDFは使わない。\n",
        "* 先ほど決めた単語群をLDAの語彙として使う。"
      ],
      "metadata": {
        "id": "-afaQSNjm8MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = dict(reduced_sorted_word_freqs).keys()\n",
        "len(vocabulary)"
      ],
      "metadata": {
        "id": "aU-6ll0qmrG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙をファイルとして保存しておく。"
      ],
      "metadata": {
        "id": "Lg-YbgM6sRUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lemmatized_livedoor_corpus_vocabulary.txt\", \"w\") as f:\n",
        "  for word in vocabulary:\n",
        "    f.write(f\"{word}\\n\")"
      ],
      "metadata": {
        "id": "Ml5FbdDprg1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrj0mmMrCzNI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "with open(\"lemmatized_livedoor_corpus.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "corpus = [line.strip() for line in lines]\n",
        "\n",
        "# 英語の単語は小文字にしないようにする\n",
        "vectorizer = CountVectorizer(lowercase=False, vocabulary=vocabulary)\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2wVko9vXnlq"
      },
      "source": [
        "* 文書数と語彙サイズを変数にセット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAd821DGFXXp"
      },
      "outputs": [],
      "source": [
        "n_samples, n_features = X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAEThMwZjhb"
      },
      "source": [
        "### LDAによるトピック抽出\n",
        "* 内部的には、変分推論で事後分布のパラメータを推定している。\n",
        " * `learning_method=\"online\"`として、ミニバッチ式の繰り返し計算にすることを推奨。\n",
        " * バッチ処理にするよりも性能が良くなることが多い。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxdJTl_VYNHV"
      },
      "source": [
        "* 抽出するトピックの個数は`n_components`で指定する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuuSTK6fZfu7"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "n_components = 20\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_components, #要チューニング\n",
        "    doc_topic_prior=0.05, #要チューニング\n",
        "    topic_word_prior=0.01, #要チューニング\n",
        "    learning_method=\"online\",\n",
        "    max_iter=20, #学習が収束するまで増やす\n",
        "    batch_size=200,\n",
        "    random_state=12345,\n",
        "    evaluate_every=1,\n",
        "    verbose=1, #学習の進行状況をperplexityでチェックする\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWd3DND8aAVz"
      },
      "outputs": [],
      "source": [
        "lda.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_HRjNZJcbNr"
      },
      "source": [
        "### 高確率語をワードクラウドで可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o3v2CLfb1ku"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(\n",
        "    font_path=\"/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\",\n",
        "    background_color=\"white\",\n",
        "    width=1600,\n",
        "    height=900,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFf5jcX5b45d"
      },
      "outputs": [],
      "source": [
        "n_cols = 4\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_components // n_cols ,\n",
        "    n_cols,\n",
        "    figsize=(16, 16),\n",
        "    sharex=True,\n",
        "    sharey=True,\n",
        "    )\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  # キーが単語で値が重みの辞書を作っている\n",
        "  wordcloud.generate_from_frequencies(\n",
        "      dict(zip(vocabulary, lda.components_[i]))\n",
        "      )\n",
        "  plt.gca().imshow(wordcloud)\n",
        "  plt.gca().set_title(f\"Topic {i:02d}\")\n",
        "  plt.gca().axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDAのチューニング\n",
        "* **LDAはチューニングして使うべき。**\n",
        " * そうしないと、EDAの結果がパッとしなくなる。\n",
        "* perplexityの値ができるだけ小さくなるように、チューニングする。\n",
        "* 計算に時間がかかるからといって、`max_iter`を一桁にしないこと。\n",
        " * `max_iter`の値は十分に大きくすること。\n",
        " * perplexityの値があまり動かないところまで推定計算をちゃんと動かすため。"
      ],
      "metadata": {
        "id": "4ZrStIztoSGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### チューニングすべきパラメータ\n",
        "* `n_components`\n",
        " * 抽出するトピックの数。\n",
        " * 多すぎても、少なすぎても、分析が今ひとつになる。\n",
        " * 数千件のテキストなら、2桁のトピック数はおそらく必要。\n",
        " * 一般に、トピック数が大きいほど、`max_iter`は大きくする必要がある。\n",
        " * 最低、10, 20, 50, 100の4通りぐらいは試す。\n",
        "* `doc_topic_prior`\n",
        " * ドキュメントごとのトピック確率分布の事前分布のパラメータ。\n",
        " * 詳細は「統計モデリング2」で。\n",
        " * 最低、0.01, 0.02, 0.05, 0.1の4通りぐらいは試す。\n",
        "* `topic_word_prior`\n",
        " * トピックごとの単語確率分布の事前分布のパラメータ。\n",
        " * 詳細は「統計モデリング2」で。\n",
        " * 最低、0.01, 0.02, 0.05, 0.1の4通りぐらいは試す。"
      ],
      "metadata": {
        "id": "nHoVXLLWoZXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 上記の最低64通りを試して、validation setでのperplexityが最も小さくなる設定を探す。\n",
        "* その設定でword cloudや下記のpyLDAvisによる可視化を行う。\n",
        " * これぐらいやらないと、LDAの真価が発揮されない。\n",
        " * Web上にあるLDA関係のブログ記事等は、ほとんど、いまいちな結果をそのまま載せている。"
      ],
      "metadata": {
        "id": "ab4xSw0M9EoI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CgCE_UscGSA"
      },
      "source": [
        "## pyLDAvisによる可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* インストール"
      ],
      "metadata": {
        "id": "6YNlkQ1CqutD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD8TnZ21Q1Ex"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8QHe5IOQ_JP"
      },
      "source": [
        "* おそらくランタイムの再起動が必要。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPvA6t1djK0F"
      },
      "source": [
        "* 上で実施したLDAの学習を、下にあらためてまとめて書いておいた。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYAl4yfoR-Y2"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "with open(\"lemmatized_livedoor_corpus.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "corpus = [line.strip() for line in lines]\n",
        "\n",
        "with open(\"lemmatized_livedoor_corpus_vocabulary.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "vocabulary = [word.strip() for word in lines]\n",
        "\n",
        "vectorizer = CountVectorizer(lowercase=False, vocabulary=vocabulary)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "n_components = 20\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_components, #要チューニング\n",
        "    doc_topic_prior=0.05, #要チューニング\n",
        "    topic_word_prior=0.01, #要チューニング\n",
        "    learning_method='online',\n",
        "    max_iter=20, #学習が収束するまで増やす\n",
        "    batch_size=200,\n",
        "    random_state=12345,\n",
        "    evaluate_every=1,\n",
        "    verbose=1, #学習の進行状況をperplexityでチェックする\n",
        "    )\n",
        "lda.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X18xyTuRgD5"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "\n",
        "vis = pyLDAvis.prepare(\n",
        "  lda.components_,\n",
        "  lda.transform(X),\n",
        "  doc_lengths=X.sum(axis=1).getA1(),\n",
        "  vocab=vectorizer.get_feature_names_out(),\n",
        "  term_frequency=X.sum(axis=0).getA1(),\n",
        "  #mds=\"tsne\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEdyCc4jV4mm"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3-a2UL1tKjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12MxXsGfePft6pAHKkHrg23Yb7t_RSKiN",
      "authorship_tag": "ABX9TyM4UE51EtwZm06+2NRzmOXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}