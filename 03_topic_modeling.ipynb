{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/03_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPj0E52Uf-se"
      },
      "source": [
        "# トピックモデリング (topic modeling)\n",
        "\n",
        "* BoW (bag-of-words) の範囲で実現できる優れたEDA (exploratory data analysis)。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io0B8e-vad-8"
      },
      "source": [
        "## 解説"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4aVnL4U4nlN"
      },
      "source": [
        "### 使いみち\n",
        "* テキストの集合から、多数の異なる話題を、それぞれの話題を端的に表す単語リストとして取り出せる。\n",
        "* 単語の出現頻度を要素とするベクトルの次元圧縮にも使えるが・・・\n",
        " * トピックモデルは、次元圧縮の手法としての性能はあまり良くない。\n",
        "* EDAの手法として使うのが吉。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNZHGzh64gax"
      },
      "source": [
        "### 入力データの形式\n",
        "* 入力データは各文書における各単語の出現回数。\n",
        "* BoWとしてテキストをモデリングするので、**語順は考慮されない**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwofNZGC3bTe"
      },
      "source": [
        "### 代表的な手法: 潜在的ディリクレ配分法 (LDA; latent Dirichlet allocation)\n",
        "* LDAはテキスト集合のモデリングに使えるベイズ的な確率モデル。\n",
        " * LDAの理屈については「統計モデリング2」で。\n",
        "* 今回はsklearnの実装を使う。\n",
        " * https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gensimのLDAは非推奨\n",
        "* https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "* 最大の理由: デフォルトの設定で`passes=1`\n",
        " * 深層学習の言葉で言えばepoch数が1ということ。\n",
        " * ほとんどの状況で、学習が中途半端に終わってしまう。\n",
        " * [「gensim lda トピック」でググって](https://www.google.com/search?q=gensim+lda+%E3%83%88%E3%83%94%E3%83%83%E3%82%AF)見つかるほとんどの記事でpassesを指定していない。\n",
        " * つまり、gensimを使っているLDAの日本語解説記事の多くが、LDAの本来の性能を示せていない。\n",
        "* 他の理由: perplexityを底2の対数で求めている。\n",
        " * 多くの論文のconventionに反するので、gensimの出力を論文の値と比較できない。"
      ],
      "metadata": {
        "id": "aJKS311Cmwf0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTyeErX63ahO"
      },
      "source": [
        "### LDAのモデル構成\n",
        "* LDAは、テキスト集合から、$K$個のトピックを抽出する。\n",
        "* 各トピックは、$W$個の語彙の上に定義された確率分布として得られる。\n",
        " * 各トピックについて、全語彙にわたって和をとると1になる数値の集まりが得られる。\n",
        " * $\\phi_k = \\{ \\phi_{k,1}, \\ldots, \\phi_{k,W} \\}$ s.t. $\\sum_{w=1}^W \\phi_{k,w} = 1$ for $k=1, \\ldots, K$\n",
        "* LDAを使うと、各テキストにおけるトピックの混合率も分かる。\n",
        " * 各テキストについて、全てのトピックにわたって和を求めると1になる数値の集まりが得られる。\n",
        " * $\\theta_d = \\{ \\theta_{d,1}, \\ldots, \\theta_{d,K} \\}$ s.t. $\\sum_{k=1}^K \\theta_{d,k} = 1$ for each document $d$\n",
        "* 今回は、各トピックにおいて確率の高い単語を、ワードクラウドで可視化する。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "o8N1h_K8723D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwDmQK7a7ZUo"
      },
      "source": [
        "### spaCy日本語モデルのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q84eYYPP5yYh"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 日本語フォントのインストール"
      ],
      "metadata": {
        "id": "s2td51DGlix7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -y install fonts-ipafont-gothic"
      ],
      "metadata": {
        "id": "fZ6hkAgmoQQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 可視化ツールのインストール"
      ],
      "metadata": {
        "id": "EC5ZBVR1z6Ye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD8TnZ21Q1Ex"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8QHe5IOQ_JP"
      },
      "source": [
        "**ランタイムを再起動する。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GS2EbCt7k-r"
      },
      "source": [
        "## データセット\n",
        "* liverdoorニュースコーパスを使う。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygbsoSd36DXf"
      },
      "outputs": [],
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bj6ghpj7pki"
      },
      "source": [
        "### 前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Ugn6t46Pdr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tarfile\n",
        "\n",
        "tar_fname = \"ldcc-20140209.tar.gz\"\n",
        "\n",
        "def read_title(f):\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  title = next(f) # 3行目を返す：タイトル\n",
        "  title = title.decode('utf-8')\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  return re.sub(brackets_head, \"\", re.sub(brackets_tail, \"\", title))[:-1]\n",
        "\n",
        "corpus = []\n",
        "with tarfile.open(tar_fname) as tf:\n",
        "  for item in tf:\n",
        "    if \"LICENSE.txt\" in item.name:\n",
        "      continue\n",
        "    if len(item.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not item.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    fname = item.name\n",
        "    # 今回はクラス名は要らない\n",
        "    #class_name = fname.split('/')[1]\n",
        "    f = tf.extractfile(fname)\n",
        "    title = read_title(f)\n",
        "    corpus.append(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXircI6x6yLH"
      },
      "outputs": [],
      "source": [
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrdP9HBb7uMK"
      },
      "source": [
        "* spaCyで形態素解析し、活用語は原形に戻す。\n",
        " * 今回は、名詞、固有名詞、動詞、形容詞、副詞のみを残す。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mj2EIpW60ZU"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "\n",
        "pos_list = [\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
        "\n",
        "lemmatized = []\n",
        "for text in tqdm(corpus):\n",
        "  words = [token.lemma_ for token in nlp(text) if token.pos_ in pos_list]\n",
        "  lemmatized.append(' '.join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2QJLHI7725X"
      },
      "outputs": [],
      "source": [
        "lemmatized[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 前処理したコーパスを保存しておく。"
      ],
      "metadata": {
        "id": "rJ1bja08rxC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前処理後のコーパスの保存"
      ],
      "metadata": {
        "id": "xzma3CpemvCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvbtLJoh7yK-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "#save_path = \"./\"\n",
        "save_path = \"/content/drive/MyDrive/2023courses/nlp/\"\n",
        "\n",
        "with open(os.path.join(save_path, \"lemmatized_livedoor_corpus.txt\"), \"w\") as f:\n",
        "  for text in lemmatized:\n",
        "    f.write(f\"{text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nu0eT7OXCzD"
      },
      "source": [
        "### データ行列の作成"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LDAの場合、単語の出現頻度をそのまま使って各文書をベクトル化する。"
      ],
      "metadata": {
        "id": "-afaQSNjm8MF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrj0mmMrCzNI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/2023courses/nlp/\"\n",
        "\n",
        "with open(os.path.join(save_path, \"lemmatized_livedoor_corpus.txt\"), \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "corpus = [line.strip() for line in lines]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練用コーパスと検証用コーパスに分割する。"
      ],
      "metadata": {
        "id": "Deqy2KFQONdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_corpus, valid_corpus = train_test_split(corpus, test_size=0.1, random_state=12345)"
      ],
      "metadata": {
        "id": "3Jz87fq1OS0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データ行列を作成する。"
      ],
      "metadata": {
        "id": "cjsEWAj4OWg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAd821DGFXXp"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(min_df=5, max_df=0.2)\n",
        "X_train = vectorizer.fit_transform(train_corpus).toarray()\n",
        "n_samples, n_features = X_train.shape\n",
        "print(f\"training corpus size: {n_samples}, vocabulary size: {n_features}\")\n",
        "X_valid = vectorizer.transform(valid_corpus).toarray()\n",
        "print(f\"validation corpus size: {X_valid.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 訓練データ全体のword cloud"
      ],
      "metadata": {
        "id": "KWkAS3vXl2oE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 描画のための準備"
      ],
      "metadata": {
        "id": "e8EegAZSl7YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "1IPB14PelySy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 訓練データ全体での各単語の出現頻度を求める。"
      ],
      "metadata": {
        "id": "Sit_2oxgl9OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "word_freq = list(zip(vocabulary, X_train.sum(axis=0)))\n",
        "word_freq = sorted(word_freq, key=lambda x: - x[1])\n",
        "print(word_freq[:20])"
      ],
      "metadata": {
        "id": "y4baD5Mvl6xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* word cloudを描画する。"
      ],
      "metadata": {
        "id": "cYLiU1jdmMWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(\n",
        "    font_path=\"/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\",\n",
        "    background_color=\"white\",\n",
        "    width=1600,\n",
        "    height=900,\n",
        "    )\n",
        "wordcloud.generate_from_frequencies(dict(word_freq))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(\"word_cloud.png\")"
      ],
      "metadata": {
        "id": "N0vReJlxmMBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIEqjxRTcnan"
      },
      "source": [
        "* しかし・・・\n",
        " * テキスト集合に対して、たった一つword cloudを作ったところで、何が分かるのか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd8RjBObDASl"
      },
      "source": [
        "## LDAによるEDA\n",
        "* LDAを使うと、一つのコーパスから複数のword cloudを作ることができる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSAEThMwZjhb"
      },
      "source": [
        "### LDAのtraining\n",
        "* 内部的には、変分推論で事後分布のパラメータを推定している。\n",
        " * `learning_method=\"online\"`とすることを推奨。\n",
        "* 抽出するトピックの個数は`n_components`で指定する。\n",
        " * これがword cloudの個数になる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuuSTK6fZfu7"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "n_components = 20\n",
        "\n",
        "model = LatentDirichletAllocation(\n",
        "    n_components=n_components,\n",
        "    learning_method=\"online\",\n",
        "    max_iter=20, #学習がおおよそ収束する値に設定\n",
        "    random_state=12345,\n",
        "    evaluate_every=1, #何epochごとにモデルを評価するか\n",
        "    verbose=1, #学習の進行状況をチェック\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWd3DND8aAVz"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### perplexityメソッドで評価"
      ],
      "metadata": {
        "id": "gQrvC8I31rib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.perplexity(X_train)"
      ],
      "metadata": {
        "id": "45DOsJUbrzD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.perplexity(X_valid)"
      ],
      "metadata": {
        "id": "P6CGgffincj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* scikit-learnのLDAのperplexityメソッドは、なんだかおかしい。"
      ],
      "metadata": {
        "id": "_MqfO9Cu1wZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ハイパーパラメータのチューニング"
      ],
      "metadata": {
        "id": "ZsSXkqK_Fr-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### perplexityを求める関数\n",
        "* 各テキストでのトピック確率と、\n",
        "* 各トピックでの単語確率から、\n",
        "* 各テキストでの単語確率を求め、\n",
        "* これをもとにしてコーパスの尤度を求める、\n",
        "* ・・・というアプローチでperplexityを計算する。"
      ],
      "metadata": {
        "id": "L7x7HhNlFvdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def perplexity(model, X):\n",
        "  components = model.components_\n",
        "  topic_word_prob = components / components.sum(-1)[:, np.newaxis]\n",
        "  doc_topic_prob = model.transform(X)\n",
        "  doc_word_prob = np.dot(doc_topic_prob, topic_word_prob)\n",
        "  return np.exp(- (X * np.log(doc_word_prob)).sum() / X.sum())"
      ],
      "metadata": {
        "id": "8v5rxadh-aMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDAのtrainingのヘルパ関数"
      ],
      "metadata": {
        "id": "BERXOkXm2Rp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lda_train(n_components, max_iter=None, doc_topic_prior=None, topic_word_prior=None):\n",
        "  if max_iter is None:\n",
        "    max_iter = 20\n",
        "  model = LatentDirichletAllocation(\n",
        "      n_components=n_components,\n",
        "      doc_topic_prior=doc_topic_prior,\n",
        "      topic_word_prior=topic_word_prior,\n",
        "      learning_method=\"online\",\n",
        "      random_state=12345,\n",
        "      )\n",
        "  for epoch in range(max_iter):\n",
        "    print(f\"### epoch {epoch + 1} | \", end=\"\")\n",
        "    model.partial_fit(X_train)\n",
        "    print(f\"training perp {perplexity(model, X_train):.3f} | \", end=\"\")\n",
        "    print(f\"validation perp {perplexity(model, X_valid):.3f}\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "T1j4iWx0rJDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ハイパーパラメータのチューニング"
      ],
      "metadata": {
        "id": "lgXog9UQ2WGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 自分で決めたトピック数に合わせて・・・\n",
        "* `doc_topic_prior`と`topic_word_prior`をチューニングする。"
      ],
      "metadata": {
        "id": "uRfC2KLJ6BbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 20\n",
        "print(f\"n_components : {n_components:d}\")\n",
        "\n",
        "for doc_topic_prior in [1e-10, 1e-9, 1e-8]:\n",
        "  for topic_word_prior in [0.3, 0.5, 0.7]:\n",
        "    print(f\"doc_topic_prior : {doc_topic_prior:.1e} , \", end=\"\")\n",
        "    print(f\"topic_word_prior : {topic_word_prior:.1e}\")\n",
        "    model = lda_train(\n",
        "        n_components,\n",
        "        max_iter=10,\n",
        "        doc_topic_prior=doc_topic_prior,\n",
        "        topic_word_prior=topic_word_prior,\n",
        "        )"
      ],
      "metadata": {
        "id": "x91KXGbVGeFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 最適な`doc_topic_prior`の値が小さいのは、おそらく、テキストが短いため。"
      ],
      "metadata": {
        "id": "l0OqUNgE4phl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* word cloudの数がたくさんになっても構わないなら・・・\n",
        "* トピック数をチューニングしてもよい。"
      ],
      "metadata": {
        "id": "4pBZ6vje6EMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for topic_word_prior in [0.3, 0.5, 0.7]:\n",
        "  for n_components in [200, 150, 100]:\n",
        "    print(f\"n_components : {n_components:d} , \", end=\"\")\n",
        "    print(f\"topic_word_prior : {topic_word_prior:.1e}\")\n",
        "    model = lda_train(\n",
        "        n_components,\n",
        "        max_iter=10,\n",
        "        doc_topic_prior=1e-10,\n",
        "        topic_word_prior=topic_word_prior,\n",
        "        )"
      ],
      "metadata": {
        "id": "oIA60kM_4_9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_HRjNZJcbNr"
      },
      "source": [
        "### 高確率語をワードクラウドで可視化"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LatentDirichletAllocation(\n",
        "    n_components=20,\n",
        "    doc_topic_prior=1e-10,\n",
        "    topic_word_prior=0.5,\n",
        "    learning_method=\"online\",\n",
        "    random_state=12345,\n",
        "    )\n",
        "X = vectorizer.transform(corpus).toarray()\n",
        "for epoch in range(50):\n",
        "  print(f\"### epoch {epoch + 1} | \", end=\"\")\n",
        "  model.partial_fit(X)\n",
        "  print(f\"perplexity {perplexity(model, X):.3f}\")"
      ],
      "metadata": {
        "id": "cKXd_O1Nx8FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o3v2CLfb1ku"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(\n",
        "    font_path=\"/usr/share/fonts/opentype/ipafont-gothic/ipagp.ttf\",\n",
        "    background_color=\"white\",\n",
        "    width=1600,\n",
        "    height=900,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFf5jcX5b45d"
      },
      "outputs": [],
      "source": [
        "n_cols = 4\n",
        "\n",
        "fig, axes = plt.subplots(\n",
        "    n_components // n_cols ,\n",
        "    n_cols,\n",
        "    figsize=(16, 16),\n",
        "    sharex=True,\n",
        "    sharey=True,\n",
        "    )\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  fig.add_subplot(ax)\n",
        "  # キーが単語で値が重みの辞書を作っている\n",
        "  wordcloud.generate_from_frequencies(\n",
        "      dict(zip(vocabulary, model.components_[i]))\n",
        "      )\n",
        "  plt.gca().imshow(wordcloud)\n",
        "  plt.gca().set_title(f\"Topic {i:02d}\")\n",
        "  plt.gca().axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CgCE_UscGSA"
      },
      "source": [
        "## pyLDAvisによる可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X18xyTuRgD5"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "\n",
        "components = model.components_\n",
        "components = components / components.sum(-1)[:, np.newaxis]\n",
        "\n",
        "vis = pyLDAvis.prepare(\n",
        "  components,\n",
        "  model.transform(X),\n",
        "  doc_lengths=X.sum(axis=1),\n",
        "  vocab=vectorizer.get_feature_names_out(),\n",
        "  term_frequency=X.sum(axis=0),\n",
        "  #mds=\"tsne\",\n",
        ")\n",
        "pyLDAvis.display(vis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3-a2UL1tKjf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fx0E-vXipD9gqkda0_O_fzBuBrqYo-AL",
      "authorship_tag": "ABX9TyPlzpPyVvRbVezPoVzvfRqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}