{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJcAyAwaGWc15L0WHEHH/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/13_in_context_learning_with_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In-Context Learning"
      ],
      "metadata": {
        "id": "dwVCWP8hiuEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "3GIHAP_Rrgkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate auto-gptq"
      ],
      "metadata": {
        "id": "2W5-9kWlm1qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ここでランタイムを再起動する。**"
      ],
      "metadata": {
        "id": "ntPSyIO8r83H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* 今回は、やさしい日本語への書き換えのデータセットを使う。\n",
        " * https://huggingface.co/datasets/snow_simplified_japanese_corpus"
      ],
      "metadata": {
        "id": "eX6zotwEoK2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "9A6Euk7gmXNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"snow_simplified_japanese_corpus\")"
      ],
      "metadata": {
        "id": "MAMDFgFBMNVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "julWQgfCox2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset = dataset.filter(\n",
        "    lambda example: example[\"original_ja\"] != example[\"simplified_ja\"]\n",
        ")"
      ],
      "metadata": {
        "id": "f6ssdeeOo7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dataset.shape"
      ],
      "metadata": {
        "id": "D3XRYUG_OMaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = new_dataset"
      ],
      "metadata": {
        "id": "458y21GVOOvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)"
      ],
      "metadata": {
        "id": "qonkFBSIWIL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "0VgUccmMTF97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"test\"][0]"
      ],
      "metadata": {
        "id": "1hNRnf4kW6c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM\n"
      ],
      "metadata": {
        "id": "ACC0ED27qZtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、ELYZA-japanese-Llama-2-7b-instructを使う。\n",
        " * https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct\n",
        "* だが、Google Colab無料版では、この元のモデルは大きすぎて使えない・・・。\n",
        " * 20GB以上のメモリがあるGPUなら使える。\n",
        "* そこで、量子化された下記のモデルを代わりに使う。\n",
        " * https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k"
      ],
      "metadata": {
        "id": "kIsSj4bzq7Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "model_name = \"mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ-calib-ja-2k\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_name,\n",
        "    use_safetensors=True,\n",
        "    device=\"cuda:0\",\n",
        "    )\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "PYR1TtgLrGcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## プロンプト\n",
        "* LLMがうまく感情分析をしてくれそうなプロンプトを考える。\n",
        " * 下はあくまで一つの例。"
      ],
      "metadata": {
        "id": "i5-ymVqjvFDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### プロンプト作成用のヘルパ関数"
      ],
      "metadata": {
        "id": "c33BfNxjypMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]答え：\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"あなたは優秀な日本人のアシスタントです。\"\n",
        "\n",
        "def make_prompt(sample):\n",
        "  text = \"次の文を、小学生でも分かるようにやさしく書き換えてください。\\n\"\n",
        "  text += sample[\"original_ja\"]\n",
        "  prompt = \"{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "      b_inst=B_INST,\n",
        "      system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "      prompt=text,\n",
        "      e_inst=E_INST,\n",
        "      )\n",
        "  sample[\"prompt\"] = prompt\n",
        "  return sample"
      ],
      "metadata": {
        "id": "Q7gEATn-y3RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 元のテキストをプロンプトに一括変換する。"
      ],
      "metadata": {
        "id": "sU-cGHd2zRBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(make_prompt)"
      ],
      "metadata": {
        "id": "aJcIVmkjzPkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "aqp3lDmbzWJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"test\"][0]"
      ],
      "metadata": {
        "id": "taOECFLPXeo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## やさしい日本語への書き換え"
      ],
      "metadata": {
        "id": "6PahFnk2z-lU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKPWp6jdW9fF"
      },
      "outputs": [],
      "source": [
        "for i in range(100):\n",
        "  print(f'[{i+1}]' + '-'*80)\n",
        "  sample = dataset[\"train\"][i]\n",
        "  original = sample[\"original_ja\"]\n",
        "  ground_truth = sample[\"simplified_ja\"]\n",
        "  prompt = sample[\"prompt\"]\n",
        "  with torch.no_grad():\n",
        "    token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids=token_ids.to(model.device),\n",
        "        max_new_tokens=50,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "  output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n",
        "  prediction = output.split('\\n')[0]\n",
        "  print(f\"original:\\t\\t{original}\")\n",
        "  print(f\"prediction:\\t{prediction}\")\n",
        "  print(f\"ground truth:\\t{ground_truth}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"あなたは、やさしい文を書くのが得意な日本人のアシスタントです。\"\n",
        "\n",
        "def make_prompt(sample):\n",
        "  n_examples = 20\n",
        "  text = \"むずかしい文を、やさしくするには、\"\n",
        "  text += \"どう書き換えればいいですか。\\n\"\n",
        "  while n_examples > 0:\n",
        "    index = torch.randint(0, len(dataset[\"train\"]), (1,)).item()\n",
        "    example = dataset[\"train\"][index]\n",
        "    if example[\"original_ja\"] == sample[\"original_ja\"]:\n",
        "      continue\n",
        "    text += f\"例えば、\\n「{example['original_ja']}」\\nという文は、\\n\"\n",
        "    text += f\"「{example['simplified_ja']}」\\nと書き換えればいいです。\\n\"\n",
        "    n_examples -= 1\n",
        "  text += \"\\nでは、あなたに質問です。\\n\"\n",
        "  text += f\"例えば、\\n「{sample['original_ja']}」\\nという文は、\\n\"\n",
        "  text += \"どう書き換えればいいですか。\\n\"\n",
        "  prompt = \"{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "      b_inst=B_INST,\n",
        "      system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "      prompt=text,\n",
        "      e_inst=E_INST,\n",
        "      )\n",
        "  sample[\"prompt\"] = prompt\n",
        "  return sample"
      ],
      "metadata": {
        "id": "qGBPtStLX78l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  print(f'[{i+1}]' + '-'*80)\n",
        "  sample = dataset[\"train\"][i]\n",
        "  sample = make_prompt(sample)\n",
        "  original = sample[\"original_ja\"]\n",
        "  ground_truth = sample[\"simplified_ja\"]\n",
        "  prompt = sample[\"prompt\"]\n",
        "  with torch.no_grad():\n",
        "    token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids=token_ids.to(model.device),\n",
        "        max_new_tokens=50,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "  output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n",
        "  prediction = output\n",
        "  print(f\"original:\\t\\t{original}\")\n",
        "  print(f\"prediction:\\t{prediction}\")\n",
        "  print(f\"ground truth:\\t{ground_truth}\")"
      ],
      "metadata": {
        "id": "l6I1Ja2kaudo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* もっとうまくLLMに書き換えをさせるプロンプトを考えてみよう。"
      ],
      "metadata": {
        "id": "-cXKf5dF0vRp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUXcB4axyCQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}