{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/13_in_context_learning_with_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVCWP8hiuEG"
      },
      "source": [
        "# In-Context Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHUWUev69lUa"
      },
      "source": [
        "## 参考資料\n",
        "* 「ELYZAが公開した日本語LLM「ELYZA-japanese-Llama-2-7b」についての解説 : (1) 事前学習編」\n",
        "  * https://zenn.dev/elyza/articles/2fd451c944649d\n",
        "* \"Topic Modeling with Llama 2\" （量子化のやり方を参考にしました。）\n",
        "  * https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ランタイムの設定でGPUが使えるようにしておいてください。"
      ],
      "metadata": {
        "id": "nH7cOBZf9sC9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GIHAP_Rrgkp"
      },
      "source": [
        "## 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RxY2uWc9lUb"
      },
      "source": [
        "### 必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W5-9kWlm1qS"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets accelerate auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIuz_1A79lUc"
      },
      "source": [
        "### 量子化のためにバージョンを指定してbitsandbytesをインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll_QfdZM9lUc"
      },
      "outputs": [],
      "source": [
        "! pip install -U bitsandbytes>=0.39.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntPSyIO8r83H"
      },
      "source": [
        "**ここでランタイムを再起動する。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6zotwEoK2r"
      },
      "source": [
        "## データセット\n",
        "* 今回は、やさしい日本語への書き換えのデータセットを使う。\n",
        " * https://huggingface.co/datasets/snow_simplified_japanese_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A6Euk7gmXNv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAMDFgFBMNVs"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"snow_simplified_japanese_corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "julWQgfCox2f"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1V9zbvO9lUe"
      },
      "source": [
        "* 中には、書き換えの前後で文が変わらないものもある。\n",
        "  * これは、書き換えが不要であるという判断をさせるためのインスタンスと思われる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWYM4KPO9lUe"
      },
      "source": [
        "* 訓練データとテストデータへランダムに分割する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qonkFBSIWIL1"
      },
      "outputs": [],
      "source": [
        "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b-TlQ_g9lUe"
      },
      "outputs": [],
      "source": [
        "len(dataset[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VgUccmMTF97"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtkDZQXT9lUf"
      },
      "outputs": [],
      "source": [
        "len(dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hNRnf4kW6c2"
      },
      "outputs": [],
      "source": [
        "dataset[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACC0ED27qZtV"
      },
      "source": [
        "## LLMの準備\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIsSj4bzq7Cl"
      },
      "source": [
        "* 今回は、elyza/ELYZA-japanese-Llama-2-7b-fast-instructを使う。\n",
        "  * https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct\n",
        "* だが、Google Colab無料版では、この元のモデルは大きすぎて使えない・・・。\n",
        "  * 20GB以上のメモリがあるGPUなら使える。\n",
        "* そこで、bitsandbytesライブラリを利用して量子化する。\n",
        "  * https://huggingface.co/blog/4bit-transformers-bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSLs-Eha9lUf"
      },
      "source": [
        "### 量子化のコンフィギュレーション"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIYWuMaY9lUf"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
        "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # Computation type\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhmlWjbp9lUf"
      },
      "source": [
        "### 量子化\n",
        "* 15分ぐらい時間がかかる。\n",
        "  * 手元のPCなら、3分以内で終わる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q32neRJF9lUf"
      },
      "outputs": [],
      "source": [
        "model_name = \"elyza/ELYZA-japanese-Llama-2-7b-fast-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5-ymVqjvFDl"
      },
      "source": [
        "## プロンプト\n",
        "* LLMがうまく感情分析をしてくれそうなプロンプトを考える。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c33BfNxjypMV"
      },
      "source": [
        "### プロンプト作成用のヘルパ関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csWUA3yv9lUg"
      },
      "source": [
        "* Llama 2系言語モデルのプロンプトのテンプレートは、以下の通り。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBi1vt9k9lUg"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "\n",
        "{{ System Prompt }}\n",
        "<</SYS>>\n",
        "{{ User Prompt }}\n",
        " [/INST]\n",
        "{{ Model Answer }}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6QpOEUZ9lUg"
      },
      "source": [
        "### プロンプトを作成するヘルパ関数\n",
        "* このプロンプトが良い、というつもりではありません・・・。\n",
        "  * 改良してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7gEATn-y3RX"
      },
      "outputs": [],
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]答え：\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"あなたは優秀な日本人のアシスタントです。\"\n",
        "\n",
        "def make_prompt(sample):\n",
        "  text = f\"次のような難しい文があります：\\n{sample['original_ja']}\\n\"\n",
        "  text += \"この文を、小学生でも分かるようなやさしい文に、書き換えてください。\\n\"\n",
        "  prompt = \"{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "      b_inst=B_INST,\n",
        "      system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "      prompt=text,\n",
        "      e_inst=E_INST,\n",
        "      )\n",
        "  sample[\"prompt\"] = prompt\n",
        "  return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PahFnk2z-lU"
      },
      "source": [
        "## やさしい日本語への書き換え"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgiYOYnD9lUh"
      },
      "source": [
        "### 生成のためのパイプラインの作成\n",
        "* https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/pipelines#transformers.pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iBtuMS29lUh"
      },
      "outputs": [],
      "source": [
        "generator = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=50,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixZDWRHe9lUh"
      },
      "source": [
        "### 生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKPWp6jdW9fF"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  print(f'[{i+1}]' + '-'*80)\n",
        "  sample = dataset[\"train\"][i]\n",
        "  original = sample[\"original_ja\"]\n",
        "  ground_truth = sample[\"simplified_ja\"]\n",
        "  prompt = make_prompt(sample)[\"prompt\"]\n",
        "  with torch.no_grad():\n",
        "    output = generator(prompt, return_full_text=False)\n",
        "  prediction = output[0]['generated_text']\n",
        "  print(f\"original:\\t{original}\")\n",
        "  print(f\"prediction:\\t{prediction}\")\n",
        "  print(f\"ground truth:\\t{ground_truth}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## in-context learning"
      ],
      "metadata": {
        "id": "hHwkQ9txBriy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 生成のためのパイプラインの作成\n",
        "* 答えが長くなるので、`max_new_tokens`は多いめの方がいいかもしれません。"
      ],
      "metadata": {
        "id": "kCw-4OSLBuPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=100,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "id": "9L5wafRJBlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### プロンプトを作成するヘルパ関数"
      ],
      "metadata": {
        "id": "1KvGDn9tB_oT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]答え：\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"あなたは優秀な日本人のアシスタントです。\"\n",
        "\n",
        "def make_prompt_with_demonstrations(sample, n_demonstrations=5):\n",
        "  n_demos = n_demonstrations\n",
        "  text = \"難しい文を、小学生でも分かるようなやさしい文に、書き換えたいです。\\n\"\n",
        "  text += \"まず、書き換えの例を示します。\\n\"\n",
        "  while n_demos > 0:\n",
        "    index = torch.randint(0, len(dataset[\"train\"]), (1,)).item()\n",
        "    example = dataset[\"train\"][index]\n",
        "    if example[\"original_ja\"] == sample[\"original_ja\"]:\n",
        "      continue\n",
        "    text += f\"次のような難しい文があります：\\n「{example['original_ja']}」\\n\"\n",
        "    text += \"この文は、次のようなやさしい文に、書き換えられます。\\n\"\n",
        "    text += f\"「{example['simplified_ja']}」\\n\"\n",
        "    n_demos -= 1\n",
        "  text += \"では、あなたにお願いします。\\n\"\n",
        "  text += f\"次のような難しい文があります：\\n{sample['original_ja']}\\n\"\n",
        "  text += \"この文を、小学生でも分かるようなやさしい文に、書き換えてください。\\n\"\n",
        "  prompt = \"{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "      b_inst=B_INST,\n",
        "      system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "      prompt=text,\n",
        "      e_inst=E_INST,\n",
        "      )\n",
        "  sample[\"prompt\"] = prompt\n",
        "  return sample"
      ],
      "metadata": {
        "id": "ic-VwTxs-V4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 生成"
      ],
      "metadata": {
        "id": "fAAWQKMQBnjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6I1Ja2kaudo"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  print(f'[{i+1}]' + '-'*80)\n",
        "  sample = dataset[\"train\"][i]\n",
        "  original = sample[\"original_ja\"]\n",
        "  ground_truth = sample[\"simplified_ja\"]\n",
        "  sample = make_prompt_with_demonstrations(sample, n_demonstrations=10)\n",
        "  prompt = sample[\"prompt\"]\n",
        "  with torch.no_grad():\n",
        "    output = generator(prompt, return_full_text=False)\n",
        "  prediction = output[0]['generated_text']\n",
        "  print(f\"original:\\t\\t{original}\")\n",
        "  print(f\"prediction:\\t{prediction}\")\n",
        "  print(f\"ground truth:\\t{ground_truth}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cXKf5dF0vRp"
      },
      "source": [
        "# 本日の課題\n",
        "* もっとうまくLLMに書き換えをさせるプロンプトを考えてみよう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUXcB4axyCQO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}