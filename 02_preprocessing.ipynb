{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIVXAmw_tMRv"
      },
      "source": [
        "# テキストデータの前処理\n",
        "\n",
        "* テキストデータは、長い長い文字列。\n",
        "* 長い長い文字列のままでは、普通は分析できない。\n",
        "* 今回は、基本的な前処理について学ぶ。\n",
        " * 英語と日本語の両方。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoWにおける語彙の重要性\n",
        "* BoWの範囲内でテキストデータを分析する場合は・・・\n",
        "* 語彙がそのまま特徴量(feature)になるので・・・\n",
        "* きれいな語彙集合を作ることが重要。つまり、前処理は重要。"
      ],
      "metadata": {
        "id": "J-TA1wqNvzpv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgeSwPsGJFWj"
      },
      "source": [
        "## str型のメソッドによる前処理\n",
        "\n",
        "* Pythonに元々備わっている機能を使うだけで、簡単な前処理ができる。\n",
        "\n",
        " * 問：元のテキストにあった大文字と小文字の区別を無くしてしまうことのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 大文字から小文字への変換"
      ],
      "metadata": {
        "id": "vMLSuvIz379-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQp382lJJFWp"
      },
      "source": [
        "text = \"The quick brown fox jumped over The Big Dog\"\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaAwb7HZJFWz"
      },
      "source": [
        "text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 空白文字によるテキストの分割"
      ],
      "metadata": {
        "id": "31Dyo1mZ4BKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower().split()"
      ],
      "metadata": {
        "id": "uYTrju1DlB5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtCf5TIaJpEr"
      },
      "source": [
        "## NLTK (Natural Language Toolkit)\n",
        "\n",
        "* https://www.nltk.org/\n",
        "\n",
        "* Pythonで自然言語処理を行うためのライブラリ\n",
        "  * 2001年スタートらしい。\n",
        "\n",
        "* WordNetも使える（ここでは説明しない）\n",
        " * https://www.nltk.org/howto/wordnet.html\n",
        " * WordNetについては『IT Text 自然言語処理の基礎』3.2.2(a)を参照。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSG71XiJoka"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3GzHq46JFW_"
      },
      "source": [
        "### NLTKによるトークン化\n",
        "\n",
        "* 文に分ける、単語に分ける、など、長い文字列としてのテキストを小さな単位へ分割することを一般にtokenizationと言う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIiPr5JBJFXA"
      },
      "source": [
        "text = (\n",
        "    \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism. \"\n",
        "    'Sometimes called \"the father of modern linguistics\", Chomsky is also a major figure in analytic philosophy and one of the founders of the field of cognitive science. '\n",
        "    \"He is a laureate professor of linguistics at the University of Arizona and an institute professor emeritus at the Massachusetts Institute of Technology (MIT).\"\n",
        ")\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqUvs2QjoAy-"
      },
      "source": [
        "* 文へtokenize\n",
        " * `punkt`というパッケージが必要なのでダウンロードしておく。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "V57v79GECtBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2m8nEPmJFXD"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDN_9m_oD7E"
      },
      "source": [
        "* 単語へtokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVNIwLoJFXG"
      },
      "source": [
        "print(nltk.word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D25190-Ft9Ls"
      },
      "source": [
        "## spaCy\n",
        "\n",
        "* https://spacy.io/\n",
        "* spaCyも、Pythonの有名な自然言語処理ライブラリ。2015年スタートらしい。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "8ZBCTv6Llqdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCyの「モデル」\n",
        "* 言語ごとにモデルが用意されている。\n",
        " * https://spacy.io/models\n",
        "* モデルにはいくつか種類があり、規模が異なる。\n",
        " * 大きなモデルでは、かなり複雑な処理ができる。"
      ],
      "metadata": {
        "id": "2BBEz_R8l1Vv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS5twUUnuIPf"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルをロードする。\n",
        " * `sm`はsmallの意味。"
      ],
      "metadata": {
        "id": "4mRgG4HKvxZa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhORAuPJFXL"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6LA_YHJFXN"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 文へtokenize"
      ],
      "metadata": {
        "id": "_KVXKZkYDifl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_nN_l3zQwa"
      },
      "source": [
        "[sent.text for sent in doc.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 単語へtokenize"
      ],
      "metadata": {
        "id": "2jnlKuFmDzz8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBuAHdR8JFXQ"
      },
      "source": [
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS tagging\n",
        "* POS = part of speech （品詞）"
      ],
      "metadata": {
        "id": "TxHUp1a142a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.pos_ for token in doc])"
      ],
      "metadata": {
        "id": "vnQSrQh6v_Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTwDPU2fx0i0"
      },
      "source": [
        "## 様々な前処理の実践"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8CyGmPJFXr"
      },
      "source": [
        "### 正規表現による前処理\n",
        "* 正規表現 = regular expression\n",
        "* ここでは、特殊文字、数字、記号の除去を正規表現を使っておこなう。\n",
        "\n",
        "* 問：テキストデータの前処理において特殊文字、数字、記号などを除去することのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dkc4ESDJFXs"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUwKvQ-1JFXx"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9x4XFyJFYL"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2vT0GK5JFYQ"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgQJp2SH7OC_"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "* 動詞や形容詞は原型に、名詞は単数形に、等と、単語の元々の形に直すこと。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2ExlFqJFaw"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize(text):\n",
        "  return [word.lemma_ for word in nlp(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize('lying')"
      ],
      "metadata": {
        "id": "7ZFn3GMNGmxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize('goes')"
      ],
      "metadata": {
        "id": "Y4wuCmA7GwiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-E47JKJFaz"
      },
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\"\n",
        "print(lemmatize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsKAXlvJFa7"
      },
      "source": [
        "### ストップワード\n",
        "\n",
        "* ストップワードとは、あまりに頻出するため、分析に役に立ちそうにない単語のこと。\n",
        " * 「これこそが英語（日本語、等）のストップワードだ！」と言えるような決定的なストップワードのリストがあるわけではない。\n",
        " * 主要なNLPライブラリでは、あらかじめ用意されたストップワードのリストを使うことができる。\n",
        " * しかし、分析したいテキストデータに合わせて、ストップワードのリストをカスタマイズすることも、よくある。\n",
        "* **深層学習の世界では、ストップワードの除去は、基本的に行わない。**\n",
        " * 深層学習言語モデルでは、元のままのテキストをサブワードへ分割するため。\n",
        " * サブワードへの分割を使うと、むしろ、ストップワードの役割を尊重しつつ、テキストを分析できる。\n",
        "* BoWの範囲内でテキストを分析するときは、普通、ストップワードは除去する。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCyのストップワード\n",
        "* 活用変化する単語はlemmaの形で登録されている。"
      ],
      "metadata": {
        "id": "nXqi2LK3zWgu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaYuRW2AxvE"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print(STOP_WORDS)\n",
        "print(len(STOP_WORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  return [token.text for token in nlp(text) if token.lemma_ not in stopwords]"
      ],
      "metadata": {
        "id": "IT7dZO4Grta1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJLKKxrJFa7"
      },
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\"\n",
        "print(remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 現代的なtokenization"
      ],
      "metadata": {
        "id": "o1CneGHJ0S7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### サブワード(subword)\n",
        "* 深層学習の世界では、サブワードをテキストの構成要素とみなす。\n",
        " * 単語よりもさらに粒度が細かい。\n",
        "* サブワードを構成要素とすることで、未知語の問題を回避できる。\n",
        " * 全ての文字をサブワードとして語彙に登録しておけば、未知語は無くなる。"
      ],
      "metadata": {
        "id": "F4LM3k0c0pSE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfSpyrrCJ63"
      },
      "source": [
        "### 文字・サブワード・単語\n",
        " * https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html\n",
        "\n",
        "![Segmentation.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/Segmentation.png)\n",
        "![inherent_task_complexity.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/image3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDgoJvYe-hK1"
      },
      "source": [
        "### サブワードへのトークン化\n",
        "* ここではHugging FaceのTransformersライブラリを使ってみる。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "HzdnWpGYuybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "0vTmXBVYvAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\""
      ],
      "metadata": {
        "id": "f2Flm-H9tA7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "1s3SWJGIvCp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキスト集合からサブワードの語彙を得るアルゴリズム\n",
        "* Byte-Pair Encoding (BPE) などについて説明がある。\n",
        " * https://huggingface.co/docs/transformers/tokenizer_summary"
      ],
      "metadata": {
        "id": "qIjtx1PxJaMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日本語テキストの分析"
      ],
      "metadata": {
        "id": "5BniCoIluJtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCyの日本語モデル"
      ],
      "metadata": {
        "id": "kFcOo_b8uYtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "ZNx8VF3buW7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDMb9o6cgMg"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"ノーム・チョムスキーは1928年12月7日、アメリカ合衆国ペンシルベニア州フィラデルフィアのイースト・オーク・レーン近郊で生まれた。\"\n",
        "    \"父ウィリアム・チョムスキーは当時ロシア帝国支配下のウクライナで生まれたが、戦乱を避けて1913年にアメリカへ渡った。\"\n",
        "    \"メリーランド州ボルチモアの搾取工場で働き、貯蓄してジョンズ・ホプキンス大学で学んだ甲斐もあり市のヘブライ人系小学校教師の職を得た。\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "VQpmBzCUujQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print((token.text, token.lemma_, token.pos_))"
      ],
      "metadata": {
        "id": "Vatq3vZsuQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* livedoor ニュースコーパスでの単語の出現回数を調べよう。\n",
        " * https://www.rondhuit.com/download.html#news%20corpus\n",
        "* 活用変化する単語は、原型に戻してから、出現回数を数えよう。"
      ],
      "metadata": {
        "id": "WMm5fdQS2AK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ],
      "metadata": {
        "id": "jbidwD5n5r0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このデータセットの前処理については、下の記事を参照。\n",
        " * https://tech.fusic.co.jp/posts/2021-04-23-bert-multi-classification/"
      ],
      "metadata": {
        "id": "LOqTOBA51wkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_title(f):\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  title = next(f) # 3行目を返す：タイトル\n",
        "  title = title.decode('utf-8')\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  return re.sub(brackets_head, \"\", re.sub(brackets_tail, \"\", title))[:-1]"
      ],
      "metadata": {
        "id": "qITSE68j2vbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "corpus = []\n",
        "\n",
        "tar_fname = \"ldcc-20140209.tar.gz\"\n",
        "\n",
        "with tarfile.open(tar_fname) as tf:\n",
        "  for item in tf:\n",
        "    if \"LICENSE.txt\" in item.name:\n",
        "      continue\n",
        "    if len(item.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not item.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    fname = item.name\n",
        "    # 今回はクラス名は要らない\n",
        "    #class_name = fname.split('/')[1]\n",
        "    f = tf.extractfile(fname)\n",
        "    title = read_title(f)\n",
        "    corpus.append(title)"
      ],
      "metadata": {
        "id": "CdAhU7FY3dLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "akiiB-5r5YNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "print([token.lemma_ for token in nlp(corpus[0])])"
      ],
      "metadata": {
        "id": "JUNaeasTzC5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsQkWoXTzQIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}