{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIVXAmw_tMRv"
      },
      "source": [
        "# テキストデータの前処理\n",
        "\n",
        "* テキストデータは、長い長い文字列。\n",
        "* 長い長い文字列のままでは、普通は分析できない。\n",
        "* 今回は、基本的な前処理について学ぶ。\n",
        " * 英語と日本語の両方。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgeSwPsGJFWj"
      },
      "source": [
        "## str型のメソッドによる前処理\n",
        "\n",
        "* 例えば、大文字と小文字の間の変換などが実行できる。\n",
        "\n",
        " * 問：元のテキストにあった大文字と小文字の区別を無くしてしまうことのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQp382lJJFWp"
      },
      "source": [
        "text = \"The quick brown fox jumped over The Big Dog\"\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaAwb7HZJFWz"
      },
      "source": [
        "text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihX9LwVuJFW4"
      },
      "source": [
        "text.upper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower().split()"
      ],
      "metadata": {
        "id": "uYTrju1DlB5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtCf5TIaJpEr"
      },
      "source": [
        "## NLTK\n",
        "\n",
        "* https://www.nltk.org/\n",
        "\n",
        "* Pythonで自然言語処理を行うためのライブラリ\n",
        "  * 2001年スタートらしい。\n",
        "\n",
        "* WordNetも使える（ここでは説明しない）\n",
        " * https://www.nltk.org/howto/wordnet.html\n",
        " * WordNetについては『IT Text 自然言語処理の基礎』3.2.2(a)を参照。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSG71XiJoka"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3GzHq46JFW_"
      },
      "source": [
        "### NLTKによるトークン化\n",
        "\n",
        "* 文に分ける、単語に分ける、など、長い文字列としての言語データをより小さな単位へと分割することを、一般にtokenizationと言う。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFm7LXKmnoS9"
      },
      "source": [
        "* Pythonの文字列は、複数行にわたっていても、丸括弧でくくれば一つの長い文字列になる。\n",
        " * ただし、最後の行を除いて、末尾に空白を入れておくのを忘れないように。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIiPr5JBJFXA"
      },
      "source": [
        "text = (\n",
        "    \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism. \"\n",
        "    'Sometimes called \"the father of modern linguistics\", Chomsky is also a major figure in analytic philosophy and one of the founders of the field of cognitive science. '\n",
        "    \"He is a laureate professor of linguistics at the University of Arizona and an institute professor emeritus at the Massachusetts Institute of Technology (MIT).\"\n",
        ")\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqUvs2QjoAy-"
      },
      "source": [
        "* 文へtokenize\n",
        " * `punkt`というパッケージが必要なのでダウンロードしておく。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "V57v79GECtBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2m8nEPmJFXD"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDN_9m_oD7E"
      },
      "source": [
        "* 単語へtokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVNIwLoJFXG"
      },
      "source": [
        "print(nltk.word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D25190-Ft9Ls"
      },
      "source": [
        "## spaCy\n",
        "\n",
        "* spaCyも、Pythonの有名な自然言語処理ライブラリ。2015年スタートらしい。\n",
        "\n",
        "* https://spacy.io/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "8ZBCTv6Llqdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCyの「モデル」\n",
        "* 各言語ごとにモデルが用意されている。\n",
        " * https://spacy.io/models\n",
        "* モデルにはいくつか種類があり、規模が異なる。\n",
        " * 大きなモデルでは、かなり複雑な処理ができる。"
      ],
      "metadata": {
        "id": "2BBEz_R8l1Vv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS5twUUnuIPf"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルをロードする。"
      ],
      "metadata": {
        "id": "4mRgG4HKvxZa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhORAuPJFXL"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6LA_YHJFXN"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 文へtokenize"
      ],
      "metadata": {
        "id": "_KVXKZkYDifl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_nN_l3zQwa"
      },
      "source": [
        "[sent.text for sent in doc.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 単語へtokenize"
      ],
      "metadata": {
        "id": "2jnlKuFmDzz8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBuAHdR8JFXQ"
      },
      "source": [
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 品詞"
      ],
      "metadata": {
        "id": "vtgQO8nBwAAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.pos_ for token in doc])"
      ],
      "metadata": {
        "id": "vnQSrQh6v_Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhxnJkIsJFXS"
      },
      "source": [
        "## 例題: HTML文書\n",
        "\n",
        "* __`<p>`__や__`<a>`__や__`<div>`__など、頻繁に使うHTMLタグは頭に入れておいてください。\n",
        "\n",
        "* なぜなら、ある程度HTMLタグが読めてはじめて、スクレイピングのコードを書くための、HTMLソースの下調べができるからです。\n",
        " * 自前でWeb上から分析対象のテキストデータを取得するときは、ダウンロードしようとするWebページのHTMLの構造を自分の目で確認する。\n",
        "\n",
        "* 注意: スクレイピングを禁止しているWebサイトもあるので注意しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewyVIu3auUl"
      },
      "source": [
        "### HTML文書のダウンロード\n",
        "* いくつか方法はあるが、ここではrequestsモジュールを使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3qV1WOpJFXT"
      },
      "source": [
        "import requests\n",
        "\n",
        "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
        "content = data.text\n",
        "print(content[4000:6000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vte11lpuXV_"
      },
      "source": [
        "### Beautiful Soup\n",
        "\n",
        "* HTML文書の構造を解析するためによく使われるライブラリ。\n",
        "\n",
        "* 参考資料：「Beautiful Soup 4によるスクレイピングの基礎」\n",
        "\n",
        " * https://www.atmarkit.co.jp/ait/articles/1910/18/news015.html\n",
        "\n",
        "* 注意：スクレイピングを禁止しているWebサイトもあるので注意しよう。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* HTMLタグを取り除くコードの例\n",
        " * 改行記号をまとめるために`re`（正規表現）モジュールを使っている。"
      ],
      "metadata": {
        "id": "N5IM18XyrU1Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6UAz3mjJFXY"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  # 下の正規表現の意味を説明してみよう。\n",
        "  return re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', soup.get_text())\n",
        "\n",
        "clean_content = strip_html_tags(content)\n",
        "print(clean_content[3000:5000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au1nuTllwX07"
      },
      "source": [
        "# 演習\n",
        "* 上の`clean_content`を単語に分割し、各単語の出現頻度を求め、出現頻度の高い順に上位100の単語を、出現頻度とともに表示しよう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBlgeuCY1YPX"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTwDPU2fx0i0"
      },
      "source": [
        "## 様々な前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8CyGmPJFXr"
      },
      "source": [
        "### 特殊文字、数字、記号の除去\n",
        "\n",
        "* 問：テキストデータの前処理において特殊文字、数字、記号などを除去することのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dkc4ESDJFXs"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUwKvQ-1JFXx"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9x4XFyJFYL"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2vT0GK5JFYQ"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgQJp2SH7OC_"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "* 動詞や形容詞は原型に、名詞は単数形に、等と、単語の元々の形に直すこと。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2ExlFqJFaw"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize(text):\n",
        "  return [word.lemma_ for word in nlp(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize('lying')"
      ],
      "metadata": {
        "id": "7ZFn3GMNGmxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize('goes')"
      ],
      "metadata": {
        "id": "Y4wuCmA7GwiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-E47JKJFaz"
      },
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\"\n",
        "print(lemmatize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsKAXlvJFa7"
      },
      "source": [
        "### ストップワード\n",
        "\n",
        "* ストップワードとは、非常に頻繁に使われるため、言語データの分析にあまり役に立ちそうにない単語のこと。\n",
        "* これこそが英語のストップワードだ！と言えるような決定的なストップワードのリストがあるわけではない。\n",
        " * 主要なNLPライブラリでは、あらかじめ用意されたストップワードのリストを使うことができる。\n",
        " * しかし、分析したいテキストデータに合わせて、ストップワードのリストをカスタマイズすることも、よくある。\n",
        "* **ストップワードの除去は最近あまり行われなくなっている。**\n",
        " * 深層学習言語モデルでは、元のままのテキストをサブワードへ分割するため。\n",
        " * 深層学習言語モデルを使うと、むしろ、ストップワードの役割を尊重しつつ、テキストを分析できる。\n",
        " * ストップワードの除去は、BoWでテキストを分析するときには、今でも実施する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaYuRW2AxvE"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print(STOP_WORDS)\n",
        "print(len(STOP_WORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  return [token.text for token in nlp(text) if token.text not in stopwords]"
      ],
      "metadata": {
        "id": "IT7dZO4Grta1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJLKKxrJFa7"
      },
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\"\n",
        "print(remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfSpyrrCJ63"
      },
      "source": [
        "## 現代的なtokenization\n",
        "* 図表は下記のブログ記事より。\n",
        " * https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html\n",
        "\n",
        "![Segmentation.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/Segmentation.png)\n",
        "![inherent_task_complexity.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/image3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDgoJvYe-hK1"
      },
      "source": [
        "* 今は、tokenizationと言えば、ほぼ、サブワードに分けることを意味する。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "HzdnWpGYuybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "0vTmXBVYvAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\""
      ],
      "metadata": {
        "id": "f2Flm-H9tA7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "1s3SWJGIvCp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Byte-Pair Encoding (BPE)\n",
        " * https://huggingface.co/docs/transformers/tokenizer_summary"
      ],
      "metadata": {
        "id": "qIjtx1PxJaMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日本語テキスト"
      ],
      "metadata": {
        "id": "5BniCoIluJtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Google Colabではインストールの必要あり。"
      ],
      "metadata": {
        "id": "kFcOo_b8uYtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "ZNx8VF3buW7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDMb9o6cgMg"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.ja.examples import sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"ノーム・チョムスキーは1928年12月7日、アメリカ合衆国ペンシルベニア州フィラデルフィアのイースト・オーク・レーン近郊で生まれた。\"\n",
        "    \"父ウィリアム・チョムスキーは当時ロシア帝国支配下のウクライナで生まれたが、戦乱を避けて1913年にアメリカへ渡った。\"\n",
        "    \"メリーランド州ボルチモアの搾取工場で働き、貯蓄してジョンズ・ホプキンス大学で学んだ甲斐もあり市のヘブライ人系小学校教師の職を得た。\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "VQpmBzCUujQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print((token.text, token.lemma_, token.pos_))"
      ],
      "metadata": {
        "id": "Vatq3vZsuQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課題 2\n",
        "* livedoor ニュースコーパスでの、単語の出現回数を調べよう。\n",
        " * https://www.rondhuit.com/download.html#news%20corpus\n",
        " * 活用変化する単語は、原型に戻してから、出現回数を数えよう。"
      ],
      "metadata": {
        "id": "WMm5fdQS2AK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ],
      "metadata": {
        "id": "jbidwD5n5r0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_fname = \"ldcc-20140209.tar.gz\""
      ],
      "metadata": {
        "id": "p8CDdBXZ1Bti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このデータセットの前処理については、下の記事を参照。\n",
        " * https://tech.fusic.co.jp/posts/2021-04-23-bert-multi-classification/"
      ],
      "metadata": {
        "id": "LOqTOBA51wkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 余分な括弧の除去\n",
        "def remove_brackets(text):\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  return re.sub(brackets_head, \"\", re.sub(brackets_tail, \"\", text))\n",
        "\n",
        "# このデータセットのフォーマットに従ったファイルの読み込み\n",
        "def read_title(f):\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  title = next(f) # 3行目を返す：タイトル\n",
        "  title = remove_brackets(title.decode('utf-8'))\n",
        "  return title[:-1]"
      ],
      "metadata": {
        "id": "qITSE68j2vbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "corpus = []\n",
        "\n",
        "with tarfile.open(tar_fname) as tf:\n",
        "  for item in tf:\n",
        "    if \"LICENSE.txt\" in item.name:\n",
        "      continue\n",
        "    if len(item.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not item.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    fname = item.name\n",
        "    # 今回はクラス名は要らない\n",
        "    #class_name = fname.split('/')[1]\n",
        "    f = tf.extractfile(fname)\n",
        "    title = read_title(f)\n",
        "    corpus.append(title)"
      ],
      "metadata": {
        "id": "CdAhU7FY3dLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "akiiB-5r5YNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "print([token.lemma_ for token in nlp(corpus[0])])"
      ],
      "metadata": {
        "id": "JUNaeasTzC5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsQkWoXTzQIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}