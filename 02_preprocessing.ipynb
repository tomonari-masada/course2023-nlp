{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/02_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIVXAmw_tMRv"
      },
      "source": [
        "# テキストデータの前処理\n",
        "\n",
        "* テキストデータは、長い長い文字列。\n",
        "* 長い長い文字列のままでは、普通は分析できない。\n",
        "* 今回は、基本的な前処理について学ぶ。\n",
        " * 英語と日本語の両方。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgeSwPsGJFWj"
      },
      "source": [
        "## str型のメソッドによる前処理\n",
        "\n",
        "* Pythonそのものを使うだけで、簡単な前処理ができる。\n",
        "\n",
        " * 問：元のテキストにあった大文字と小文字の区別を無くしてしまうことのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 大文字から小文字への変換"
      ],
      "metadata": {
        "id": "vMLSuvIz379-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQp382lJJFWp"
      },
      "source": [
        "text = \"The quick brown fox jumped over The Big Dog\"\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaAwb7HZJFWz"
      },
      "source": [
        "text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 空白文字によるテキストの分割"
      ],
      "metadata": {
        "id": "31Dyo1mZ4BKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text.lower().split()"
      ],
      "metadata": {
        "id": "uYTrju1DlB5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtCf5TIaJpEr"
      },
      "source": [
        "## NLTK (Natural Language Toolkit)\n",
        "\n",
        "* https://www.nltk.org/\n",
        "\n",
        "* Pythonで自然言語処理を行うためのライブラリ\n",
        "  * 2001年スタートらしい。\n",
        "\n",
        "* WordNetも使える（ここでは説明しない）\n",
        " * https://www.nltk.org/howto/wordnet.html\n",
        " * WordNetについては『IT Text 自然言語処理の基礎』3.2.2(a)を参照。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSG71XiJoka"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3GzHq46JFW_"
      },
      "source": [
        "### NLTKによるトークン化\n",
        "\n",
        "* 文に分ける、単語に分ける、など、長い文字列としてのテキストを小さな単位へ分割することを一般にtokenizationと言う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIiPr5JBJFXA"
      },
      "source": [
        "text = (\n",
        "    \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism. \"\n",
        "    'Sometimes called \"the father of modern linguistics\", Chomsky is also a major figure in analytic philosophy and one of the founders of the field of cognitive science. '\n",
        "    \"He is a laureate professor of linguistics at the University of Arizona and an institute professor emeritus at the Massachusetts Institute of Technology (MIT).\"\n",
        ")\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqUvs2QjoAy-"
      },
      "source": [
        "* 文へtokenize\n",
        " * `punkt`というパッケージが必要なのでダウンロードしておく。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "V57v79GECtBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2m8nEPmJFXD"
      },
      "source": [
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDN_9m_oD7E"
      },
      "source": [
        "* 単語へtokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjVNIwLoJFXG"
      },
      "source": [
        "print(nltk.word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D25190-Ft9Ls"
      },
      "source": [
        "## spaCy\n",
        "\n",
        "* https://spacy.io/\n",
        "* spaCyも、Pythonの有名な自然言語処理ライブラリ。2015年スタートらしい。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "8ZBCTv6Llqdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCyの「モデル」\n",
        "* 言語ごとにモデルが用意されている。\n",
        " * https://spacy.io/models\n",
        "* モデルにはいくつか種類があり、規模が異なる。\n",
        " * 大きなモデルでは、かなり複雑な処理ができる。"
      ],
      "metadata": {
        "id": "2BBEz_R8l1Vv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS5twUUnuIPf"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルをロードする。"
      ],
      "metadata": {
        "id": "4mRgG4HKvxZa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjhORAuPJFXL"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6LA_YHJFXN"
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 文へtokenize"
      ],
      "metadata": {
        "id": "_KVXKZkYDifl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_nN_l3zQwa"
      },
      "source": [
        "[sent.text for sent in doc.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 単語へtokenize"
      ],
      "metadata": {
        "id": "2jnlKuFmDzz8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBuAHdR8JFXQ"
      },
      "source": [
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS tagging\n",
        "* POS = part of speech （品詞）"
      ],
      "metadata": {
        "id": "TxHUp1a142a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.pos_ for token in doc])"
      ],
      "metadata": {
        "id": "vnQSrQh6v_Cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTwDPU2fx0i0"
      },
      "source": [
        "## 様々な前処理の実践"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj8CyGmPJFXr"
      },
      "source": [
        "### 正規表現による前処理\n",
        "* 正規表現 = regular expression\n",
        "* ここでは、特殊文字、数字、記号の除去を正規表現を使っておこなう。\n",
        "\n",
        "* 問：テキストデータの前処理において特殊文字、数字、記号などを除去することのメリットとデメリットは何か？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dkc4ESDJFXs"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "  pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "  text = re.sub(pattern, '', text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUwKvQ-1JFXx"
      },
      "source": [
        "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ 🙂🙂🙂\"\n",
        "s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy9x4XFyJFYL"
      },
      "source": [
        "remove_special_characters(s, remove_digits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2vT0GK5JFYQ"
      },
      "source": [
        "remove_special_characters(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgQJp2SH7OC_"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "* 動詞や形容詞は原型に、名詞は単数形に、等と、単語の元々の形に直すこと。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2ExlFqJFaw"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize(text):\n",
        "  return [word.lemma_ for word in nlp(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize('lying')"
      ],
      "metadata": {
        "id": "7ZFn3GMNGmxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize('goes')"
      ],
      "metadata": {
        "id": "Y4wuCmA7GwiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga-E47JKJFaz"
      },
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\"\n",
        "print(lemmatize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsKAXlvJFa7"
      },
      "source": [
        "### ストップワード\n",
        "\n",
        "* ストップワードとは、非常に頻繁に使われるため、言語データの分析にあまり役に立ちそうにない単語のこと。\n",
        "* これこそが英語のストップワードだ！と言えるような決定的なストップワードのリストがあるわけではない。\n",
        " * 主要なNLPライブラリでは、あらかじめ用意されたストップワードのリストを使うことができる。\n",
        " * しかし、分析したいテキストデータに合わせて、ストップワードのリストをカスタマイズすることも、よくある。\n",
        "* **ストップワードの除去は最近あまり行われなくなっている。**\n",
        " * 深層学習言語モデルでは、元のままのテキストをサブワードへ分割するため。\n",
        " * サブワードへの分割を使うと、むしろ、ストップワードの役割を尊重しつつ、テキストを分析できる。\n",
        " * ストップワードの除去は、BoWでテキストを分析するときには、今でも実施する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TaYuRW2AxvE"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "print(STOP_WORDS)\n",
        "print(len(STOP_WORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "  return [token.text for token in nlp(text) if token.text not in stopwords]"
      ],
      "metadata": {
        "id": "IT7dZO4Grta1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJLKKxrJFa7"
      },
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\"\n",
        "print(remove_stopwords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVfSpyrrCJ63"
      },
      "source": [
        "## 現代的なtokenization\n",
        "* 図表は下記のブログ記事より。\n",
        " * https://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html\n",
        "\n",
        "![Segmentation.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/Segmentation.png)\n",
        "![inherent_task_complexity.png](https://raw.githubusercontent.com/tomonari-masada/course2022-nlp/main/image3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDgoJvYe-hK1"
      },
      "source": [
        "* 今は、tokenizationと言えば、ほぼ、サブワードに分けることを意味する。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "HzdnWpGYuybH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "0vTmXBVYvAjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Avram Noam Chomsky (born December 7, 1928) is an American professor and public intellectual known for his work in linguistics, political activism, and social criticism.\""
      ],
      "metadata": {
        "id": "f2Flm-H9tA7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "id": "1s3SWJGIvCp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 参考: Byte-Pair Encoding (BPE)\n",
        " * https://huggingface.co/docs/transformers/tokenizer_summary"
      ],
      "metadata": {
        "id": "qIjtx1PxJaMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日本語テキストの分析"
      ],
      "metadata": {
        "id": "5BniCoIluJtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* spaCyの日本語モデルのインストール"
      ],
      "metadata": {
        "id": "kFcOo_b8uYtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_sm"
      ],
      "metadata": {
        "id": "ZNx8VF3buW7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDMb9o6cgMg"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"ノーム・チョムスキーは1928年12月7日、アメリカ合衆国ペンシルベニア州フィラデルフィアのイースト・オーク・レーン近郊で生まれた。\"\n",
        "    \"父ウィリアム・チョムスキーは当時ロシア帝国支配下のウクライナで生まれたが、戦乱を避けて1913年にアメリカへ渡った。\"\n",
        "    \"メリーランド州ボルチモアの搾取工場で働き、貯蓄してジョンズ・ホプキンス大学で学んだ甲斐もあり市のヘブライ人系小学校教師の職を得た。\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "VQpmBzCUujQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print((token.text, token.lemma_, token.pos_))"
      ],
      "metadata": {
        "id": "Vatq3vZsuQEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 本日の課題\n",
        "* livedoor ニュースコーパスでの単語の出現回数を調べよう。\n",
        " * https://www.rondhuit.com/download.html#news%20corpus\n",
        "* 活用変化する単語は、原型に戻してから、出現回数を数えよう。"
      ],
      "metadata": {
        "id": "WMm5fdQS2AK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
      ],
      "metadata": {
        "id": "jbidwD5n5r0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_fname = \"ldcc-20140209.tar.gz\""
      ],
      "metadata": {
        "id": "p8CDdBXZ1Bti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* このデータセットの前処理については、下の記事を参照。\n",
        " * https://tech.fusic.co.jp/posts/2021-04-23-bert-multi-classification/"
      ],
      "metadata": {
        "id": "LOqTOBA51wkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 余分な括弧の除去\n",
        "def remove_brackets(text):\n",
        "  brackets_tail = re.compile('【[^】]*】$')\n",
        "  brackets_head = re.compile('^【[^】]*】')\n",
        "  return re.sub(brackets_head, \"\", re.sub(brackets_tail, \"\", text))\n",
        "\n",
        "# このデータセットのフォーマットに従ったファイルの読み込み\n",
        "def read_title(f):\n",
        "  next(f) # URL\n",
        "  next(f) # タイムスタンプ\n",
        "  title = next(f) # 3行目を返す：タイトル\n",
        "  title = remove_brackets(title.decode('utf-8'))\n",
        "  return title[:-1]"
      ],
      "metadata": {
        "id": "qITSE68j2vbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "corpus = []\n",
        "\n",
        "with tarfile.open(tar_fname) as tf:\n",
        "  for item in tf:\n",
        "    if \"LICENSE.txt\" in item.name:\n",
        "      continue\n",
        "    if len(item.name.split('/')) < 3:\n",
        "      continue\n",
        "    if not item.name.endswith(\".txt\"):\n",
        "      continue\n",
        "    fname = item.name\n",
        "    # 今回はクラス名は要らない\n",
        "    #class_name = fname.split('/')[1]\n",
        "    f = tf.extractfile(fname)\n",
        "    title = read_title(f)\n",
        "    corpus.append(title)"
      ],
      "metadata": {
        "id": "CdAhU7FY3dLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "id": "akiiB-5r5YNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "print([token.lemma_ for token in nlp(corpus[0])])"
      ],
      "metadata": {
        "id": "JUNaeasTzC5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsQkWoXTzQIt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}