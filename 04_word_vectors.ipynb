{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Hz9rXJHZDZeEY4pbWkLYFhZftVekADdY",
      "authorship_tag": "ABX9TyPo1TYUsBWk6S4jwLFO/5vH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/04_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t-iVvqcVnB5"
      },
      "source": [
        "# 単語ベクトル\n",
        "* いわゆるword2vec。\n",
        " * https://arxiv.org/abs/1301.3781\n",
        " * https://en.wikipedia.org/wiki/Word2vec\n",
        "* 単語をベクトルとして表現したもの。\n",
        " * 単語埋め込み、単語分散表現、などとも言われる。\n",
        "* 意味が近い単語はベクトルとしても近くなるように、作成されている。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はランタイムのタイプでGPUを選んでおいてください。"
      ],
      "metadata": {
        "id": "aFa_gCwe-9MF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使いみち"
      ],
      "metadata": {
        "id": "7i5nod9rraEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 単語の類似度評価\n",
        "* ベクトルどうしの遠い近いを表す尺度はなんでも使える。\n",
        "* 内積やコサイン類似度が使われることが多い。\n"
      ],
      "metadata": {
        "id": "IE7l-ut8CjdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テキストのベクトル化\n",
        "* 最もシンプルには、テキストに含まれるトークンの単語ベクトルの平均を取ればよい。\n",
        " * これをmean poolingと呼ぶ。\n",
        "* word2vecの単語ベクトルを使ってテキストをベクトル化することは、最近はあまりない。\n",
        " * テキストのベクトル化に、今は深層学習言語モデルを使う。\n",
        " * 今回は最後に少しBERTを使った方法を紹介する。"
      ],
      "metadata": {
        "id": "q9N1r-DYCFJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単語ベクトルを作るアルゴリズム\n",
        "* アルゴリズム自体の説明は、この授業では割愛する。\n",
        " * https://www.tensorflow.org/text/tutorials/word2vec"
      ],
      "metadata": {
        "id": "pJf_QZP6r-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "9JjurXjctOMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "BAWlTIYQ_m2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WRIME: 主観と客観の感情分析データセット\n",
        "* https://github.com/ids-cv/wrime"
      ],
      "metadata": {
        "id": "qrBMTq28uDNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"shunk031/wrime\", \"ver2\")"
      ],
      "metadata": {
        "id": "ex4ADCbhtGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* はじめから、train, validation, testの3つの集合に分けられている。"
      ],
      "metadata": {
        "id": "Q3F06BjLAqTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tags = [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "texts = {}\n",
        "labels = {}\n",
        "for tag in tags:\n",
        "  texts[tag] = dataset[tag][\"sentence\"]\n",
        "  labels[tag] = [item[\"sentiment\"] for item in dataset[tag][\"avg_readers\"]]\n",
        "  labels[tag] = np.array(labels[tag])"
      ],
      "metadata": {
        "id": "cZNSO3N8uExN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3F0XNxABxH_"
      },
      "source": [
        "## spaCyの単語ベクトル\n",
        "* 今回は英語テキストのみ。\n",
        "* 小規模のモデル（名前が__`_sm`__で終わるモデル）は単語ベクトルを含まない。\n",
        "* 大規模モデルはダウンロードに時間がかかる。\n",
        "* そのため、中規模モデルをインストールする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4qKeUfdCDNu"
      },
      "source": [
        "### 日本語中規模モデルのインストール\n",
        "* https://spacy.io/models/ja#ja_core_news_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_md"
      ],
      "metadata": {
        "id": "-4jI7POINHwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 全テキストのベクトル化\n",
        "* spaCyではテキストを直接ベクトル化できる。\n",
        " * 内部では単語ベクトルの平均を求めている。"
      ],
      "metadata": {
        "id": "s1Qsw6KFyCel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_core_news_md')\n",
        "\n",
        "X = {}\n",
        "for tag in tags:\n",
        "  X[tag] = []\n",
        "  for text in tqdm(texts[tag]):\n",
        "    tokens = nlp(text)\n",
        "    X[tag].append(tokens.vector)\n",
        "  X[tag] = np.array(X[tag])"
      ],
      "metadata": {
        "id": "uTeYy91XwEsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"validation\"].shape"
      ],
      "metadata": {
        "id": "5R22Zu7bxxs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ベクトル化した結果を保存しておく。"
      ],
      "metadata": {
        "id": "m5-qau2lB_xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in tags:\n",
        "  with open(f'wrime_{tag}_vec.npy', 'wb') as f:\n",
        "    np.save(f, X[tag])\n",
        "  with open(f'wrime_{tag}_label.npy', 'wb') as f:\n",
        "    np.save(f, labels[tag])"
      ],
      "metadata": {
        "id": "fWNSFVz4yZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EIlDmklyYaP"
      },
      "source": [
        "## 単語ベクトルを使った文書分類"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 保存しておいたテキストのベクトル表現を読み込む。"
      ],
      "metadata": {
        "id": "MLHWsnKbAS0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tags = [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "X = {}\n",
        "labels = {}\n",
        "for tag in tags:\n",
        "  with open(f'wrime_{tag}_vec.npy', 'rb') as f:\n",
        "    X[tag] = np.load(f)\n",
        "  with open(f'wrime_{tag}_label.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "metadata": {
        "id": "ymyGRShtzMxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"train\"].shape"
      ],
      "metadata": {
        "id": "0rZG7F6R1oan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[\"train\"].shape"
      ],
      "metadata": {
        "id": "EFtCpabq1rN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ここではデータセットのラベルを2値に単純化する\n",
        " * ラベル0のテキストは取り除く。\n",
        " * negativeとpositiveは、それぞれ一つのクラスにまとめる。"
      ],
      "metadata": {
        "id": "M3fPpNHdAXXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_binary = {}\n",
        "labels_binary = {}\n",
        "for tag in tags:\n",
        "  indices = labels[tag] != 0\n",
        "  X_binary[tag] = X[tag][indices]\n",
        "  labels_binary[tag] = labels[tag][indices]\n",
        "  labels_binary[tag] = (labels_binary[tag] > 0) * 1"
      ],
      "metadata": {
        "id": "sjKnS5zl2Zqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "cls = LinearSVC()\n",
        "cls.fit(X_binary[\"train\"], labels_binary[\"train\"])\n",
        "cls.score(X_binary[\"validation\"], labels_binary[\"validation\"])"
      ],
      "metadata": {
        "id": "YMbe-h8d1uYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8djYmZkB3b"
      },
      "source": [
        "# BERTによるテキストのベクトル化\n",
        "* BERTの説明はしない。とりあえず使う。\n",
        "* BERTを単にテキストをベクトル化するツールとして使う。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 準備"
      ],
      "metadata": {
        "id": "AfX3eDm2BmAY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6V_dyqv0n1o"
      },
      "source": [
        "!pip install -q transformers fugashi[unidic-lite]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "dLFNjOLq5SEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sentence BERTのロード\n",
        "* 初回だけダウンロードに時間がかかる。\n",
        "* 2回目以降は、ローカルに保存したモデルをロードするだけ。"
      ],
      "metadata": {
        "id": "I3RGWUNzBoo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer(\"cl-tohoku/bert-base-japanese-v3\")"
      ],
      "metadata": {
        "id": "P1yDjdhl4rid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 全テキストのベクトル化"
      ],
      "metadata": {
        "id": "6cRGusJkB32F"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LalaUJ22k9dU"
      },
      "source": [
        "X = {}\n",
        "for tag in tags:\n",
        "  X[tag] = embedder.encode(texts[tag])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ベクトル化した結果を保存しておく。"
      ],
      "metadata": {
        "id": "kHCop4-NB6aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for tag in tags:\n",
        "  with open(f'wrime_{tag}_bert_vec.npy', 'wb') as f:\n",
        "    np.save(f, X[tag])"
      ],
      "metadata": {
        "id": "Zt1V4CIY9ASi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ラベルを2値に単純化する。\n",
        " * 先ほどと同じ。"
      ],
      "metadata": {
        "id": "KZn8tgZHDLvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_binary = {}\n",
        "labels_binary = {}\n",
        "for tag in tags:\n",
        "  indices = labels[tag] != 0\n",
        "  X_binary[tag] = X[tag][indices]\n",
        "  labels_binary[tag] = labels[tag][indices]\n",
        "  labels_binary[tag] = (labels_binary[tag] > 0) * 1"
      ],
      "metadata": {
        "id": "DkS3L1229RBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "cls = LinearSVC()\n",
        "cls.fit(X_binary[\"train\"], labels_binary[\"train\"])\n",
        "cls.score(X_binary[\"validation\"], labels_binary[\"validation\"])"
      ],
      "metadata": {
        "id": "-tPqsl069Upi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss1gBFpoATIR"
      },
      "source": [
        "# 本日の課題\n",
        "* 上で実行した感情分析の性能を上げてください。\n",
        "* チューニングが済んだら、テストセットでscoreを計算してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnK61odfnND7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}