{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Hz9rXJHZDZeEY4pbWkLYFhZftVekADdY",
      "authorship_tag": "ABX9TyNWxDXWZjeppa5vm+qEK02D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/04_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t-iVvqcVnB5"
      },
      "source": [
        "# 単語ベクトル\n",
        "* いわゆるword2vec。\n",
        " * https://arxiv.org/abs/1301.3781\n",
        " * https://en.wikipedia.org/wiki/Word2vec\n",
        "* 単語をベクトルとして表現したもの。\n",
        " * 単語埋め込み、単語分散表現、などとも言われる。\n",
        "* 意味が近い単語はベクトルとしても近くなるように、作成されている。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はランタイムのタイプでGPUを選んでおいてください。"
      ],
      "metadata": {
        "id": "aFa_gCwe-9MF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使いみち\n",
        "* 単語どうしの類似度評価に使う。\n",
        "* 文書どうしの類似度評価にも使う。\n",
        " * 文書をbag-of-wordsモデルによってベクトル表現することは、最近はあまりない。\n",
        " * 単語を使って文書のベクトル表現を作る。\n",
        " * とはいえ、word2vecも最近はあまり使われない。\n",
        " * 今は言語モデルを使う。何回か後の授業で説明する。"
      ],
      "metadata": {
        "id": "7i5nod9rraEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単語ベクトルを作るアルゴリズム\n",
        "* 説明は割愛する。"
      ],
      "metadata": {
        "id": "pJf_QZP6r-8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "9JjurXjctOMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WRIME: 主観と客観の感情分析データセット\n",
        "* https://github.com/ids-cv/wrime"
      ],
      "metadata": {
        "id": "qrBMTq28uDNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"shunk031/wrime\", \"ver2\")"
      ],
      "metadata": {
        "id": "ex4ADCbhtGOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "tags = [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "texts = {}\n",
        "labels = {}\n",
        "for tag in tags:\n",
        "  texts[tag] = dataset[tag][\"sentence\"]\n",
        "  labels[tag] = [item[\"sentiment\"] for item in dataset[tag][\"avg_readers\"]]\n",
        "  labels[tag] = np.array(labels[tag])"
      ],
      "metadata": {
        "id": "cZNSO3N8uExN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3F0XNxABxH_"
      },
      "source": [
        "## spaCyの単語ベクトル\n",
        "* 今回は英語テキストのみ。\n",
        "* 小規模のモデル（名前が__`_sm`__で終わるモデル）は単語ベクトルを含まない。\n",
        "* 大規模モデルはダウンロードに時間がかかる。\n",
        "* そのため、中規模モデルをインストールする。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4qKeUfdCDNu"
      },
      "source": [
        "### spaCyの中規模モデルをダウンロード\n",
        "* https://spacy.io/models/ja#ja_core_news_md"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ja_core_news_md"
      ],
      "metadata": {
        "id": "-4jI7POINHwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 全テキストのベクトル化\n",
        " * 数分待つ。"
      ],
      "metadata": {
        "id": "s1Qsw6KFyCel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ja_core_news_md')\n",
        "\n",
        "X = {}\n",
        "for tag in tags:\n",
        "  X[tag] = []\n",
        "  for text in tqdm(texts[tag]):\n",
        "    tokens = nlp(text)\n",
        "    X[tag].append(tokens.vector)\n",
        "  X[tag] = np.array(X[tag])"
      ],
      "metadata": {
        "id": "uTeYy91XwEsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"validation\"].shape"
      ],
      "metadata": {
        "id": "5R22Zu7bxxs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tag in tags:\n",
        "  with open(f'wrime_{tag}_vec.npy', 'wb') as f:\n",
        "    np.save(f, X[tag])\n",
        "  with open(f'wrime_{tag}_label.npy', 'wb') as f:\n",
        "    np.save(f, labels[tag])"
      ],
      "metadata": {
        "id": "fWNSFVz4yZrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EIlDmklyYaP"
      },
      "source": [
        "## 単語ベクトルを使った文書分類"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = {}\n",
        "labels = {}\n",
        "for tag in tags:\n",
        "  with open(f'wrime_{tag}_vec.npy', 'rb') as f:\n",
        "    X[tag] = np.load(f)\n",
        "  with open(f'wrime_{tag}_label.npy', 'rb') as f:\n",
        "    labels[tag] = np.load(f)"
      ],
      "metadata": {
        "id": "ymyGRShtzMxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"train\"].shape"
      ],
      "metadata": {
        "id": "0rZG7F6R1oan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[\"train\"].shape"
      ],
      "metadata": {
        "id": "EFtCpabq1rN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_binary = {}\n",
        "labels_binary = {}\n",
        "for tag in tags:\n",
        "  indices = labels[tag] != 0\n",
        "  X_binary[tag] = X[tag][indices]\n",
        "  labels_binary[tag] = labels[tag][indices]\n",
        "  labels_binary[tag] = (labels_binary[tag] > 0) * 1"
      ],
      "metadata": {
        "id": "sjKnS5zl2Zqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "cls = LinearSVC()\n",
        "cls.fit(X_binary[\"train\"], labels_binary[\"train\"])\n",
        "cls.score(X_binary[\"validation\"], labels_binary[\"validation\"])"
      ],
      "metadata": {
        "id": "YMbe-h8d1uYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im8djYmZkB3b"
      },
      "source": [
        "## BERTによる文書のベクトル化\n",
        "* BERTの説明はしない。とりあえず使う。\n",
        "* BERTを単なるエンコーダとして使う。\n",
        " * fine-tuningはしない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6V_dyqv0n1o"
      },
      "source": [
        "!pip install -q transformers fugashi[unidic-lite]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "dLFNjOLq5SEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedder = SentenceTransformer(\"cl-tohoku/bert-base-japanese-v3\")"
      ],
      "metadata": {
        "id": "P1yDjdhl4rid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LalaUJ22k9dU"
      },
      "source": [
        "X = {}\n",
        "for tag in tags:\n",
        "  X[tag] = embedder.encode(texts[tag])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for tag in tags:\n",
        "  with open(f'wrime_{tag}_bert_vec.npy', 'wb') as f:\n",
        "    np.save(f, X[tag])"
      ],
      "metadata": {
        "id": "Zt1V4CIY9ASi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_binary = {}\n",
        "labels_binary = {}\n",
        "for tag in tags:\n",
        "  indices = labels[tag] != 0\n",
        "  X_binary[tag] = X[tag][indices]\n",
        "  labels_binary[tag] = labels[tag][indices]\n",
        "  labels_binary[tag] = (labels_binary[tag] > 0) * 1"
      ],
      "metadata": {
        "id": "DkS3L1229RBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "cls = LinearSVC()\n",
        "cls.fit(X_binary[\"train\"], labels_binary[\"train\"])\n",
        "cls.score(X_binary[\"validation\"], labels_binary[\"validation\"])"
      ],
      "metadata": {
        "id": "-tPqsl069Upi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss1gBFpoATIR"
      },
      "source": [
        "# 課題\n",
        "* 上で実行した感情分析の性能を上げてください。\n",
        "* チューニングが済んだら、テストセットでscoreを計算してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnK61odfnND7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}