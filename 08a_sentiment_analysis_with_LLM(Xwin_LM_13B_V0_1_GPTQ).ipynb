{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlQ2LQZcpOmK/qa2k/c+xJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/08a_sentiment_analysis_with_LLM(Xwin_LM_13B_V0_1_GPTQ).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix for LLMを使ってみる"
      ],
      "metadata": {
        "id": "s7UHVLvsl89T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **ランタイムのタイプをGPUに設定しておくこと。**"
      ],
      "metadata": {
        "id": "R0tqelrXmhun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate auto-gptq"
      ],
      "metadata": {
        "id": "2W5-9kWlm1qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ここでランタイムを再起動する。**"
      ],
      "metadata": {
        "id": "ntPSyIO8r83H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### インポート"
      ],
      "metadata": {
        "id": "eX6zotwEoK2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "9A6Euk7gmXNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* WRIME ver.2\n",
        "  * 主観と客観の感情分析データセット https://github.com/ids-cv/wrime\n"
      ],
      "metadata": {
        "id": "ELjoQ6q_oTAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"shunk031/wrime\", \"ver2\")\n",
        "tags = [\"train\", \"validation\", \"test\"]\n",
        "\n",
        "texts = {}\n",
        "labels = {}\n",
        "for tag in tags:\n",
        "  texts[tag] = np.array(dataset[tag][\"sentence\"])\n",
        "  labels[tag] = [item[\"sentiment\"] for item in dataset[tag][\"avg_readers\"]]\n",
        "  labels[tag] = np.array(labels[tag])"
      ],
      "metadata": {
        "id": "4A0Qp1JsoRaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_binary = {}\n",
        "labels_binary = {}\n",
        "for tag in tags:\n",
        "  indices = labels[tag] != 0\n",
        "  texts_binary[tag] = texts[tag][indices]\n",
        "  labels_binary[tag] = labels[tag][indices]\n",
        "  labels_binary[tag] = (labels_binary[tag] > 0) * 1"
      ],
      "metadata": {
        "id": "tWQOyZ21qvsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_text = [\"悲しい\", \"嬉しい\"]"
      ],
      "metadata": {
        "id": "qMQjq649q3dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM\n"
      ],
      "metadata": {
        "id": "ACC0ED27qZtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、Xwin-LM-13B-V0.1を使う。\n",
        " * https://huggingface.co/Xwin-LM/Xwin-LM-13B-V0.1\n",
        "* だが、Google Colab無料版では、この元のモデルは大きすぎて使えない・・・。\n",
        "* そこで、量子化された下記のモデルを代わりに使う。\n",
        " * https://huggingface.co/TheBloke/Xwin-LM-13B-V0.1-GPTQ"
      ],
      "metadata": {
        "id": "kIsSj4bzq7Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xwin-LM-13B-V0.1-GPTQの取得\n",
        "* モデルのダウンロードに少し時間がかかる。\n",
        "* `AutoGPTQForCausalLM`クラスについては、以下を参照。\n",
        " * https://github.com/PanQiWei/AutoGPTQ/blob/main/auto_gptq/modeling/auto.py"
      ],
      "metadata": {
        "id": "qVJVYtaKrGwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* safetensorsについては、以下を参照。\n",
        " * https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors"
      ],
      "metadata": {
        "id": "sFvxES91acnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `trust_remote_code`については、[ここ](https://huggingface.co/docs/transformers/model_doc/auto)に以下のような説明がある。\n",
        "\n",
        "> Whether or not to allow for custom models defined on the Hub in their own modeling files. This option should only be set to True for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
        "\n"
      ],
      "metadata": {
        "id": "4m_BJPRmbxSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "\n",
        "model_name = \"TheBloke/Xwin-LM-13B-V0.1-GPTQ\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_name,\n",
        "    use_safetensors=True,\n",
        "    inject_fused_attention=False,\n",
        "    device=\"cuda:0\",\n",
        "    trust_remote_code=True,\n",
        "    )\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "PYR1TtgLrGcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-context learning"
      ],
      "metadata": {
        "id": "yMkguDk2rccp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Q:高い\\nA:低い\\n\\nQ:大きい\\nA:小さい\\n\\nQ:狭い\\nA:広い\\n\\nQ:少ない\\nA:多い\\n\\nQ:速い\\nA:遅い\\n\\nQ:嬉しい\\nA:\"\n",
        "print(text)"
      ],
      "metadata": {
        "id": "62BYcxHLrZoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "output_ids = model.generate(\n",
        "    input_ids=token_ids.to(model.device),\n",
        "    max_new_tokens=10,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "5lvT5srZrRHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 感情分析のプロンプト"
      ],
      "metadata": {
        "id": "i5-ymVqjvFDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### プロンプト作成用のヘルパ関数"
      ],
      "metadata": {
        "id": "c33BfNxjypMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B_INST, E_INST = \"\", \"答え：\"\n",
        "B_SYS, E_SYS = \"\\n\", \"\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
        "\n",
        "def make_prompt(text):\n",
        "  prompt = \"「\" + text + \"」\\nと言っている人の気持ちは、「嬉しい」と「悲しい」のうち、どちらですか。\\n\"\n",
        "  return \"{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "      b_inst=B_INST,\n",
        "      system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "      prompt=prompt,\n",
        "      e_inst=E_INST,\n",
        "      ).strip()"
      ],
      "metadata": {
        "id": "Q7gEATn-y3RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 感情分析"
      ],
      "metadata": {
        "id": "6PahFnk2z-lU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKPWp6jdW9fF"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  print(f'[{i+1}]' + '-'*80)\n",
        "  prompt = make_prompt(texts_binary[\"train\"][i])\n",
        "  with torch.no_grad():\n",
        "    token_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids=token_ids.to(model.device),\n",
        "        max_new_tokens=10,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "  output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n",
        "  print(f\"{prompt}\\nprediction:{output}\")\n",
        "  print(f\"ground truth:{label_to_text[labels_binary['train'][i]]}\")\n",
        "  print('-'*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUXcB4axyCQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}