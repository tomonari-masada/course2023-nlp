{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYZk2+wmvBnyAls6aUwnPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/13_finetuning_GPTQ_LLM_with_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPTQで量子化されたLLMをLoRAでfinetuningする"
      ],
      "metadata": {
        "id": "_JgBiH0s-AY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMを効率よくカスタマイズするには？\n",
        "* パラメータ数が数十億（数ビリオン）のLLMをfinetuningするのは、大変。\n",
        "* しかし、あらかじめ量子化されたモデルを、LoRAでfinetuningするなら、計算資源はさほど必要ない。\n",
        "* ただし・・・\n",
        "  * そもそも、量子化されたモデルは、元のモデルよりも性能が良くない。\n",
        "* さらに・・・\n",
        "  * LoRAでfinetuningするよりも、元のモデルを直接finetuningするほうが、性能は上がる。\n",
        "  * しかし、元のモデルを直接finetuningするには、それなりの計算資源が必要となる。"
      ],
      "metadata": {
        "id": "O0mPCYHZ-Gqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 参考資料\n",
        "* 今回の授業資料の元になっているmediumの記事\n",
        "  * https://dsmonk.medium.com/training-and-deploying-of-quantized-llms-with-lora-and-gptq-part-2-2-ec7b54659c9e\n",
        "\n",
        "* 以下は、その他の参考資料\n",
        "  * https://medium.com/@pazuzzu/in-depth-llm-fine-tuning-guide-efficiently-fine-tune-and-use-zephyr-7b-beta-assistant-using-lora-e23d8151e067\n",
        "  * https://huggingface.co/docs/trl/v0.7.4/en/sft_trainer\n",
        "  * https://blog.gopenai.com/fine-tuning-mistral-7b-instruct-model-in-colab-a-beginners-guide-0f7bebccf11c\n",
        "  * https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py"
      ],
      "metadata": {
        "id": "fvDYAbWWkAsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インストール"
      ],
      "metadata": {
        "id": "GjEY6AKC_azV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm0RTHfIfZ69"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers trl accelerate torch bitsandbytes peft datasets auto-gptq optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ここでセッションを再起動する。**"
      ],
      "metadata": {
        "id": "OeAc8PKy30oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* 今回は、ライブドアニュースコーパスの本文部分を使ってみる。"
      ],
      "metadata": {
        "id": "frmrJdyx_dYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "3Cg8MDzt252g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "vBFgaeEt8l4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"content\"][0]"
      ],
      "metadata": {
        "id": "pd7-uqmY3BtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル\n",
        "* ELYZAをGPTQで量子化したモデルを使う。"
      ],
      "metadata": {
        "id": "GYMHc2ux_yY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"TFMC/ELYZA-japanese-Llama-2-7b-instruct-GPTQ-4bit-64g\""
      ],
      "metadata": {
        "id": "CiDce6b3feSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 細かいノウハウがあるようなので、今回の資料の元になっている下の記事を参照のこと。\n",
        "  * https://dsmonk.medium.com/training-and-deploying-of-quantized-llms-with-lora-and-gptq-part-2-2-ec7b54659c9e"
      ],
      "metadata": {
        "id": "ErVnCmmu3Xx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from transformers import GPTQConfig\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    use_safetensors=True,\n",
        "    disable_exllama=False,\n",
        "    device=\"cuda:0\",\n",
        "    trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "# https://github.com/huggingface/transformers/pull/24906\n",
        "#disable tensor parallelism\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "kPToOs563PWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAの設定"
      ],
      "metadata": {
        "id": "VPLuZcYRAHGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LoRAはPEFTのなかで最もよく使われている手法。\n",
        "  * トランスフォーマモデルを構成する様々なパラメータ行列をfinetuningする。\n",
        "  * ただし、元のモデルはfreezeさせて、finetuningによる差分だけ学習する。\n",
        "  * そして、差分そのものを学習するのではなく、差分の低ランク近似を学習する。\n",
        "* 詳しくは、原論文を参照。\n",
        "  * https://arxiv.org/abs/2106.09685"
      ],
      "metadata": {
        "id": "y0JChabKAKFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "2OopEb3m4FTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuningの設定"
      ],
      "metadata": {
        "id": "0FDd7E1kA_ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `remove_unused_columns=False`\n",
        "  * これを追加しないと、なぜかデータセットに関するエラーが出てしまう。\n",
        "  * https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/24"
      ],
      "metadata": {
        "id": "Bn780nF8BEJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 下記の設定は適当に決めたもの。\n",
        "  * チューニングしないと性能が出ない。"
      ],
      "metadata": {
        "id": "c2p1gS-JBRJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    max_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True, #use mixed precision training\n",
        "    logging_steps=1,\n",
        "    output_dir=\"outputs_gptq_training\",\n",
        "    optim=\"adamw_hf\",\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    )"
      ],
      "metadata": {
        "id": "3c1f-pGY53RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `SFTTrainer`クラス"
      ],
      "metadata": {
        "id": "UnfdUWX-Bc4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TRL = Transformer Reinforcement Learning\n",
        "  * https://huggingface.co/docs/trl/\n",
        "  * TRLは強化学習によってLLMをfinetuningするためのライブラリだが・・・\n",
        "  * 今回は、単にSFT (supervised finetuning)をおこなうために使うだけ。"
      ],
      "metadata": {
        "id": "YCCVQchZBmkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    peft_config=config,\n",
        "    dataset_text_field=\"content\",\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False,\n",
        "    max_seq_length=512,\n",
        "    )"
      ],
      "metadata": {
        "id": "qNXV6YCbg15X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習の実行"
      ],
      "metadata": {
        "id": "cXRjNM2EB6xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 20分ぐらいかかる。"
      ],
      "metadata": {
        "id": "aZ4jBQtwKQRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "4bSx9SMyhA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価"
      ],
      "metadata": {
        "id": "lw9zlUxmJgiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 時間節約のため・・・\n",
        "* validation setのインスタンスの`title`で・・・\n",
        "* 最も近いインスタンスが同じ`category`である割合を調べる。"
      ],
      "metadata": {
        "id": "uEDdaLmwJhn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_eos_token = True\n",
        "batch_dict = tokenizer(\n",
        "    dataset[\"validation\"][\"title\"][0],\n",
        "    add_special_tokens=True,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        "    )"
      ],
      "metadata": {
        "id": "mCP4oLTf6fs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_dict.input_ids"
      ],
      "metadata": {
        "id": "a5ZJkSQlCDkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_dict.attention_mask"
      ],
      "metadata": {
        "id": "E2u2QSQxFNd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  last_hidden_state = model(\n",
        "      input_ids=batch_dict.input_ids.to(model.device),\n",
        "      attention_mask=batch_dict.attention_mask.to(model.device),\n",
        "      output_hidden_states=True,\n",
        "      ).hidden_states[-1].cpu()"
      ],
      "metadata": {
        "id": "DCLG8aEPExIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state.shape"
      ],
      "metadata": {
        "id": "9pwAKpLgFfMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_dict.input_ids[0,-1]"
      ],
      "metadata": {
        "id": "OWw6rbyyFgKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(batch_dict.input_ids == tokenizer.eos_token_id).nonzero()"
      ],
      "metadata": {
        "id": "Htph29etGgle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state[0,-1]"
      ],
      "metadata": {
        "id": "wXCGsFyHFma8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset[\"validation\"])"
      ],
      "metadata": {
        "id": "fl99-G3oFrDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook\n",
        "\n",
        "tokenizer.add_eos_token = True\n",
        "\n",
        "embeddings_list = list()\n",
        "for i in tqdm_notebook(range(len(dataset[\"validation\"]))):\n",
        "  batch_dict = tokenizer(\n",
        "    dataset[\"validation\"][\"title\"][i],\n",
        "    add_special_tokens=True,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\",\n",
        "    )\n",
        "  with torch.no_grad():\n",
        "    last_hidden_state = model(\n",
        "        input_ids=batch_dict.input_ids.to(model.device),\n",
        "        attention_mask=batch_dict.attention_mask.to(model.device),\n",
        "        output_hidden_states=True,\n",
        "        ).hidden_states[-1].cpu()\n",
        "    embeddings = last_hidden_state[0,-1]\n",
        "    embeddings_list.append(embeddings)"
      ],
      "metadata": {
        "id": "RR-4rD0lFwlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = torch.stack(embeddings_list).type(torch.float32)"
      ],
      "metadata": {
        "id": "4tmBtG0kFzfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "c42NhGvGIorp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "similarities = torch.matmul(\n",
        "    F.normalize(embeddings, dim=-1),\n",
        "    F.normalize(embeddings, dim=-1).t()\n",
        ")"
      ],
      "metadata": {
        "id": "iu_mzUSEH3-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities"
      ],
      "metadata": {
        "id": "ORcKK5ckIW3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices = torch.argsort(similarities, descending=True).cpu()"
      ],
      "metadata": {
        "id": "3sXantJ2H9Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices"
      ],
      "metadata": {
        "id": "7v5hnrHeIT-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices[:,1]"
      ],
      "metadata": {
        "id": "WRmioMFCIVRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_categories = torch.tensor(dataset[\"validation\"][\"category\"], dtype=torch.int64)"
      ],
      "metadata": {
        "id": "mjy5V-AwJDSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_categories[sorted_indices[:,1]]"
      ],
      "metadata": {
        "id": "I1wD01tjJRSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(validation_categories == validation_categories[sorted_indices[:,1]]).sum()"
      ],
      "metadata": {
        "id": "Wa6hgO41JUs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(validation_categories)"
      ],
      "metadata": {
        "id": "SpbKslC3JYpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "368 / 737"
      ],
      "metadata": {
        "id": "EvoioYckJaq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYNCFgIIJczK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}