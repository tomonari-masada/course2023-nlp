{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPPYhWDXI1/vzT1ZimYsv8f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2023-nlp/blob/main/13_finetuning_GPTQ_LLM_with_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPTQで量子化されたLLMをLoRAでfinetuningする"
      ],
      "metadata": {
        "id": "_JgBiH0s-AY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMを効率よくカスタマイズするには？\n",
        "* パラメータ数が数B（数十億）のLLMをfinetuningするのは、大変。\n",
        "* しかし、あらかじめ量子化されたモデルを、LoRAでfinetuningするなら、まあ、手に負える。\n",
        "* ただし・・・\n",
        "  * そもそも、量子化されたモデルは、元のモデルほどは性能が良くない。\n",
        "* さらに・・・\n",
        "  * LoRAでfinetuningするよりも、元のモデルを直接finetuningするほうが、性能は上がる。"
      ],
      "metadata": {
        "id": "O0mPCYHZ-Gqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 参考資料\n",
        "* 今回の授業資料の元になっているmediumの記事\n",
        "  * https://dsmonk.medium.com/training-and-deploying-of-quantized-llms-with-lora-and-gptq-part-2-2-ec7b54659c9e\n",
        "\n",
        "* LLMのfinetuningに関するその他の参考資料\n",
        "  * https://medium.com/@pazuzzu/in-depth-llm-fine-tuning-guide-efficiently-fine-tune-and-use-zephyr-7b-beta-assistant-using-lora-e23d8151e067\n",
        "  * https://huggingface.co/docs/trl/main/en/sft_trainer\n",
        "  * https://blog.gopenai.com/fine-tuning-mistral-7b-instruct-model-in-colab-a-beginners-guide-0f7bebccf11c\n",
        "* SFTTrainerによるfinetuningのコードの雛形\n",
        "  * https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py\n",
        "* LLMを効率的に動かすこと全般に関するHugging Faceのチュートリアル（finetuningは関係ない）\n",
        "  * https://huggingface.co/docs/transformers/llm_tutorial_optimization"
      ],
      "metadata": {
        "id": "fvDYAbWWkAsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## インストール"
      ],
      "metadata": {
        "id": "GjEY6AKC_azV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* transformersライブラリは最新のものに更新した方がいいかも。\n",
        "  * finetuningでlossがゼロになったら、原因はおそらくこれ。"
      ],
      "metadata": {
        "id": "2OpLzW-mZUCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm0RTHfIfZ69"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/huggingface/transformers trl accelerate torch bitsandbytes peft datasets auto-gptq optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ここでセッションを再起動する。**"
      ],
      "metadata": {
        "id": "OeAc8PKy30oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 再現性の確保\n",
        "* transformersのset_seed関数\n",
        "  * randomもnumpyもPyTorchも、ちゃんと乱数のシードを設定しているようです。\n",
        "  * https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L85"
      ],
      "metadata": {
        "id": "MqQ7l84WeOoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(123)"
      ],
      "metadata": {
        "id": "CyxLItnJd_li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "* finetuningにはライブドアニュースコーパスの`title`フィールドを使う。\n",
        "  * `content`フィールドを使ってみてもいいです。"
      ],
      "metadata": {
        "id": "frmrJdyx_dYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_state=42, # 再現性の確保\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "3Cg8MDzt252g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "vBFgaeEt8l4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"title\"][0]"
      ],
      "metadata": {
        "id": "pd7-uqmY3BtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデル\n",
        "* 前回と同様、`weblab-10b-instruction-sft`をGPTQで量子化したモデルを使う。\n",
        "  * https://huggingface.co/dahara1/weblab-10b-instruction-sft-GPTQ"
      ],
      "metadata": {
        "id": "GYMHc2ux_yY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"dahara1/weblab-10b-instruction-sft-GPTQ\"\n",
        "model_basename = \"gptq_model-4bit-128g\""
      ],
      "metadata": {
        "id": "CiDce6b3feSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回主に参考にしたのは、下の記事。\n",
        "  * https://dsmonk.medium.com/training-and-deploying-of-quantized-llms-with-lora-and-gptq-part-2-2-ec7b54659c9e"
      ],
      "metadata": {
        "id": "ErVnCmmu3Xx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* gradient checkpointingについては、下記を参照。\n",
        "  * https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#gradient-checkpointing"
      ],
      "metadata": {
        "id": "nCz8oBeTOH0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    model_basename=model_basename,\n",
        "    device=\"cuda:0\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Gradient Checkpointingを行うために必要な設定\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "kPToOs563PWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルのどの部分をfinetuningするか？\n",
        "  * 今回は、attention部分で使われている行列をfinetuningする。"
      ],
      "metadata": {
        "id": "vSyeDNOaMqtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "qVOyEq0yjemo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRAの設定"
      ],
      "metadata": {
        "id": "VPLuZcYRAHGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LoRAはPEFTのなかで最もよく使われている手法。\n",
        "  * トランスフォーマモデルを構成する様々なパラメータ行列をfinetuningする。\n",
        "  * ただし、元のモデルはfreezeさせて、finetuningによる差分だけ学習する。\n",
        "  * そして、差分そのものを学習するのではなく、差分の低ランク近似を学習する。\n",
        "* 詳しくは、原論文を参照。\n",
        "  * https://arxiv.org/abs/2106.09685"
      ],
      "metadata": {
        "id": "y0JChabKAKFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `lora_alpha`をLoRAのランクの2倍にするというのは、rule of thumb。\n",
        "  * https://lightning.ai/pages/community/lora-insights/"
      ],
      "metadata": {
        "id": "zs5_iRAubnGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"attention.query_key_value\", \"attention.dense\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "2OopEb3m4FTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "mzVbNG9I7jFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetuningの設定"
      ],
      "metadata": {
        "id": "0FDd7E1kA_ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `remove_unused_columns=False`\n",
        "  * これを追加した上で・・・\n",
        "  * finetuningの実行前に、余分なcolumnsを手動で削除する（後出）。\n",
        "  * こうしないと、なぜかデータセットに関するエラーが出てしまう。\n"
      ],
      "metadata": {
        "id": "Bn780nF8BEJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    max_steps=100,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True, #use mixed precision training\n",
        "    logging_steps=1,\n",
        "    output_dir=\"outputs_gptq_training\",\n",
        "    optim=\"adamw_hf\",\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    # 以下の2行がvalidation setによる評価のための設定\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    )"
      ],
      "metadata": {
        "id": "3c1f-pGY53RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 上の設定は適当に決めたもの。\n",
        "* これをチューニングしてはじめて性能が出る。\n",
        "* 以下のハイパーパラメータは特に重要。\n",
        "  * `learning_rate`\n",
        "  * `per_device_train_batch_size`\n",
        "  * `gradient_accumulation_steps`"
      ],
      "metadata": {
        "id": "c2p1gS-JBRJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `SFTTrainer`"
      ],
      "metadata": {
        "id": "UnfdUWX-Bc4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* TRL = Transformer Reinforcement Learning\n",
        "  * https://huggingface.co/docs/trl/\n",
        "  * TRLは強化学習によってLLMをfinetuningするためのライブラリだが・・・\n",
        "  * 今回は、単にSFT (supervised finetuning)をおこなうために使うだけ。"
      ],
      "metadata": {
        "id": "YCCVQchZBmkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    peft_config=config,\n",
        "    dataset_text_field=\"title\",\n",
        "    tokenizer=tokenizer,\n",
        "    packing=False,\n",
        "    max_seq_length=256,\n",
        "    )"
      ],
      "metadata": {
        "id": "qNXV6YCbg15X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習の実行"
      ],
      "metadata": {
        "id": "cXRjNM2EB6xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* なぜかこの操作が必要。"
      ],
      "metadata": {
        "id": "cnJwHUaXYAFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train_dataset = trainer.train_dataset.remove_columns(['url', 'date', 'content', 'category', 'title'])\n",
        "trainer.eval_dataset = trainer.eval_dataset.remove_columns(['url', 'date', 'content', 'category', 'title'])"
      ],
      "metadata": {
        "id": "1FobPmxgX4UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回の設定でのfinetuningには7分弱かかる。\n",
        "* finetuning終了後に表示される情報の意味は、下記を参照。\n",
        "  * https://huggingface.co/docs/transformers/main_classes/callback#transformers.TrainerState"
      ],
      "metadata": {
        "id": "aZ4jBQtwKQRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "4bSx9SMyhA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(\".\")"
      ],
      "metadata": {
        "id": "-oJFJG5IEQio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**評価の前にセッションを再起動する**"
      ],
      "metadata": {
        "id": "S2O8hF4nH5Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 評価"
      ],
      "metadata": {
        "id": "lw9zlUxmJgiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット"
      ],
      "metadata": {
        "id": "p03w7OHXjOCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"shunk031/livedoor-news-corpus\",\n",
        "    train_ratio=0.8,\n",
        "    val_ratio=0.1,\n",
        "    test_ratio=0.1,\n",
        "    random_state=42, # 再現性の確保\n",
        "    shuffle=True,\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "id": "KYEtrrcVFMab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"title\"][0]"
      ],
      "metadata": {
        "id": "JUYffZGNgxZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 保存したLoRAモデルの読み込み"
      ],
      "metadata": {
        "id": "jO2T5CLjjRMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"dahara1/weblab-10b-instruction-sft-GPTQ\"\n",
        "model_basename = \"gptq_model-4bit-128g\""
      ],
      "metadata": {
        "id": "A1zCPbHGFVkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* まずモデル本体をロードする。"
      ],
      "metadata": {
        "id": "-ojj21Q3kBYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    model_basename=model_basename,\n",
        "    device=\"cuda:0\",\n",
        "    )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Gradient Checkpointingを行うために必要な設定\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "815jXEbVE0Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 次にLoRAをロードする。"
      ],
      "metadata": {
        "id": "MTRKcNfZkEnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, PeftConfig\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig.from_pretrained(\".\")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "mATdI2PPE58n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## コーパスの埋め込み"
      ],
      "metadata": {
        "id": "njp_0LWKjXBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* average poolingをおこなう関数"
      ],
      "metadata": {
        "id": "_hsZe_xNfICZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_pool(last_hidden_states, attention_mask):\n",
        "  last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "  return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
      ],
      "metadata": {
        "id": "lm1yCtmhDIAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 全てのテキストの埋め込み\n",
        "  * 手元のRTX3090で実行すると、1分半で終わる。"
      ],
      "metadata": {
        "id": "8jGfdaCFfOzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "embeddings_list = {}\n",
        "for key in dataset.keys():\n",
        "  corpus = dataset[key][\"title\"]\n",
        "  offset = 0\n",
        "  embeddings_list[key] = list()\n",
        "  for offset in tqdm(range(0, len(corpus), batch_size)):\n",
        "    batch_dict = tokenizer(\n",
        "        corpus[offset:offset+batch_size],\n",
        "        padding=True, truncation=True, return_tensors='pt'\n",
        "        ).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "      last_hidden_state = model(\n",
        "          input_ids=batch_dict.input_ids,\n",
        "          attention_mask=batch_dict.attention_mask,\n",
        "          output_hidden_states=True,\n",
        "          ).hidden_states[-1].cpu()\n",
        "    tmp_embeddings = average_pool(last_hidden_state, batch_dict.attention_mask.cpu())\n",
        "    embeddings_list[key].append(tmp_embeddings)\n",
        "    offset += batch_size"
      ],
      "metadata": {
        "id": "kiUtPSNUDO3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = {}\n",
        "for key in dataset.keys():\n",
        "  embeddings[key] = torch.concat(embeddings_list[key]).type(torch.float32)\n",
        "  print(key, embeddings[key].shape)"
      ],
      "metadata": {
        "id": "fwUs4-r_DUV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in dataset.keys():\n",
        "  torch.save(embeddings[key], f\"livedoor_weblab-10b-instruction-sft-GPTQ_finetuned_{key}.pt\")"
      ],
      "metadata": {
        "id": "Z_4BFkT4DVxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-NNによる分類"
      ],
      "metadata": {
        "id": "fNQ6paufjc23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "similarities = torch.matmul(\n",
        "    F.normalize(embeddings[\"validation\"], dim=-1),\n",
        "    F.normalize(embeddings[\"train\"], dim=-1).t()\n",
        ")"
      ],
      "metadata": {
        "id": "dktd14R1DZSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_indices = torch.argsort(similarities, descending=True).cpu()"
      ],
      "metadata": {
        "id": "PVC6hb-qDZ9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category = {}\n",
        "for key in dataset.keys():\n",
        "  category[key] = torch.tensor(dataset[key][\"category\"])"
      ],
      "metadata": {
        "id": "eH_4aDHfINz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(category[\"validation\"][0], category[\"train\"][sorted_indices[0,:20]])"
      ],
      "metadata": {
        "id": "FN2wxFt8DgDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(category[\"train\"][sorted_indices[:,0]] == category[\"validation\"]).sum() / len(category[\"validation\"])"
      ],
      "metadata": {
        "id": "Gl7SlwpTDedi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* finetuningでvalidation lossをチェックしつつ、ハイパーパラメータをチューニングするのが正しい手順。"
      ],
      "metadata": {
        "id": "NCfkEFddkNmN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cYNCFgIIJczK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}